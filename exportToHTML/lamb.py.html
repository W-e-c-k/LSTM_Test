<html>
<head>
<title>lamb.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
lamb.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">optimizer</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.optimizers.Lamb&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">Lamb</span><span class="s2">(</span><span class="s1">optimizer</span><span class="s2">.</span><span class="s1">Optimizer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Optimizer that implements the Lamb algorithm. 
 
    Lamb is a stochastic gradient descent method that 
    uses layer-wise adaptive moments to adjusts the 
    learning rate for each parameter based on the ratio of the 
    norm of the weight to the norm of the gradient 
    This helps to stabilize the training process and improves convergence 
    especially for large batch sizes. 
 
    Args: 
        learning_rate: A float, a 
            `keras.optimizers.schedules.LearningRateSchedule` instance, or 
            a callable that takes no arguments and returns the actual value to 
            use. The learning rate. Defaults to `0.001`. 
        beta_1: A float value or a constant float tensor, or a callable 
            that takes no arguments and returns the actual value to use. The 
            exponential decay rate for the 1st moment estimates. Defaults to 
            `0.9`. 
        beta_2: A float value or a constant float tensor, or a callable 
            that takes no arguments and returns the actual value to use. The 
            exponential decay rate for the 2nd moment estimates. Defaults to 
            `0.999`. 
        epsilon: A small constant for numerical stability. 
            Defaults to `1e-7`. 
        {{base_optimizer_keyword_args}} 
 
    References: 
        - [Yang et al.](https://arxiv.org/pdf/1904.00962) 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">learning_rate</span><span class="s2">=</span><span class="s5">0.001</span><span class="s2">,</span>
        <span class="s1">beta_1</span><span class="s2">=</span><span class="s5">0.9</span><span class="s2">,</span>
        <span class="s1">beta_2</span><span class="s2">=</span><span class="s5">0.999</span><span class="s2">,</span>
        <span class="s1">epsilon</span><span class="s2">=</span><span class="s5">1e-7</span><span class="s2">,</span>
        <span class="s1">weight_decay</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipvalue</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">global_clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_ema</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">ema_momentum</span><span class="s2">=</span><span class="s5">0.99</span><span class="s2">,</span>
        <span class="s1">ema_overwrite_frequency</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">loss_scale_factor</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">gradient_accumulation_steps</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;lamb&quot;</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">learning_rate</span><span class="s2">=</span><span class="s1">learning_rate</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s1">weight_decay</span><span class="s2">=</span><span class="s1">weight_decay</span><span class="s2">,</span>
            <span class="s1">clipnorm</span><span class="s2">=</span><span class="s1">clipnorm</span><span class="s2">,</span>
            <span class="s1">clipvalue</span><span class="s2">=</span><span class="s1">clipvalue</span><span class="s2">,</span>
            <span class="s1">global_clipnorm</span><span class="s2">=</span><span class="s1">global_clipnorm</span><span class="s2">,</span>
            <span class="s1">use_ema</span><span class="s2">=</span><span class="s1">use_ema</span><span class="s2">,</span>
            <span class="s1">ema_momentum</span><span class="s2">=</span><span class="s1">ema_momentum</span><span class="s2">,</span>
            <span class="s1">ema_overwrite_frequency</span><span class="s2">=</span><span class="s1">ema_overwrite_frequency</span><span class="s2">,</span>
            <span class="s1">loss_scale_factor</span><span class="s2">=</span><span class="s1">loss_scale_factor</span><span class="s2">,</span>
            <span class="s1">gradient_accumulation_steps</span><span class="s2">=</span><span class="s1">gradient_accumulation_steps</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">beta_1 </span><span class="s2">= </span><span class="s1">beta_1</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2 </span><span class="s2">= </span><span class="s1">beta_2</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon </span><span class="s2">= </span><span class="s1">epsilon</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">var_list</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Initialize optimizer variables. 
 
        Lamb optimizer has 2 types of variables: momentums and velocities 
 
        Args: 
            var_list: list of model variables to build Lamb variables on. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">build</span><span class="s2">(</span><span class="s1">var_list</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_momentums </span><span class="s2">= []</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_velocities </span><span class="s2">= []</span>
        <span class="s0">for </span><span class="s1">var </span><span class="s0">in </span><span class="s1">var_list</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_momentums</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
                    <span class="s1">reference_variable</span><span class="s2">=</span><span class="s1">var</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;momentum&quot;</span>
                <span class="s2">)</span>
            <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_velocities</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
                    <span class="s1">reference_variable</span><span class="s2">=</span><span class="s1">var</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;velocity&quot;</span>
                <span class="s2">)</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">update_step</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Update step given gradient and the associated model variable.&quot;&quot;&quot;</span>
        <span class="s1">lr </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">gradient </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">local_step </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">iterations </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>

        <span class="s1">beta_1_power </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">power</span><span class="s2">(</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_1</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">), </span><span class="s1">local_step</span>
        <span class="s2">)</span>
        <span class="s1">beta_2_power </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">power</span><span class="s2">(</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">), </span><span class="s1">local_step</span>
        <span class="s2">)</span>

        <span class="s1">m </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_momentums</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>
        <span class="s1">v </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_velocities</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">assign_add</span><span class="s2">(</span>
            <span class="s1">m</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">m</span><span class="s2">), </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_1</span><span class="s2">)</span>
        <span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">assign_add</span><span class="s2">(</span>
            <span class="s1">v</span><span class="s2">,</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">), </span><span class="s1">v</span><span class="s2">), </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2</span>
            <span class="s2">),</span>
        <span class="s2">)</span>

        <span class="s1">m_t_hat </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">m</span><span class="s2">, (</span><span class="s5">1.0 </span><span class="s2">- </span><span class="s1">beta_1_power</span><span class="s2">))</span>
        <span class="s1">v_sqrt </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">v</span><span class="s2">, (</span><span class="s5">1.0 </span><span class="s2">- </span><span class="s1">beta_2_power</span><span class="s2">))), </span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon</span>
        <span class="s2">)</span>

        <span class="s1">update </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">m_t_hat</span><span class="s2">, </span><span class="s1">v_sqrt</span><span class="s2">)</span>
        <span class="s1">w_norm </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">power</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">, </span><span class="s5">2</span><span class="s2">)))</span>
        <span class="s1">g_norm </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">power</span><span class="s2">(</span><span class="s1">update</span><span class="s2">, </span><span class="s5">2</span><span class="s2">)))</span>

        <span class="s6"># ratio = w_norm / g_norm if w_norm &gt; 0 and g_norm &gt; 0 else 1</span>
        <span class="s1">ratio </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">greater</span><span class="s2">(</span><span class="s1">w_norm</span><span class="s2">, </span><span class="s5">0</span><span class="s2">),</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">greater</span><span class="s2">(</span><span class="s1">g_norm</span><span class="s2">, </span><span class="s5">0</span><span class="s2">), (</span><span class="s1">w_norm </span><span class="s2">/ </span><span class="s1">g_norm</span><span class="s2">), </span><span class="s5">1.0</span><span class="s2">),</span>
            <span class="s5">1.0</span><span class="s2">,</span>
        <span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">assign_sub</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">, </span><span class="s1">ratio </span><span class="s2">* </span><span class="s1">lr </span><span class="s2">* </span><span class="s1">update</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s1">config</span><span class="s2">.</span><span class="s1">update</span><span class="s2">(</span>
            <span class="s2">{</span>
                <span class="s3">&quot;beta_1&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_1</span><span class="s2">,</span>
                <span class="s3">&quot;beta_2&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2</span><span class="s2">,</span>
                <span class="s3">&quot;epsilon&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">,</span>
            <span class="s2">}</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">config</span>


<span class="s1">Lamb</span><span class="s2">.</span><span class="s1">__doc__ </span><span class="s2">= </span><span class="s1">Lamb</span><span class="s2">.</span><span class="s1">__doc__</span><span class="s2">.</span><span class="s1">replace</span><span class="s2">(</span>
    <span class="s3">&quot;{{base_optimizer_keyword_args}}&quot;</span><span class="s2">, </span><span class="s1">optimizer</span><span class="s2">.</span><span class="s1">base_optimizer_keyword_args</span>
<span class="s2">)</span>
</pre>
</body>
</html>