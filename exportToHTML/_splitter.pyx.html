<html>
<head>
<title>_splitter.pyx</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #bcbec4;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_splitter.pyx</font>
</center></td></tr></table>
<pre><span class="s0"># Authors: Gilles Louppe &lt;g.louppe@gmail.com&gt;</span>
<span class="s0">#          Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt;</span>
<span class="s0">#          Brian Holt &lt;bdholt1@gmail.com&gt;</span>
<span class="s0">#          Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="s0">#          Satrajit Gosh &lt;satrajit.ghosh@gmail.com&gt;</span>
<span class="s0">#          Lars Buitinck</span>
<span class="s0">#          Arnaud Joly &lt;arnaud.v.joly@gmail.com&gt;</span>
<span class="s0">#          Joel Nothman &lt;joel.nothman@gmail.com&gt;</span>
<span class="s0">#          Fares Hedayati &lt;fares.hedayati@gmail.com&gt;</span>
<span class="s0">#          Jacob Schreiber &lt;jmschreiber91@gmail.com&gt;</span>
<span class="s0">#</span>
<span class="s0"># License: BSD 3 clause</span>

<span class="s0">from cython cimport final</span>
<span class="s0">from libc.math cimport isnan</span>
<span class="s0">from libc.stdlib cimport qsort</span>
<span class="s0">from libc.string cimport memcpy</span>

<span class="s0">from ._criterion cimport Criterion</span>
<span class="s0">from ._utils cimport log</span>
<span class="s0">from ._utils cimport rand_int</span>
<span class="s0">from ._utils cimport rand_uniform</span>
<span class="s0">from ._utils cimport RAND_R_MAX</span>
<span class="s0">from ..utils._typedefs cimport int8_t</span>

<span class="s0">import numpy as np</span>
<span class="s0">from scipy.sparse import issparse</span>


<span class="s0">cdef float64_t INFINITY = np.inf</span>

<span class="s0"># Mitigate precision differences between 32 bit and 64 bit</span>
<span class="s0">cdef float32_t FEATURE_THRESHOLD = 1e-7</span>

<span class="s0"># Constant to switch between algorithm non zero value extract algorithm</span>
<span class="s0"># in SparsePartitioner</span>
<span class="s0">cdef float32_t EXTRACT_NNZ_SWITCH = 0.1</span>

<span class="s0">cdef inline void _init_split(SplitRecord* self, intp_t start_pos) noexcept nogil:</span>
    <span class="s0">self.impurity_left = INFINITY</span>
    <span class="s0">self.impurity_right = INFINITY</span>
    <span class="s0">self.pos = start_pos</span>
    <span class="s0">self.feature = 0</span>
    <span class="s0">self.threshold = 0.</span>
    <span class="s0">self.improvement = -INFINITY</span>
    <span class="s0">self.missing_go_to_left = False</span>
    <span class="s0">self.n_missing = 0</span>

<span class="s0">cdef class Splitter:</span>
    <span class="s0">&quot;&quot;&quot;Abstract splitter class.</span>

    <span class="s0">Splitters are called by tree builders to find the best splits on both</span>
    <span class="s0">sparse and dense data, one split at a time.</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">def __cinit__(</span>
        <span class="s0">self,</span>
        <span class="s0">Criterion criterion,</span>
        <span class="s0">intp_t max_features,</span>
        <span class="s0">intp_t min_samples_leaf,</span>
        <span class="s0">float64_t min_weight_leaf,</span>
        <span class="s0">object random_state,</span>
        <span class="s0">const int8_t[:] monotonic_cst,</span>
    <span class="s0">):</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">criterion : Criterion</span>
            <span class="s0">The criterion to measure the quality of a split.</span>

        <span class="s0">max_features : intp_t</span>
            <span class="s0">The maximal number of randomly selected features which can be</span>
            <span class="s0">considered for a split.</span>

        <span class="s0">min_samples_leaf : intp_t</span>
            <span class="s0">The minimal number of samples each leaf can have, where splits</span>
            <span class="s0">which would result in having less samples in a leaf are not</span>
            <span class="s0">considered.</span>

        <span class="s0">min_weight_leaf : float64_t</span>
            <span class="s0">The minimal weight each leaf can have, where the weight is the sum</span>
            <span class="s0">of the weights of each sample in it.</span>

        <span class="s0">random_state : object</span>
            <span class="s0">The user inputted random state to be used for pseudo-randomness</span>

        <span class="s0">monotonic_cst : const int8_t[:]</span>
            <span class="s0">Monotonicity constraints</span>

        <span class="s0">&quot;&quot;&quot;</span>

        <span class="s0">self.criterion = criterion</span>

        <span class="s0">self.n_samples = 0</span>
        <span class="s0">self.n_features = 0</span>

        <span class="s0">self.max_features = max_features</span>
        <span class="s0">self.min_samples_leaf = min_samples_leaf</span>
        <span class="s0">self.min_weight_leaf = min_weight_leaf</span>
        <span class="s0">self.random_state = random_state</span>
        <span class="s0">self.monotonic_cst = monotonic_cst</span>
        <span class="s0">self.with_monotonic_cst = monotonic_cst is not None</span>

    <span class="s0">def __getstate__(self):</span>
        <span class="s0">return {}</span>

    <span class="s0">def __setstate__(self, d):</span>
        <span class="s0">pass</span>

    <span class="s0">def __reduce__(self):</span>
        <span class="s0">return (type(self), (self.criterion,</span>
                             <span class="s0">self.max_features,</span>
                             <span class="s0">self.min_samples_leaf,</span>
                             <span class="s0">self.min_weight_leaf,</span>
                             <span class="s0">self.random_state,</span>
                             <span class="s0">self.monotonic_cst), self.__getstate__())</span>

    <span class="s0">cdef int init(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">const float64_t[:, ::1] y,</span>
        <span class="s0">const float64_t[:] sample_weight,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">) except -1:</span>
        <span class="s0">&quot;&quot;&quot;Initialize the splitter.</span>

        <span class="s0">Take in the input data X, the target Y, and optional sample weights.</span>

        <span class="s0">Returns -1 in case of failure to allocate memory (and raise MemoryError)</span>
        <span class="s0">or 0 otherwise.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">X : object</span>
            <span class="s0">This contains the inputs. Usually it is a 2d numpy array.</span>

        <span class="s0">y : ndarray, dtype=float64_t</span>
            <span class="s0">This is the vector of targets, or true labels, for the samples represented</span>
            <span class="s0">as a Cython memoryview.</span>

        <span class="s0">sample_weight : ndarray, dtype=float64_t</span>
            <span class="s0">The weights of the samples, where higher weighted samples are fit</span>
            <span class="s0">closer than lower weight samples. If not provided, all samples</span>
            <span class="s0">are assumed to have uniform weight. This is represented</span>
            <span class="s0">as a Cython memoryview.</span>

        <span class="s0">has_missing : bool</span>
            <span class="s0">At least one missing values is in X.</span>
        <span class="s0">&quot;&quot;&quot;</span>

        <span class="s0">self.rand_r_state = self.random_state.randint(0, RAND_R_MAX)</span>
        <span class="s0">cdef intp_t n_samples = X.shape[0]</span>

        <span class="s0"># Create a new array which will be used to store nonzero</span>
        <span class="s0"># samples from the feature of interest</span>
        <span class="s0">self.samples = np.empty(n_samples, dtype=np.intp)</span>
        <span class="s0">cdef intp_t[::1] samples = self.samples</span>

        <span class="s0">cdef intp_t i, j</span>
        <span class="s0">cdef float64_t weighted_n_samples = 0.0</span>
        <span class="s0">j = 0</span>

        <span class="s0">for i in range(n_samples):</span>
            <span class="s0"># Only work with positively weighted samples</span>
            <span class="s0">if sample_weight is None or sample_weight[i] != 0.0:</span>
                <span class="s0">samples[j] = i</span>
                <span class="s0">j += 1</span>

            <span class="s0">if sample_weight is not None:</span>
                <span class="s0">weighted_n_samples += sample_weight[i]</span>
            <span class="s0">else:</span>
                <span class="s0">weighted_n_samples += 1.0</span>

        <span class="s0"># Number of samples is number of positively weighted samples</span>
        <span class="s0">self.n_samples = j</span>
        <span class="s0">self.weighted_n_samples = weighted_n_samples</span>

        <span class="s0">cdef intp_t n_features = X.shape[1]</span>
        <span class="s0">self.features = np.arange(n_features, dtype=np.intp)</span>
        <span class="s0">self.n_features = n_features</span>

        <span class="s0">self.feature_values = np.empty(n_samples, dtype=np.float32)</span>
        <span class="s0">self.constant_features = np.empty(n_features, dtype=np.intp)</span>

        <span class="s0">self.y = y</span>

        <span class="s0">self.sample_weight = sample_weight</span>
        <span class="s0">if missing_values_in_feature_mask is not None:</span>
            <span class="s0">self.criterion.init_sum_missing()</span>
        <span class="s0">return 0</span>

    <span class="s0">cdef int node_reset(</span>
        <span class="s0">self,</span>
        <span class="s0">intp_t start,</span>
        <span class="s0">intp_t end,</span>
        <span class="s0">float64_t* weighted_n_node_samples</span>
    <span class="s0">) except -1 nogil:</span>
        <span class="s0">&quot;&quot;&quot;Reset splitter on node samples[start:end].</span>

        <span class="s0">Returns -1 in case of failure to allocate memory (and raise MemoryError)</span>
        <span class="s0">or 0 otherwise.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">start : intp_t</span>
            <span class="s0">The index of the first sample to consider</span>
        <span class="s0">end : intp_t</span>
            <span class="s0">The index of the last sample to consider</span>
        <span class="s0">weighted_n_node_samples : ndarray, dtype=float64_t pointer</span>
            <span class="s0">The total weight of those samples</span>
        <span class="s0">&quot;&quot;&quot;</span>

        <span class="s0">self.start = start</span>
        <span class="s0">self.end = end</span>

        <span class="s0">self.criterion.init(</span>
            <span class="s0">self.y,</span>
            <span class="s0">self.sample_weight,</span>
            <span class="s0">self.weighted_n_samples,</span>
            <span class="s0">self.samples,</span>
            <span class="s0">start,</span>
            <span class="s0">end</span>
        <span class="s0">)</span>

        <span class="s0">weighted_n_node_samples[0] = self.criterion.weighted_n_node_samples</span>
        <span class="s0">return 0</span>

    <span class="s0">cdef int node_split(</span>
        <span class="s0">self,</span>
        <span class="s0">ParentInfo* parent_record,</span>
        <span class="s0">SplitRecord* split,</span>
    <span class="s0">) except -1 nogil:</span>

        <span class="s0">&quot;&quot;&quot;Find the best split on node samples[start:end].</span>

        <span class="s0">This is a placeholder method. The majority of computation will be done</span>
        <span class="s0">here.</span>

        <span class="s0">It should return -1 upon errors.</span>
        <span class="s0">&quot;&quot;&quot;</span>

        <span class="s0">pass</span>

    <span class="s0">cdef void node_value(self, float64_t* dest) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Copy the value of node samples[start:end] into dest.&quot;&quot;&quot;</span>

        <span class="s0">self.criterion.node_value(dest)</span>

    <span class="s0">cdef inline void clip_node_value(self, float64_t* dest, float64_t lower_bound, float64_t upper_bound) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Clip the value in dest between lower_bound and upper_bound for monotonic constraints.&quot;&quot;&quot;</span>

        <span class="s0">self.criterion.clip_node_value(dest, lower_bound, upper_bound)</span>

    <span class="s0">cdef float64_t node_impurity(self) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Return the impurity of the current node.&quot;&quot;&quot;</span>

        <span class="s0">return self.criterion.node_impurity()</span>

<span class="s0">cdef inline void shift_missing_values_to_left_if_required(</span>
    <span class="s0">SplitRecord* best,</span>
    <span class="s0">intp_t[::1] samples,</span>
    <span class="s0">intp_t end,</span>
<span class="s0">) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Shift missing value sample indices to the left of the split if required.</span>

    <span class="s0">Note: this should always be called at the very end because it will</span>
    <span class="s0">move samples around, thereby affecting the criterion.</span>
    <span class="s0">This affects the computation of the children impurity, which affects</span>
    <span class="s0">the computation of the next node.</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef intp_t i, p, current_end</span>
    <span class="s0"># The partitioner partitions the data such that the missing values are in</span>
    <span class="s0"># samples[-n_missing:] for the criterion to consume. If the missing values</span>
    <span class="s0"># are going to the right node, then the missing values are already in the</span>
    <span class="s0"># correct position. If the missing values go left, then we move the missing</span>
    <span class="s0"># values to samples[best.pos:best.pos+n_missing] and update `best.pos`.</span>
    <span class="s0">if best.n_missing &gt; 0 and best.missing_go_to_left:</span>
        <span class="s0">for p in range(best.n_missing):</span>
            <span class="s0">i = best.pos + p</span>
            <span class="s0">current_end = end - 1 - p</span>
            <span class="s0">samples[i], samples[current_end] = samples[current_end], samples[i]</span>
        <span class="s0">best.pos += best.n_missing</span>

<span class="s0"># Introduce a fused-class to make it possible to share the split implementation</span>
<span class="s0"># between the dense and sparse cases in the node_split_best and node_split_random</span>
<span class="s0"># functions. The alternative would have been to use inheritance-based polymorphism</span>
<span class="s0"># but it would have resulted in a ~10% overall tree fitting performance</span>
<span class="s0"># degradation caused by the overhead frequent virtual method lookups.</span>
<span class="s0">ctypedef fused Partitioner:</span>
    <span class="s0">DensePartitioner</span>
    <span class="s0">SparsePartitioner</span>

<span class="s0">cdef inline int node_split_best(</span>
    <span class="s0">Splitter splitter,</span>
    <span class="s0">Partitioner partitioner,</span>
    <span class="s0">Criterion criterion,</span>
    <span class="s0">SplitRecord* split,</span>
    <span class="s0">ParentInfo* parent_record,</span>
    <span class="s0">bint with_monotonic_cst,</span>
    <span class="s0">const int8_t[:] monotonic_cst,</span>
<span class="s0">) except -1 nogil:</span>
    <span class="s0">&quot;&quot;&quot;Find the best split on node samples[start:end]</span>

    <span class="s0">Returns -1 in case of failure to allocate memory (and raise MemoryError)</span>
    <span class="s0">or 0 otherwise.</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0"># Find the best split</span>
    <span class="s0">cdef intp_t start = splitter.start</span>
    <span class="s0">cdef intp_t end = splitter.end</span>
    <span class="s0">cdef intp_t end_non_missing</span>
    <span class="s0">cdef intp_t n_missing = 0</span>
    <span class="s0">cdef bint has_missing = 0</span>
    <span class="s0">cdef intp_t n_searches</span>
    <span class="s0">cdef intp_t n_left, n_right</span>
    <span class="s0">cdef bint missing_go_to_left</span>

    <span class="s0">cdef intp_t[::1] samples = splitter.samples</span>
    <span class="s0">cdef intp_t[::1] features = splitter.features</span>
    <span class="s0">cdef intp_t[::1] constant_features = splitter.constant_features</span>
    <span class="s0">cdef intp_t n_features = splitter.n_features</span>

    <span class="s0">cdef float32_t[::1] feature_values = splitter.feature_values</span>
    <span class="s0">cdef intp_t max_features = splitter.max_features</span>
    <span class="s0">cdef intp_t min_samples_leaf = splitter.min_samples_leaf</span>
    <span class="s0">cdef float64_t min_weight_leaf = splitter.min_weight_leaf</span>
    <span class="s0">cdef uint32_t* random_state = &amp;splitter.rand_r_state</span>

    <span class="s0">cdef SplitRecord best_split, current_split</span>
    <span class="s0">cdef float64_t current_proxy_improvement = -INFINITY</span>
    <span class="s0">cdef float64_t best_proxy_improvement = -INFINITY</span>

    <span class="s0">cdef float64_t impurity = parent_record.impurity</span>
    <span class="s0">cdef float64_t lower_bound = parent_record.lower_bound</span>
    <span class="s0">cdef float64_t upper_bound = parent_record.upper_bound</span>

    <span class="s0">cdef intp_t f_i = n_features</span>
    <span class="s0">cdef intp_t f_j</span>
    <span class="s0">cdef intp_t p</span>
    <span class="s0">cdef intp_t p_prev</span>

    <span class="s0">cdef intp_t n_visited_features = 0</span>
    <span class="s0"># Number of features discovered to be constant during the split search</span>
    <span class="s0">cdef intp_t n_found_constants = 0</span>
    <span class="s0"># Number of features known to be constant and drawn without replacement</span>
    <span class="s0">cdef intp_t n_drawn_constants = 0</span>
    <span class="s0">cdef intp_t n_known_constants = parent_record.n_constant_features</span>
    <span class="s0"># n_total_constants = n_known_constants + n_found_constants</span>
    <span class="s0">cdef intp_t n_total_constants = n_known_constants</span>

    <span class="s0">_init_split(&amp;best_split, end)</span>

    <span class="s0">partitioner.init_node_split(start, end)</span>

    <span class="s0"># Sample up to max_features without replacement using a</span>
    <span class="s0"># Fisher-Yates-based algorithm (using the local variables `f_i` and</span>
    <span class="s0"># `f_j` to compute a permutation of the `features` array).</span>
    <span class="s0">#</span>
    <span class="s0"># Skip the CPU intensive evaluation of the impurity criterion for</span>
    <span class="s0"># features that were already detected as constant (hence not suitable</span>
    <span class="s0"># for good splitting) by ancestor nodes and save the information on</span>
    <span class="s0"># newly discovered constant features to spare computation on descendant</span>
    <span class="s0"># nodes.</span>
    <span class="s0">while (f_i &gt; n_total_constants and  # Stop early if remaining features</span>
                                        <span class="s0"># are constant</span>
            <span class="s0">(n_visited_features &lt; max_features or</span>
             <span class="s0"># At least one drawn features must be non constant</span>
             <span class="s0">n_visited_features &lt;= n_found_constants + n_drawn_constants)):</span>

        <span class="s0">n_visited_features += 1</span>

        <span class="s0"># Loop invariant: elements of features in</span>
        <span class="s0"># - [:n_drawn_constant[ holds drawn and known constant features;</span>
        <span class="s0"># - [n_drawn_constant:n_known_constant[ holds known constant</span>
        <span class="s0">#   features that haven't been drawn yet;</span>
        <span class="s0"># - [n_known_constant:n_total_constant[ holds newly found constant</span>
        <span class="s0">#   features;</span>
        <span class="s0"># - [n_total_constant:f_i[ holds features that haven't been drawn</span>
        <span class="s0">#   yet and aren't constant apriori.</span>
        <span class="s0"># - [f_i:n_features[ holds features that have been drawn</span>
        <span class="s0">#   and aren't constant.</span>

        <span class="s0"># Draw a feature at random</span>
        <span class="s0">f_j = rand_int(n_drawn_constants, f_i - n_found_constants,</span>
                       <span class="s0">random_state)</span>

        <span class="s0">if f_j &lt; n_known_constants:</span>
            <span class="s0"># f_j in the interval [n_drawn_constants, n_known_constants[</span>
            <span class="s0">features[n_drawn_constants], features[f_j] = features[f_j], features[n_drawn_constants]</span>

            <span class="s0">n_drawn_constants += 1</span>
            <span class="s0">continue</span>

        <span class="s0"># f_j in the interval [n_known_constants, f_i - n_found_constants[</span>
        <span class="s0">f_j += n_found_constants</span>
        <span class="s0"># f_j in the interval [n_total_constants, f_i[</span>
        <span class="s0">current_split.feature = features[f_j]</span>
        <span class="s0">partitioner.sort_samples_and_feature_values(current_split.feature)</span>
        <span class="s0">n_missing = partitioner.n_missing</span>
        <span class="s0">end_non_missing = end - n_missing</span>

        <span class="s0">if (</span>
            <span class="s0"># All values for this feature are missing, or</span>
            <span class="s0">end_non_missing == start or</span>
            <span class="s0"># This feature is considered constant (max - min &lt;= FEATURE_THRESHOLD)</span>
            <span class="s0">feature_values[end_non_missing - 1] &lt;= feature_values[start] + FEATURE_THRESHOLD</span>
        <span class="s0">):</span>
            <span class="s0"># We consider this feature constant in this case.</span>
            <span class="s0"># Since finding a split among constant feature is not valuable,</span>
            <span class="s0"># we do not consider this feature for splitting.</span>
            <span class="s0">features[f_j], features[n_total_constants] = features[n_total_constants], features[f_j]</span>

            <span class="s0">n_found_constants += 1</span>
            <span class="s0">n_total_constants += 1</span>
            <span class="s0">continue</span>

        <span class="s0">f_i -= 1</span>
        <span class="s0">features[f_i], features[f_j] = features[f_j], features[f_i]</span>
        <span class="s0">has_missing = n_missing != 0</span>
        <span class="s0">criterion.init_missing(n_missing)  # initialize even when n_missing == 0</span>

        <span class="s0"># Evaluate all splits</span>

        <span class="s0"># If there are missing values, then we search twice for the most optimal split.</span>
        <span class="s0"># The first search will have all the missing values going to the right node.</span>
        <span class="s0"># The second search will have all the missing values going to the left node.</span>
        <span class="s0"># If there are no missing values, then we search only once for the most</span>
        <span class="s0"># optimal split.</span>
        <span class="s0">n_searches = 2 if has_missing else 1</span>

        <span class="s0">for i in range(n_searches):</span>
            <span class="s0">missing_go_to_left = i == 1</span>
            <span class="s0">criterion.missing_go_to_left = missing_go_to_left</span>
            <span class="s0">criterion.reset()</span>

            <span class="s0">p = start</span>

            <span class="s0">while p &lt; end_non_missing:</span>
                <span class="s0">partitioner.next_p(&amp;p_prev, &amp;p)</span>

                <span class="s0">if p &gt;= end_non_missing:</span>
                    <span class="s0">continue</span>

                <span class="s0">if missing_go_to_left:</span>
                    <span class="s0">n_left = p - start + n_missing</span>
                    <span class="s0">n_right = end_non_missing - p</span>
                <span class="s0">else:</span>
                    <span class="s0">n_left = p - start</span>
                    <span class="s0">n_right = end_non_missing - p + n_missing</span>

                <span class="s0"># Reject if min_samples_leaf is not guaranteed</span>
                <span class="s0">if n_left &lt; min_samples_leaf or n_right &lt; min_samples_leaf:</span>
                    <span class="s0">continue</span>

                <span class="s0">current_split.pos = p</span>
                <span class="s0">criterion.update(current_split.pos)</span>

                <span class="s0"># Reject if monotonicity constraints are not satisfied</span>
                <span class="s0">if (</span>
                    <span class="s0">with_monotonic_cst and</span>
                    <span class="s0">monotonic_cst[current_split.feature] != 0 and</span>
                    <span class="s0">not criterion.check_monotonicity(</span>
                        <span class="s0">monotonic_cst[current_split.feature],</span>
                        <span class="s0">lower_bound,</span>
                        <span class="s0">upper_bound,</span>
                    <span class="s0">)</span>
                <span class="s0">):</span>
                    <span class="s0">continue</span>

                <span class="s0"># Reject if min_weight_leaf is not satisfied</span>
                <span class="s0">if ((criterion.weighted_n_left &lt; min_weight_leaf) or</span>
                        <span class="s0">(criterion.weighted_n_right &lt; min_weight_leaf)):</span>
                    <span class="s0">continue</span>

                <span class="s0">current_proxy_improvement = criterion.proxy_impurity_improvement()</span>

                <span class="s0">if current_proxy_improvement &gt; best_proxy_improvement:</span>
                    <span class="s0">best_proxy_improvement = current_proxy_improvement</span>
                    <span class="s0"># sum of halves is used to avoid infinite value</span>
                    <span class="s0">current_split.threshold = (</span>
                        <span class="s0">feature_values[p_prev] / 2.0 + feature_values[p] / 2.0</span>
                    <span class="s0">)</span>

                    <span class="s0">if (</span>
                        <span class="s0">current_split.threshold == feature_values[p] or</span>
                        <span class="s0">current_split.threshold == INFINITY or</span>
                        <span class="s0">current_split.threshold == -INFINITY</span>
                    <span class="s0">):</span>
                        <span class="s0">current_split.threshold = feature_values[p_prev]</span>

                    <span class="s0">current_split.n_missing = n_missing</span>
                    <span class="s0">if n_missing == 0:</span>
                        <span class="s0">current_split.missing_go_to_left = n_left &gt; n_right</span>
                    <span class="s0">else:</span>
                        <span class="s0">current_split.missing_go_to_left = missing_go_to_left</span>

                    <span class="s0">best_split = current_split  # copy</span>

        <span class="s0"># Evaluate when there are missing values and all missing values goes</span>
        <span class="s0"># to the right node and non-missing values goes to the left node.</span>
        <span class="s0">if has_missing:</span>
            <span class="s0">n_left, n_right = end - start - n_missing, n_missing</span>
            <span class="s0">p = end - n_missing</span>
            <span class="s0">missing_go_to_left = 0</span>

            <span class="s0">if not (n_left &lt; min_samples_leaf or n_right &lt; min_samples_leaf):</span>
                <span class="s0">criterion.missing_go_to_left = missing_go_to_left</span>
                <span class="s0">criterion.update(p)</span>

                <span class="s0">if not ((criterion.weighted_n_left &lt; min_weight_leaf) or</span>
                        <span class="s0">(criterion.weighted_n_right &lt; min_weight_leaf)):</span>
                    <span class="s0">current_proxy_improvement = criterion.proxy_impurity_improvement()</span>

                    <span class="s0">if current_proxy_improvement &gt; best_proxy_improvement:</span>
                        <span class="s0">best_proxy_improvement = current_proxy_improvement</span>
                        <span class="s0">current_split.threshold = INFINITY</span>
                        <span class="s0">current_split.missing_go_to_left = missing_go_to_left</span>
                        <span class="s0">current_split.n_missing = n_missing</span>
                        <span class="s0">current_split.pos = p</span>
                        <span class="s0">best_split = current_split</span>

    <span class="s0"># Reorganize into samples[start:best_split.pos] + samples[best_split.pos:end]</span>
    <span class="s0">if best_split.pos &lt; end:</span>
        <span class="s0">partitioner.partition_samples_final(</span>
            <span class="s0">best_split.pos,</span>
            <span class="s0">best_split.threshold,</span>
            <span class="s0">best_split.feature,</span>
            <span class="s0">best_split.n_missing</span>
        <span class="s0">)</span>
        <span class="s0">criterion.init_missing(best_split.n_missing)</span>
        <span class="s0">criterion.missing_go_to_left = best_split.missing_go_to_left</span>

        <span class="s0">criterion.reset()</span>
        <span class="s0">criterion.update(best_split.pos)</span>
        <span class="s0">criterion.children_impurity(</span>
            <span class="s0">&amp;best_split.impurity_left, &amp;best_split.impurity_right</span>
        <span class="s0">)</span>
        <span class="s0">best_split.improvement = criterion.impurity_improvement(</span>
            <span class="s0">impurity,</span>
            <span class="s0">best_split.impurity_left,</span>
            <span class="s0">best_split.impurity_right</span>
        <span class="s0">)</span>

        <span class="s0">shift_missing_values_to_left_if_required(&amp;best_split, samples, end)</span>

    <span class="s0"># Respect invariant for constant features: the original order of</span>
    <span class="s0"># element in features[:n_known_constants] must be preserved for sibling</span>
    <span class="s0"># and child nodes</span>
    <span class="s0">memcpy(&amp;features[0], &amp;constant_features[0], sizeof(intp_t) * n_known_constants)</span>

    <span class="s0"># Copy newly found constant features</span>
    <span class="s0">memcpy(&amp;constant_features[n_known_constants],</span>
           <span class="s0">&amp;features[n_known_constants],</span>
           <span class="s0">sizeof(intp_t) * n_found_constants)</span>

    <span class="s0"># Return values</span>
    <span class="s0">parent_record.n_constant_features = n_total_constants</span>
    <span class="s0">split[0] = best_split</span>
    <span class="s0">return 0</span>


<span class="s0"># Sort n-element arrays pointed to by feature_values and samples, simultaneously,</span>
<span class="s0"># by the values in feature_values. Algorithm: Introsort (Musser, SP&amp;E, 1997).</span>
<span class="s0">cdef inline void sort(float32_t* feature_values, intp_t* samples, intp_t n) noexcept nogil:</span>
    <span class="s0">if n == 0:</span>
        <span class="s0">return</span>
    <span class="s0">cdef intp_t maxd = 2 * &lt;intp_t&gt;log(n)</span>
    <span class="s0">introsort(feature_values, samples, n, maxd)</span>


<span class="s0">cdef inline void swap(float32_t* feature_values, intp_t* samples,</span>
                      <span class="s0">intp_t i, intp_t j) noexcept nogil:</span>
    <span class="s0"># Helper for sort</span>
    <span class="s0">feature_values[i], feature_values[j] = feature_values[j], feature_values[i]</span>
    <span class="s0">samples[i], samples[j] = samples[j], samples[i]</span>


<span class="s0">cdef inline float32_t median3(float32_t* feature_values, intp_t n) noexcept nogil:</span>
    <span class="s0"># Median of three pivot selection, after Bentley and McIlroy (1993).</span>
    <span class="s0"># Engineering a sort function. SP&amp;E. Requires 8/3 comparisons on average.</span>
    <span class="s0">cdef float32_t a = feature_values[0], b = feature_values[n / 2], c = feature_values[n - 1]</span>
    <span class="s0">if a &lt; b:</span>
        <span class="s0">if b &lt; c:</span>
            <span class="s0">return b</span>
        <span class="s0">elif a &lt; c:</span>
            <span class="s0">return c</span>
        <span class="s0">else:</span>
            <span class="s0">return a</span>
    <span class="s0">elif b &lt; c:</span>
        <span class="s0">if a &lt; c:</span>
            <span class="s0">return a</span>
        <span class="s0">else:</span>
            <span class="s0">return c</span>
    <span class="s0">else:</span>
        <span class="s0">return b</span>


<span class="s0"># Introsort with median of 3 pivot selection and 3-way partition function</span>
<span class="s0"># (robust to repeated elements, e.g. lots of zero features).</span>
<span class="s0">cdef void introsort(float32_t* feature_values, intp_t *samples,</span>
                    <span class="s0">intp_t n, intp_t maxd) noexcept nogil:</span>
    <span class="s0">cdef float32_t pivot</span>
    <span class="s0">cdef intp_t i, l, r</span>

    <span class="s0">while n &gt; 1:</span>
        <span class="s0">if maxd &lt;= 0:   # max depth limit exceeded (&quot;gone quadratic&quot;)</span>
            <span class="s0">heapsort(feature_values, samples, n)</span>
            <span class="s0">return</span>
        <span class="s0">maxd -= 1</span>

        <span class="s0">pivot = median3(feature_values, n)</span>

        <span class="s0"># Three-way partition.</span>
        <span class="s0">i = l = 0</span>
        <span class="s0">r = n</span>
        <span class="s0">while i &lt; r:</span>
            <span class="s0">if feature_values[i] &lt; pivot:</span>
                <span class="s0">swap(feature_values, samples, i, l)</span>
                <span class="s0">i += 1</span>
                <span class="s0">l += 1</span>
            <span class="s0">elif feature_values[i] &gt; pivot:</span>
                <span class="s0">r -= 1</span>
                <span class="s0">swap(feature_values, samples, i, r)</span>
            <span class="s0">else:</span>
                <span class="s0">i += 1</span>

        <span class="s0">introsort(feature_values, samples, l, maxd)</span>
        <span class="s0">feature_values += r</span>
        <span class="s0">samples += r</span>
        <span class="s0">n -= r</span>


<span class="s0">cdef inline void sift_down(float32_t* feature_values, intp_t* samples,</span>
                           <span class="s0">intp_t start, intp_t end) noexcept nogil:</span>
    <span class="s0"># Restore heap order in feature_values[start:end] by moving the max element to start.</span>
    <span class="s0">cdef intp_t child, maxind, root</span>

    <span class="s0">root = start</span>
    <span class="s0">while True:</span>
        <span class="s0">child = root * 2 + 1</span>

        <span class="s0"># find max of root, left child, right child</span>
        <span class="s0">maxind = root</span>
        <span class="s0">if child &lt; end and feature_values[maxind] &lt; feature_values[child]:</span>
            <span class="s0">maxind = child</span>
        <span class="s0">if child + 1 &lt; end and feature_values[maxind] &lt; feature_values[child + 1]:</span>
            <span class="s0">maxind = child + 1</span>

        <span class="s0">if maxind == root:</span>
            <span class="s0">break</span>
        <span class="s0">else:</span>
            <span class="s0">swap(feature_values, samples, root, maxind)</span>
            <span class="s0">root = maxind</span>


<span class="s0">cdef void heapsort(float32_t* feature_values, intp_t* samples, intp_t n) noexcept nogil:</span>
    <span class="s0">cdef intp_t start, end</span>

    <span class="s0"># heapify</span>
    <span class="s0">start = (n - 2) / 2</span>
    <span class="s0">end = n</span>
    <span class="s0">while True:</span>
        <span class="s0">sift_down(feature_values, samples, start, end)</span>
        <span class="s0">if start == 0:</span>
            <span class="s0">break</span>
        <span class="s0">start -= 1</span>

    <span class="s0"># sort by shrinking the heap, putting the max element immediately after it</span>
    <span class="s0">end = n - 1</span>
    <span class="s0">while end &gt; 0:</span>
        <span class="s0">swap(feature_values, samples, 0, end)</span>
        <span class="s0">sift_down(feature_values, samples, 0, end)</span>
        <span class="s0">end = end - 1</span>

<span class="s0">cdef inline int node_split_random(</span>
    <span class="s0">Splitter splitter,</span>
    <span class="s0">Partitioner partitioner,</span>
    <span class="s0">Criterion criterion,</span>
    <span class="s0">SplitRecord* split,</span>
    <span class="s0">ParentInfo* parent_record,</span>
    <span class="s0">bint with_monotonic_cst,</span>
    <span class="s0">const int8_t[:] monotonic_cst,</span>
<span class="s0">) except -1 nogil:</span>
    <span class="s0">&quot;&quot;&quot;Find the best random split on node samples[start:end]</span>

    <span class="s0">Returns -1 in case of failure to allocate memory (and raise MemoryError)</span>
    <span class="s0">or 0 otherwise.</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0"># Draw random splits and pick the best</span>
    <span class="s0">cdef intp_t start = splitter.start</span>
    <span class="s0">cdef intp_t end = splitter.end</span>

    <span class="s0">cdef intp_t[::1] features = splitter.features</span>
    <span class="s0">cdef intp_t[::1] constant_features = splitter.constant_features</span>
    <span class="s0">cdef intp_t n_features = splitter.n_features</span>

    <span class="s0">cdef intp_t max_features = splitter.max_features</span>
    <span class="s0">cdef intp_t min_samples_leaf = splitter.min_samples_leaf</span>
    <span class="s0">cdef float64_t min_weight_leaf = splitter.min_weight_leaf</span>
    <span class="s0">cdef uint32_t* random_state = &amp;splitter.rand_r_state</span>

    <span class="s0">cdef SplitRecord best_split, current_split</span>
    <span class="s0">cdef float64_t current_proxy_improvement = - INFINITY</span>
    <span class="s0">cdef float64_t best_proxy_improvement = - INFINITY</span>

    <span class="s0">cdef float64_t impurity = parent_record.impurity</span>
    <span class="s0">cdef float64_t lower_bound = parent_record.lower_bound</span>
    <span class="s0">cdef float64_t upper_bound = parent_record.upper_bound</span>

    <span class="s0">cdef intp_t f_i = n_features</span>
    <span class="s0">cdef intp_t f_j</span>
    <span class="s0"># Number of features discovered to be constant during the split search</span>
    <span class="s0">cdef intp_t n_found_constants = 0</span>
    <span class="s0"># Number of features known to be constant and drawn without replacement</span>
    <span class="s0">cdef intp_t n_drawn_constants = 0</span>
    <span class="s0">cdef intp_t n_known_constants = parent_record.n_constant_features</span>
    <span class="s0"># n_total_constants = n_known_constants + n_found_constants</span>
    <span class="s0">cdef intp_t n_total_constants = n_known_constants</span>
    <span class="s0">cdef intp_t n_visited_features = 0</span>
    <span class="s0">cdef float32_t min_feature_value</span>
    <span class="s0">cdef float32_t max_feature_value</span>

    <span class="s0">_init_split(&amp;best_split, end)</span>

    <span class="s0">partitioner.init_node_split(start, end)</span>

    <span class="s0"># Sample up to max_features without replacement using a</span>
    <span class="s0"># Fisher-Yates-based algorithm (using the local variables `f_i` and</span>
    <span class="s0"># `f_j` to compute a permutation of the `features` array).</span>
    <span class="s0">#</span>
    <span class="s0"># Skip the CPU intensive evaluation of the impurity criterion for</span>
    <span class="s0"># features that were already detected as constant (hence not suitable</span>
    <span class="s0"># for good splitting) by ancestor nodes and save the information on</span>
    <span class="s0"># newly discovered constant features to spare computation on descendant</span>
    <span class="s0"># nodes.</span>
    <span class="s0">while (f_i &gt; n_total_constants and  # Stop early if remaining features</span>
                                        <span class="s0"># are constant</span>
            <span class="s0">(n_visited_features &lt; max_features or</span>
             <span class="s0"># At least one drawn features must be non constant</span>
             <span class="s0">n_visited_features &lt;= n_found_constants + n_drawn_constants)):</span>
        <span class="s0">n_visited_features += 1</span>

        <span class="s0"># Loop invariant: elements of features in</span>
        <span class="s0"># - [:n_drawn_constant[ holds drawn and known constant features;</span>
        <span class="s0"># - [n_drawn_constant:n_known_constant[ holds known constant</span>
        <span class="s0">#   features that haven't been drawn yet;</span>
        <span class="s0"># - [n_known_constant:n_total_constant[ holds newly found constant</span>
        <span class="s0">#   features;</span>
        <span class="s0"># - [n_total_constant:f_i[ holds features that haven't been drawn</span>
        <span class="s0">#   yet and aren't constant apriori.</span>
        <span class="s0"># - [f_i:n_features[ holds features that have been drawn</span>
        <span class="s0">#   and aren't constant.</span>

        <span class="s0"># Draw a feature at random</span>
        <span class="s0">f_j = rand_int(n_drawn_constants, f_i - n_found_constants,</span>
                       <span class="s0">random_state)</span>

        <span class="s0">if f_j &lt; n_known_constants:</span>
            <span class="s0"># f_j in the interval [n_drawn_constants, n_known_constants[</span>
            <span class="s0">features[n_drawn_constants], features[f_j] = features[f_j], features[n_drawn_constants]</span>
            <span class="s0">n_drawn_constants += 1</span>
            <span class="s0">continue</span>

        <span class="s0"># f_j in the interval [n_known_constants, f_i - n_found_constants[</span>
        <span class="s0">f_j += n_found_constants</span>
        <span class="s0"># f_j in the interval [n_total_constants, f_i[</span>

        <span class="s0">current_split.feature = features[f_j]</span>

        <span class="s0"># Find min, max</span>
        <span class="s0">partitioner.find_min_max(</span>
            <span class="s0">current_split.feature, &amp;min_feature_value, &amp;max_feature_value</span>
        <span class="s0">)</span>

        <span class="s0">if max_feature_value &lt;= min_feature_value + FEATURE_THRESHOLD:</span>
            <span class="s0">features[f_j], features[n_total_constants] = features[n_total_constants], current_split.feature</span>

            <span class="s0">n_found_constants += 1</span>
            <span class="s0">n_total_constants += 1</span>
            <span class="s0">continue</span>

        <span class="s0">f_i -= 1</span>
        <span class="s0">features[f_i], features[f_j] = features[f_j], features[f_i]</span>

        <span class="s0"># Draw a random threshold</span>
        <span class="s0">current_split.threshold = rand_uniform(</span>
            <span class="s0">min_feature_value,</span>
            <span class="s0">max_feature_value,</span>
            <span class="s0">random_state,</span>
        <span class="s0">)</span>

        <span class="s0">if current_split.threshold == max_feature_value:</span>
            <span class="s0">current_split.threshold = min_feature_value</span>

        <span class="s0"># Partition</span>
        <span class="s0">current_split.pos = partitioner.partition_samples(current_split.threshold)</span>

        <span class="s0"># Reject if min_samples_leaf is not guaranteed</span>
        <span class="s0">if (((current_split.pos - start) &lt; min_samples_leaf) or</span>
                <span class="s0">((end - current_split.pos) &lt; min_samples_leaf)):</span>
            <span class="s0">continue</span>

        <span class="s0"># Evaluate split</span>
        <span class="s0"># At this point, the criterion has a view into the samples that was partitioned</span>
        <span class="s0"># by the partitioner. The criterion will use the partition to evaluating the split.</span>
        <span class="s0">criterion.reset()</span>
        <span class="s0">criterion.update(current_split.pos)</span>

        <span class="s0"># Reject if min_weight_leaf is not satisfied</span>
        <span class="s0">if ((criterion.weighted_n_left &lt; min_weight_leaf) or</span>
                <span class="s0">(criterion.weighted_n_right &lt; min_weight_leaf)):</span>
            <span class="s0">continue</span>

        <span class="s0"># Reject if monotonicity constraints are not satisfied</span>
        <span class="s0">if (</span>
                <span class="s0">with_monotonic_cst and</span>
                <span class="s0">monotonic_cst[current_split.feature] != 0 and</span>
                <span class="s0">not criterion.check_monotonicity(</span>
                    <span class="s0">monotonic_cst[current_split.feature],</span>
                    <span class="s0">lower_bound,</span>
                    <span class="s0">upper_bound,</span>
                <span class="s0">)</span>
        <span class="s0">):</span>
            <span class="s0">continue</span>

        <span class="s0">current_proxy_improvement = criterion.proxy_impurity_improvement()</span>

        <span class="s0">if current_proxy_improvement &gt; best_proxy_improvement:</span>
            <span class="s0">best_proxy_improvement = current_proxy_improvement</span>
            <span class="s0">best_split = current_split  # copy</span>

    <span class="s0"># Reorganize into samples[start:best.pos] + samples[best.pos:end]</span>
    <span class="s0">if best_split.pos &lt; end:</span>
        <span class="s0">if current_split.feature != best_split.feature:</span>
            <span class="s0"># TODO: Pass in best.n_missing when random splitter supports missing values.</span>
            <span class="s0">partitioner.partition_samples_final(</span>
                <span class="s0">best_split.pos, best_split.threshold, best_split.feature, 0</span>
            <span class="s0">)</span>

        <span class="s0">criterion.reset()</span>
        <span class="s0">criterion.update(best_split.pos)</span>
        <span class="s0">criterion.children_impurity(</span>
            <span class="s0">&amp;best_split.impurity_left, &amp;best_split.impurity_right</span>
        <span class="s0">)</span>
        <span class="s0">best_split.improvement = criterion.impurity_improvement(</span>
            <span class="s0">impurity, best_split.impurity_left, best_split.impurity_right</span>
        <span class="s0">)</span>

    <span class="s0"># Respect invariant for constant features: the original order of</span>
    <span class="s0"># element in features[:n_known_constants] must be preserved for sibling</span>
    <span class="s0"># and child nodes</span>
    <span class="s0">memcpy(&amp;features[0], &amp;constant_features[0], sizeof(intp_t) * n_known_constants)</span>

    <span class="s0"># Copy newly found constant features</span>
    <span class="s0">memcpy(&amp;constant_features[n_known_constants],</span>
           <span class="s0">&amp;features[n_known_constants],</span>
           <span class="s0">sizeof(intp_t) * n_found_constants)</span>

    <span class="s0"># Return values</span>
    <span class="s0">parent_record.n_constant_features = n_total_constants</span>
    <span class="s0">split[0] = best_split</span>
    <span class="s0">return 0</span>


<span class="s0">@final</span>
<span class="s0">cdef class DensePartitioner:</span>
    <span class="s0">&quot;&quot;&quot;Partitioner specialized for dense data.</span>

    <span class="s0">Note that this partitioner is agnostic to the splitting strategy (best vs. random).</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">const float32_t[:, :] X</span>
        <span class="s0">cdef intp_t[::1] samples</span>
        <span class="s0">cdef float32_t[::1] feature_values</span>
        <span class="s0">cdef intp_t start</span>
        <span class="s0">cdef intp_t end</span>
        <span class="s0">cdef intp_t n_missing</span>
        <span class="s0">cdef const unsigned char[::1] missing_values_in_feature_mask</span>

    <span class="s0">def __init__(</span>
        <span class="s0">self,</span>
        <span class="s0">const float32_t[:, :] X,</span>
        <span class="s0">intp_t[::1] samples,</span>
        <span class="s0">float32_t[::1] feature_values,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">):</span>
        <span class="s0">self.X = X</span>
        <span class="s0">self.samples = samples</span>
        <span class="s0">self.feature_values = feature_values</span>
        <span class="s0">self.missing_values_in_feature_mask = missing_values_in_feature_mask</span>

    <span class="s0">cdef inline void init_node_split(self, intp_t start, intp_t end) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Initialize splitter at the beginning of node_split.&quot;&quot;&quot;</span>
        <span class="s0">self.start = start</span>
        <span class="s0">self.end = end</span>
        <span class="s0">self.n_missing = 0</span>

    <span class="s0">cdef inline void sort_samples_and_feature_values(</span>
        <span class="s0">self, intp_t current_feature</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Simultaneously sort based on the feature_values.</span>

        <span class="s0">Missing values are stored at the end of feature_values.</span>
        <span class="s0">The number of missing values observed in feature_values is stored</span>
        <span class="s0">in self.n_missing.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t i, current_end</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>
            <span class="s0">const float32_t[:, :] X = self.X</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>
            <span class="s0">intp_t n_missing = 0</span>
            <span class="s0">const unsigned char[::1] missing_values_in_feature_mask = self.missing_values_in_feature_mask</span>

        <span class="s0"># Sort samples along that feature; by</span>
        <span class="s0"># copying the values into an array and</span>
        <span class="s0"># sorting the array in a manner which utilizes the cache more</span>
        <span class="s0"># effectively.</span>
        <span class="s0">if missing_values_in_feature_mask is not None and missing_values_in_feature_mask[current_feature]:</span>
            <span class="s0">i, current_end = self.start, self.end - 1</span>
            <span class="s0"># Missing values are placed at the end and do not participate in the sorting.</span>
            <span class="s0">while i &lt;= current_end:</span>
                <span class="s0"># Finds the right-most value that is not missing so that</span>
                <span class="s0"># it can be swapped with missing values at its left.</span>
                <span class="s0">if isnan(X[samples[current_end], current_feature]):</span>
                    <span class="s0">n_missing += 1</span>
                    <span class="s0">current_end -= 1</span>
                    <span class="s0">continue</span>

                <span class="s0"># X[samples[current_end], current_feature] is a non-missing value</span>
                <span class="s0">if isnan(X[samples[i], current_feature]):</span>
                    <span class="s0">samples[i], samples[current_end] = samples[current_end], samples[i]</span>
                    <span class="s0">n_missing += 1</span>
                    <span class="s0">current_end -= 1</span>

                <span class="s0">feature_values[i] = X[samples[i], current_feature]</span>
                <span class="s0">i += 1</span>
        <span class="s0">else:</span>
            <span class="s0"># When there are no missing values, we only need to copy the data into</span>
            <span class="s0"># feature_values</span>
            <span class="s0">for i in range(self.start, self.end):</span>
                <span class="s0">feature_values[i] = X[samples[i], current_feature]</span>

        <span class="s0">sort(&amp;feature_values[self.start], &amp;samples[self.start], self.end - self.start - n_missing)</span>
        <span class="s0">self.n_missing = n_missing</span>

    <span class="s0">cdef inline void find_min_max(</span>
        <span class="s0">self,</span>
        <span class="s0">intp_t current_feature,</span>
        <span class="s0">float32_t* min_feature_value_out,</span>
        <span class="s0">float32_t* max_feature_value_out,</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Find the minimum and maximum value for current_feature.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t p</span>
            <span class="s0">float32_t current_feature_value</span>
            <span class="s0">const float32_t[:, :] X = self.X</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>
            <span class="s0">float32_t min_feature_value = X[samples[self.start], current_feature]</span>
            <span class="s0">float32_t max_feature_value = min_feature_value</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>

        <span class="s0">feature_values[self.start] = min_feature_value</span>

        <span class="s0">for p in range(self.start + 1, self.end):</span>
            <span class="s0">current_feature_value = X[samples[p], current_feature]</span>
            <span class="s0">feature_values[p] = current_feature_value</span>

            <span class="s0">if current_feature_value &lt; min_feature_value:</span>
                <span class="s0">min_feature_value = current_feature_value</span>
            <span class="s0">elif current_feature_value &gt; max_feature_value:</span>
                <span class="s0">max_feature_value = current_feature_value</span>

        <span class="s0">min_feature_value_out[0] = min_feature_value</span>
        <span class="s0">max_feature_value_out[0] = max_feature_value</span>

    <span class="s0">cdef inline void next_p(self, intp_t* p_prev, intp_t* p) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Compute the next p_prev and p for iteratiing over feature values.</span>

        <span class="s0">The missing values are not included when iterating through the feature values.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>
            <span class="s0">intp_t end_non_missing = self.end - self.n_missing</span>

        <span class="s0">while (</span>
            <span class="s0">p[0] + 1 &lt; end_non_missing and</span>
            <span class="s0">feature_values[p[0] + 1] &lt;= feature_values[p[0]] + FEATURE_THRESHOLD</span>
        <span class="s0">):</span>
            <span class="s0">p[0] += 1</span>

        <span class="s0">p_prev[0] = p[0]</span>

        <span class="s0"># By adding 1, we have</span>
        <span class="s0"># (feature_values[p] &gt;= end) or (feature_values[p] &gt; feature_values[p - 1])</span>
        <span class="s0">p[0] += 1</span>

    <span class="s0">cdef inline intp_t partition_samples(self, float64_t current_threshold) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Partition samples for feature_values at the current_threshold.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t p = self.start</span>
            <span class="s0">intp_t partition_end = self.end</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>

        <span class="s0">while p &lt; partition_end:</span>
            <span class="s0">if feature_values[p] &lt;= current_threshold:</span>
                <span class="s0">p += 1</span>
            <span class="s0">else:</span>
                <span class="s0">partition_end -= 1</span>

                <span class="s0">feature_values[p], feature_values[partition_end] = (</span>
                    <span class="s0">feature_values[partition_end], feature_values[p]</span>
                <span class="s0">)</span>
                <span class="s0">samples[p], samples[partition_end] = samples[partition_end], samples[p]</span>

        <span class="s0">return partition_end</span>

    <span class="s0">cdef inline void partition_samples_final(</span>
        <span class="s0">self,</span>
        <span class="s0">intp_t best_pos,</span>
        <span class="s0">float64_t best_threshold,</span>
        <span class="s0">intp_t best_feature,</span>
        <span class="s0">intp_t best_n_missing,</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Partition samples for X at the best_threshold and best_feature.</span>

        <span class="s0">If missing values are present, this method partitions `samples`</span>
        <span class="s0">so that the `best_n_missing` missing values' indices are in the</span>
        <span class="s0">right-most end of `samples`, that is `samples[end_non_missing:end]`.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0"># Local invariance: start &lt;= p &lt;= partition_end &lt;= end</span>
            <span class="s0">intp_t start = self.start</span>
            <span class="s0">intp_t p = start</span>
            <span class="s0">intp_t end = self.end - 1</span>
            <span class="s0">intp_t partition_end = end - best_n_missing</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>
            <span class="s0">const float32_t[:, :] X = self.X</span>
            <span class="s0">float32_t current_value</span>

        <span class="s0">if best_n_missing != 0:</span>
            <span class="s0"># Move samples with missing values to the end while partitioning the</span>
            <span class="s0"># non-missing samples</span>
            <span class="s0">while p &lt; partition_end:</span>
                <span class="s0"># Keep samples with missing values at the end</span>
                <span class="s0">if isnan(X[samples[end], best_feature]):</span>
                    <span class="s0">end -= 1</span>
                    <span class="s0">continue</span>

                <span class="s0"># Swap sample with missing values with the sample at the end</span>
                <span class="s0">current_value = X[samples[p], best_feature]</span>
                <span class="s0">if isnan(current_value):</span>
                    <span class="s0">samples[p], samples[end] = samples[end], samples[p]</span>
                    <span class="s0">end -= 1</span>

                    <span class="s0"># The swapped sample at the end is always a non-missing value, so</span>
                    <span class="s0"># we can continue the algorithm without checking for missingness.</span>
                    <span class="s0">current_value = X[samples[p], best_feature]</span>

                <span class="s0"># Partition the non-missing samples</span>
                <span class="s0">if current_value &lt;= best_threshold:</span>
                    <span class="s0">p += 1</span>
                <span class="s0">else:</span>
                    <span class="s0">samples[p], samples[partition_end] = samples[partition_end], samples[p]</span>
                    <span class="s0">partition_end -= 1</span>
        <span class="s0">else:</span>
            <span class="s0"># Partitioning routine when there are no missing values</span>
            <span class="s0">while p &lt; partition_end:</span>
                <span class="s0">if X[samples[p], best_feature] &lt;= best_threshold:</span>
                    <span class="s0">p += 1</span>
                <span class="s0">else:</span>
                    <span class="s0">samples[p], samples[partition_end] = samples[partition_end], samples[p]</span>
                    <span class="s0">partition_end -= 1</span>


<span class="s0">@final</span>
<span class="s0">cdef class SparsePartitioner:</span>
    <span class="s0">&quot;&quot;&quot;Partitioner specialized for sparse CSC data.</span>

    <span class="s0">Note that this partitioner is agnostic to the splitting strategy (best vs. random).</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef intp_t[::1] samples</span>
    <span class="s0">cdef float32_t[::1] feature_values</span>
    <span class="s0">cdef intp_t start</span>
    <span class="s0">cdef intp_t end</span>
    <span class="s0">cdef intp_t n_missing</span>
    <span class="s0">cdef const unsigned char[::1] missing_values_in_feature_mask</span>

    <span class="s0">cdef const float32_t[::1] X_data</span>
    <span class="s0">cdef const int32_t[::1] X_indices</span>
    <span class="s0">cdef const int32_t[::1] X_indptr</span>

    <span class="s0">cdef intp_t n_total_samples</span>

    <span class="s0">cdef intp_t[::1] index_to_samples</span>
    <span class="s0">cdef intp_t[::1] sorted_samples</span>

    <span class="s0">cdef intp_t start_positive</span>
    <span class="s0">cdef intp_t end_negative</span>
    <span class="s0">cdef bint is_samples_sorted</span>

    <span class="s0">def __init__(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">intp_t[::1] samples,</span>
        <span class="s0">intp_t n_samples,</span>
        <span class="s0">float32_t[::1] feature_values,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">):</span>
        <span class="s0">if not (issparse(X) and X.format == &quot;csc&quot;):</span>
            <span class="s0">raise ValueError(&quot;X should be in csc format&quot;)</span>

        <span class="s0">self.samples = samples</span>
        <span class="s0">self.feature_values = feature_values</span>

        <span class="s0"># Initialize X</span>
        <span class="s0">cdef intp_t n_total_samples = X.shape[0]</span>

        <span class="s0">self.X_data = X.data</span>
        <span class="s0">self.X_indices = X.indices</span>
        <span class="s0">self.X_indptr = X.indptr</span>
        <span class="s0">self.n_total_samples = n_total_samples</span>

        <span class="s0"># Initialize auxiliary array used to perform split</span>
        <span class="s0">self.index_to_samples = np.full(n_total_samples, fill_value=-1, dtype=np.intp)</span>
        <span class="s0">self.sorted_samples = np.empty(n_samples, dtype=np.intp)</span>

        <span class="s0">cdef intp_t p</span>
        <span class="s0">for p in range(n_samples):</span>
            <span class="s0">self.index_to_samples[samples[p]] = p</span>

        <span class="s0">self.missing_values_in_feature_mask = missing_values_in_feature_mask</span>

    <span class="s0">cdef inline void init_node_split(self, intp_t start, intp_t end) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Initialize splitter at the beginning of node_split.&quot;&quot;&quot;</span>
        <span class="s0">self.start = start</span>
        <span class="s0">self.end = end</span>
        <span class="s0">self.is_samples_sorted = 0</span>
        <span class="s0">self.n_missing = 0</span>

    <span class="s0">cdef inline void sort_samples_and_feature_values(</span>
        <span class="s0">self, intp_t current_feature</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Simultaneously sort based on the feature_values.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>
            <span class="s0">intp_t[::1] index_to_samples = self.index_to_samples</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>

        <span class="s0">self.extract_nnz(current_feature)</span>
        <span class="s0"># Sort the positive and negative parts of `feature_values`</span>
        <span class="s0">sort(&amp;feature_values[self.start], &amp;samples[self.start], self.end_negative - self.start)</span>
        <span class="s0">if self.start_positive &lt; self.end:</span>
            <span class="s0">sort(</span>
                <span class="s0">&amp;feature_values[self.start_positive],</span>
                <span class="s0">&amp;samples[self.start_positive],</span>
                <span class="s0">self.end - self.start_positive</span>
            <span class="s0">)</span>

        <span class="s0"># Update index_to_samples to take into account the sort</span>
        <span class="s0">for p in range(self.start, self.end_negative):</span>
            <span class="s0">index_to_samples[samples[p]] = p</span>
        <span class="s0">for p in range(self.start_positive, self.end):</span>
            <span class="s0">index_to_samples[samples[p]] = p</span>

        <span class="s0"># Add one or two zeros in feature_values, if there is any</span>
        <span class="s0">if self.end_negative &lt; self.start_positive:</span>
            <span class="s0">self.start_positive -= 1</span>
            <span class="s0">feature_values[self.start_positive] = 0.</span>

            <span class="s0">if self.end_negative != self.start_positive:</span>
                <span class="s0">feature_values[self.end_negative] = 0.</span>
                <span class="s0">self.end_negative += 1</span>

        <span class="s0"># XXX: When sparse supports missing values, this should be set to the</span>
        <span class="s0"># number of missing values for current_feature</span>
        <span class="s0">self.n_missing = 0</span>

    <span class="s0">cdef inline void find_min_max(</span>
        <span class="s0">self,</span>
        <span class="s0">intp_t current_feature,</span>
        <span class="s0">float32_t* min_feature_value_out,</span>
        <span class="s0">float32_t* max_feature_value_out,</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Find the minimum and maximum value for current_feature.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t p</span>
            <span class="s0">float32_t current_feature_value, min_feature_value, max_feature_value</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>

        <span class="s0">self.extract_nnz(current_feature)</span>

        <span class="s0">if self.end_negative != self.start_positive:</span>
            <span class="s0"># There is a zero</span>
            <span class="s0">min_feature_value = 0</span>
            <span class="s0">max_feature_value = 0</span>
        <span class="s0">else:</span>
            <span class="s0">min_feature_value = feature_values[self.start]</span>
            <span class="s0">max_feature_value = min_feature_value</span>

        <span class="s0"># Find min, max in feature_values[start:end_negative]</span>
        <span class="s0">for p in range(self.start, self.end_negative):</span>
            <span class="s0">current_feature_value = feature_values[p]</span>

            <span class="s0">if current_feature_value &lt; min_feature_value:</span>
                <span class="s0">min_feature_value = current_feature_value</span>
            <span class="s0">elif current_feature_value &gt; max_feature_value:</span>
                <span class="s0">max_feature_value = current_feature_value</span>

        <span class="s0"># Update min, max given feature_values[start_positive:end]</span>
        <span class="s0">for p in range(self.start_positive, self.end):</span>
            <span class="s0">current_feature_value = feature_values[p]</span>

            <span class="s0">if current_feature_value &lt; min_feature_value:</span>
                <span class="s0">min_feature_value = current_feature_value</span>
            <span class="s0">elif current_feature_value &gt; max_feature_value:</span>
                <span class="s0">max_feature_value = current_feature_value</span>

        <span class="s0">min_feature_value_out[0] = min_feature_value</span>
        <span class="s0">max_feature_value_out[0] = max_feature_value</span>

    <span class="s0">cdef inline void next_p(self, intp_t* p_prev, intp_t* p) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Compute the next p_prev and p for iteratiing over feature values.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t p_next</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>

        <span class="s0">if p[0] + 1 != self.end_negative:</span>
            <span class="s0">p_next = p[0] + 1</span>
        <span class="s0">else:</span>
            <span class="s0">p_next = self.start_positive</span>

        <span class="s0">while (p_next &lt; self.end and</span>
                <span class="s0">feature_values[p_next] &lt;= feature_values[p[0]] + FEATURE_THRESHOLD):</span>
            <span class="s0">p[0] = p_next</span>
            <span class="s0">if p[0] + 1 != self.end_negative:</span>
                <span class="s0">p_next = p[0] + 1</span>
            <span class="s0">else:</span>
                <span class="s0">p_next = self.start_positive</span>

        <span class="s0">p_prev[0] = p[0]</span>
        <span class="s0">p[0] = p_next</span>

    <span class="s0">cdef inline intp_t partition_samples(self, float64_t current_threshold) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Partition samples for feature_values at the current_threshold.&quot;&quot;&quot;</span>
        <span class="s0">return self._partition(current_threshold, self.start_positive)</span>

    <span class="s0">cdef inline void partition_samples_final(</span>
        <span class="s0">self,</span>
        <span class="s0">intp_t best_pos,</span>
        <span class="s0">float64_t best_threshold,</span>
        <span class="s0">intp_t best_feature,</span>
        <span class="s0">intp_t n_missing,</span>
    <span class="s0">) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Partition samples for X at the best_threshold and best_feature.&quot;&quot;&quot;</span>
        <span class="s0">self.extract_nnz(best_feature)</span>
        <span class="s0">self._partition(best_threshold, best_pos)</span>

    <span class="s0">cdef inline intp_t _partition(self, float64_t threshold, intp_t zero_pos) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Partition samples[start:end] based on threshold.&quot;&quot;&quot;</span>
        <span class="s0">cdef:</span>
            <span class="s0">intp_t p, partition_end</span>
            <span class="s0">intp_t[::1] index_to_samples = self.index_to_samples</span>
            <span class="s0">float32_t[::1] feature_values = self.feature_values</span>
            <span class="s0">intp_t[::1] samples = self.samples</span>

        <span class="s0">if threshold &lt; 0.:</span>
            <span class="s0">p = self.start</span>
            <span class="s0">partition_end = self.end_negative</span>
        <span class="s0">elif threshold &gt; 0.:</span>
            <span class="s0">p = self.start_positive</span>
            <span class="s0">partition_end = self.end</span>
        <span class="s0">else:</span>
            <span class="s0"># Data are already split</span>
            <span class="s0">return zero_pos</span>

        <span class="s0">while p &lt; partition_end:</span>
            <span class="s0">if feature_values[p] &lt;= threshold:</span>
                <span class="s0">p += 1</span>

            <span class="s0">else:</span>
                <span class="s0">partition_end -= 1</span>

                <span class="s0">feature_values[p], feature_values[partition_end] = (</span>
                    <span class="s0">feature_values[partition_end], feature_values[p]</span>
                <span class="s0">)</span>
                <span class="s0">sparse_swap(index_to_samples, samples, p, partition_end)</span>

        <span class="s0">return partition_end</span>

    <span class="s0">cdef inline void extract_nnz(self, intp_t feature) noexcept nogil:</span>
        <span class="s0">&quot;&quot;&quot;Extract and partition values for a given feature.</span>

        <span class="s0">The extracted values are partitioned between negative values</span>
        <span class="s0">feature_values[start:end_negative[0]] and positive values</span>
        <span class="s0">feature_values[start_positive[0]:end].</span>
        <span class="s0">The samples and index_to_samples are modified according to this</span>
        <span class="s0">partition.</span>

        <span class="s0">The extraction corresponds to the intersection between the arrays</span>
        <span class="s0">X_indices[indptr_start:indptr_end] and samples[start:end].</span>
        <span class="s0">This is done efficiently using either an index_to_samples based approach</span>
        <span class="s0">or binary search based approach.</span>

        <span class="s0">Parameters</span>
        <span class="s0">----------</span>
        <span class="s0">feature : intp_t,</span>
            <span class="s0">Index of the feature we want to extract non zero value.</span>
        <span class="s0">&quot;&quot;&quot;</span>
        <span class="s0">cdef intp_t[::1] samples = self.samples</span>
        <span class="s0">cdef float32_t[::1] feature_values = self.feature_values</span>
        <span class="s0">cdef intp_t indptr_start = self.X_indptr[feature],</span>
        <span class="s0">cdef intp_t indptr_end = self.X_indptr[feature + 1]</span>
        <span class="s0">cdef intp_t n_indices = &lt;intp_t&gt;(indptr_end - indptr_start)</span>
        <span class="s0">cdef intp_t n_samples = self.end - self.start</span>
        <span class="s0">cdef intp_t[::1] index_to_samples = self.index_to_samples</span>
        <span class="s0">cdef intp_t[::1] sorted_samples = self.sorted_samples</span>
        <span class="s0">cdef const int32_t[::1] X_indices = self.X_indices</span>
        <span class="s0">cdef const float32_t[::1] X_data = self.X_data</span>

        <span class="s0"># Use binary search if n_samples * log(n_indices) &lt;</span>
        <span class="s0"># n_indices and index_to_samples approach otherwise.</span>
        <span class="s0"># O(n_samples * log(n_indices)) is the running time of binary</span>
        <span class="s0"># search and O(n_indices) is the running time of index_to_samples</span>
        <span class="s0"># approach.</span>
        <span class="s0">if ((1 - self.is_samples_sorted) * n_samples * log(n_samples) +</span>
                <span class="s0">n_samples * log(n_indices) &lt; EXTRACT_NNZ_SWITCH * n_indices):</span>
            <span class="s0">extract_nnz_binary_search(X_indices, X_data,</span>
                                      <span class="s0">indptr_start, indptr_end,</span>
                                      <span class="s0">samples, self.start, self.end,</span>
                                      <span class="s0">index_to_samples,</span>
                                      <span class="s0">feature_values,</span>
                                      <span class="s0">&amp;self.end_negative, &amp;self.start_positive,</span>
                                      <span class="s0">sorted_samples, &amp;self.is_samples_sorted)</span>

        <span class="s0"># Using an index to samples  technique to extract non zero values</span>
        <span class="s0"># index_to_samples is a mapping from X_indices to samples</span>
        <span class="s0">else:</span>
            <span class="s0">extract_nnz_index_to_samples(X_indices, X_data,</span>
                                         <span class="s0">indptr_start, indptr_end,</span>
                                         <span class="s0">samples, self.start, self.end,</span>
                                         <span class="s0">index_to_samples,</span>
                                         <span class="s0">feature_values,</span>
                                         <span class="s0">&amp;self.end_negative, &amp;self.start_positive)</span>


<span class="s0">cdef int compare_SIZE_t(const void* a, const void* b) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Comparison function for sort.</span>

    <span class="s0">This must return an `int` as it is used by stdlib's qsort, which expects</span>
    <span class="s0">an `int` return value.</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">return &lt;int&gt;((&lt;intp_t*&gt;a)[0] - (&lt;intp_t*&gt;b)[0])</span>


<span class="s0">cdef inline void binary_search(const int32_t[::1] sorted_array,</span>
                               <span class="s0">int32_t start, int32_t end,</span>
                               <span class="s0">intp_t value, intp_t* index,</span>
                               <span class="s0">int32_t* new_start) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Return the index of value in the sorted array.</span>

    <span class="s0">If not found, return -1. new_start is the last pivot + 1</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef int32_t pivot</span>
    <span class="s0">index[0] = -1</span>
    <span class="s0">while start &lt; end:</span>
        <span class="s0">pivot = start + (end - start) / 2</span>

        <span class="s0">if sorted_array[pivot] == value:</span>
            <span class="s0">index[0] = pivot</span>
            <span class="s0">start = pivot + 1</span>
            <span class="s0">break</span>

        <span class="s0">if sorted_array[pivot] &lt; value:</span>
            <span class="s0">start = pivot + 1</span>
        <span class="s0">else:</span>
            <span class="s0">end = pivot</span>
    <span class="s0">new_start[0] = start</span>


<span class="s0">cdef inline void extract_nnz_index_to_samples(const int32_t[::1] X_indices,</span>
                                              <span class="s0">const float32_t[::1] X_data,</span>
                                              <span class="s0">int32_t indptr_start,</span>
                                              <span class="s0">int32_t indptr_end,</span>
                                              <span class="s0">intp_t[::1] samples,</span>
                                              <span class="s0">intp_t start,</span>
                                              <span class="s0">intp_t end,</span>
                                              <span class="s0">intp_t[::1] index_to_samples,</span>
                                              <span class="s0">float32_t[::1] feature_values,</span>
                                              <span class="s0">intp_t* end_negative,</span>
                                              <span class="s0">intp_t* start_positive) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Extract and partition values for a feature using index_to_samples.</span>

    <span class="s0">Complexity is O(indptr_end - indptr_start).</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef int32_t k</span>
    <span class="s0">cdef intp_t index</span>
    <span class="s0">cdef intp_t end_negative_ = start</span>
    <span class="s0">cdef intp_t start_positive_ = end</span>

    <span class="s0">for k in range(indptr_start, indptr_end):</span>
        <span class="s0">if start &lt;= index_to_samples[X_indices[k]] &lt; end:</span>
            <span class="s0">if X_data[k] &gt; 0:</span>
                <span class="s0">start_positive_ -= 1</span>
                <span class="s0">feature_values[start_positive_] = X_data[k]</span>
                <span class="s0">index = index_to_samples[X_indices[k]]</span>
                <span class="s0">sparse_swap(index_to_samples, samples, index, start_positive_)</span>

            <span class="s0">elif X_data[k] &lt; 0:</span>
                <span class="s0">feature_values[end_negative_] = X_data[k]</span>
                <span class="s0">index = index_to_samples[X_indices[k]]</span>
                <span class="s0">sparse_swap(index_to_samples, samples, index, end_negative_)</span>
                <span class="s0">end_negative_ += 1</span>

    <span class="s0"># Returned values</span>
    <span class="s0">end_negative[0] = end_negative_</span>
    <span class="s0">start_positive[0] = start_positive_</span>


<span class="s0">cdef inline void extract_nnz_binary_search(const int32_t[::1] X_indices,</span>
                                           <span class="s0">const float32_t[::1] X_data,</span>
                                           <span class="s0">int32_t indptr_start,</span>
                                           <span class="s0">int32_t indptr_end,</span>
                                           <span class="s0">intp_t[::1] samples,</span>
                                           <span class="s0">intp_t start,</span>
                                           <span class="s0">intp_t end,</span>
                                           <span class="s0">intp_t[::1] index_to_samples,</span>
                                           <span class="s0">float32_t[::1] feature_values,</span>
                                           <span class="s0">intp_t* end_negative,</span>
                                           <span class="s0">intp_t* start_positive,</span>
                                           <span class="s0">intp_t[::1] sorted_samples,</span>
                                           <span class="s0">bint* is_samples_sorted) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Extract and partition values for a given feature using binary search.</span>

    <span class="s0">If n_samples = end - start and n_indices = indptr_end - indptr_start,</span>
    <span class="s0">the complexity is</span>

        <span class="s0">O((1 - is_samples_sorted[0]) * n_samples * log(n_samples) +</span>
          <span class="s0">n_samples * log(n_indices)).</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef intp_t n_samples</span>

    <span class="s0">if not is_samples_sorted[0]:</span>
        <span class="s0">n_samples = end - start</span>
        <span class="s0">memcpy(&amp;sorted_samples[start], &amp;samples[start],</span>
               <span class="s0">n_samples * sizeof(intp_t))</span>
        <span class="s0">qsort(&amp;sorted_samples[start], n_samples, sizeof(intp_t),</span>
              <span class="s0">compare_SIZE_t)</span>
        <span class="s0">is_samples_sorted[0] = 1</span>

    <span class="s0">while (indptr_start &lt; indptr_end and</span>
           <span class="s0">sorted_samples[start] &gt; X_indices[indptr_start]):</span>
        <span class="s0">indptr_start += 1</span>

    <span class="s0">while (indptr_start &lt; indptr_end and</span>
           <span class="s0">sorted_samples[end - 1] &lt; X_indices[indptr_end - 1]):</span>
        <span class="s0">indptr_end -= 1</span>

    <span class="s0">cdef intp_t p = start</span>
    <span class="s0">cdef intp_t index</span>
    <span class="s0">cdef intp_t k</span>
    <span class="s0">cdef intp_t end_negative_ = start</span>
    <span class="s0">cdef intp_t start_positive_ = end</span>

    <span class="s0">while (p &lt; end and indptr_start &lt; indptr_end):</span>
        <span class="s0"># Find index of sorted_samples[p] in X_indices</span>
        <span class="s0">binary_search(X_indices, indptr_start, indptr_end,</span>
                      <span class="s0">sorted_samples[p], &amp;k, &amp;indptr_start)</span>

        <span class="s0">if k != -1:</span>
            <span class="s0"># If k != -1, we have found a non zero value</span>

            <span class="s0">if X_data[k] &gt; 0:</span>
                <span class="s0">start_positive_ -= 1</span>
                <span class="s0">feature_values[start_positive_] = X_data[k]</span>
                <span class="s0">index = index_to_samples[X_indices[k]]</span>
                <span class="s0">sparse_swap(index_to_samples, samples, index, start_positive_)</span>

            <span class="s0">elif X_data[k] &lt; 0:</span>
                <span class="s0">feature_values[end_negative_] = X_data[k]</span>
                <span class="s0">index = index_to_samples[X_indices[k]]</span>
                <span class="s0">sparse_swap(index_to_samples, samples, index, end_negative_)</span>
                <span class="s0">end_negative_ += 1</span>
        <span class="s0">p += 1</span>

    <span class="s0"># Returned values</span>
    <span class="s0">end_negative[0] = end_negative_</span>
    <span class="s0">start_positive[0] = start_positive_</span>


<span class="s0">cdef inline void sparse_swap(intp_t[::1] index_to_samples, intp_t[::1] samples,</span>
                             <span class="s0">intp_t pos_1, intp_t pos_2) noexcept nogil:</span>
    <span class="s0">&quot;&quot;&quot;Swap sample pos_1 and pos_2 preserving sparse invariant.&quot;&quot;&quot;</span>
    <span class="s0">samples[pos_1], samples[pos_2] = samples[pos_2], samples[pos_1]</span>
    <span class="s0">index_to_samples[samples[pos_1]] = pos_1</span>
    <span class="s0">index_to_samples[samples[pos_2]] = pos_2</span>


<span class="s0">cdef class BestSplitter(Splitter):</span>
    <span class="s0">&quot;&quot;&quot;Splitter for finding the best split on dense data.&quot;&quot;&quot;</span>
    <span class="s0">cdef DensePartitioner partitioner</span>
    <span class="s0">cdef int init(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">const float64_t[:, ::1] y,</span>
        <span class="s0">const float64_t[:] sample_weight,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">) except -1:</span>
        <span class="s0">Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)</span>
        <span class="s0">self.partitioner = DensePartitioner(</span>
            <span class="s0">X, self.samples, self.feature_values, missing_values_in_feature_mask</span>
        <span class="s0">)</span>

    <span class="s0">cdef int node_split(</span>
            <span class="s0">self,</span>
            <span class="s0">ParentInfo* parent_record,</span>
            <span class="s0">SplitRecord* split,</span>
    <span class="s0">) except -1 nogil:</span>
        <span class="s0">return node_split_best(</span>
            <span class="s0">self,</span>
            <span class="s0">self.partitioner,</span>
            <span class="s0">self.criterion,</span>
            <span class="s0">split,</span>
            <span class="s0">parent_record,</span>
            <span class="s0">self.with_monotonic_cst,</span>
            <span class="s0">self.monotonic_cst,</span>
        <span class="s0">)</span>

<span class="s0">cdef class BestSparseSplitter(Splitter):</span>
    <span class="s0">&quot;&quot;&quot;Splitter for finding the best split, using the sparse data.&quot;&quot;&quot;</span>
    <span class="s0">cdef SparsePartitioner partitioner</span>
    <span class="s0">cdef int init(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">const float64_t[:, ::1] y,</span>
        <span class="s0">const float64_t[:] sample_weight,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">) except -1:</span>
        <span class="s0">Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)</span>
        <span class="s0">self.partitioner = SparsePartitioner(</span>
            <span class="s0">X, self.samples, self.n_samples, self.feature_values, missing_values_in_feature_mask</span>
        <span class="s0">)</span>

    <span class="s0">cdef int node_split(</span>
            <span class="s0">self,</span>
            <span class="s0">ParentInfo* parent_record,</span>
            <span class="s0">SplitRecord* split,</span>
    <span class="s0">) except -1 nogil:</span>
        <span class="s0">return node_split_best(</span>
            <span class="s0">self,</span>
            <span class="s0">self.partitioner,</span>
            <span class="s0">self.criterion,</span>
            <span class="s0">split,</span>
            <span class="s0">parent_record,</span>
            <span class="s0">self.with_monotonic_cst,</span>
            <span class="s0">self.monotonic_cst,</span>
        <span class="s0">)</span>

<span class="s0">cdef class RandomSplitter(Splitter):</span>
    <span class="s0">&quot;&quot;&quot;Splitter for finding the best random split on dense data.&quot;&quot;&quot;</span>
    <span class="s0">cdef DensePartitioner partitioner</span>
    <span class="s0">cdef int init(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">const float64_t[:, ::1] y,</span>
        <span class="s0">const float64_t[:] sample_weight,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">) except -1:</span>
        <span class="s0">Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)</span>
        <span class="s0">self.partitioner = DensePartitioner(</span>
            <span class="s0">X, self.samples, self.feature_values, missing_values_in_feature_mask</span>
        <span class="s0">)</span>

    <span class="s0">cdef int node_split(</span>
            <span class="s0">self,</span>
            <span class="s0">ParentInfo* parent_record,</span>
            <span class="s0">SplitRecord* split,</span>
    <span class="s0">) except -1 nogil:</span>
        <span class="s0">return node_split_random(</span>
            <span class="s0">self,</span>
            <span class="s0">self.partitioner,</span>
            <span class="s0">self.criterion,</span>
            <span class="s0">split,</span>
            <span class="s0">parent_record,</span>
            <span class="s0">self.with_monotonic_cst,</span>
            <span class="s0">self.monotonic_cst,</span>
        <span class="s0">)</span>

<span class="s0">cdef class RandomSparseSplitter(Splitter):</span>
    <span class="s0">&quot;&quot;&quot;Splitter for finding the best random split, using the sparse data.&quot;&quot;&quot;</span>
    <span class="s0">cdef SparsePartitioner partitioner</span>
    <span class="s0">cdef int init(</span>
        <span class="s0">self,</span>
        <span class="s0">object X,</span>
        <span class="s0">const float64_t[:, ::1] y,</span>
        <span class="s0">const float64_t[:] sample_weight,</span>
        <span class="s0">const unsigned char[::1] missing_values_in_feature_mask,</span>
    <span class="s0">) except -1:</span>
        <span class="s0">Splitter.init(self, X, y, sample_weight, missing_values_in_feature_mask)</span>
        <span class="s0">self.partitioner = SparsePartitioner(</span>
            <span class="s0">X, self.samples, self.n_samples, self.feature_values, missing_values_in_feature_mask</span>
        <span class="s0">)</span>
    <span class="s0">cdef int node_split(</span>
            <span class="s0">self,</span>
            <span class="s0">ParentInfo* parent_record,</span>
            <span class="s0">SplitRecord* split,</span>
    <span class="s0">) except -1 nogil:</span>
        <span class="s0">return node_split_random(</span>
            <span class="s0">self,</span>
            <span class="s0">self.partitioner,</span>
            <span class="s0">self.criterion,</span>
            <span class="s0">split,</span>
            <span class="s0">parent_record,</span>
            <span class="s0">self.with_monotonic_cst,</span>
            <span class="s0">self.monotonic_cst,</span>
        <span class="s0">)</span>
</pre>
</body>
</html>