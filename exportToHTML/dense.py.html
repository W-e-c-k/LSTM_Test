<html>
<head>
<title>dense.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
dense.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">ml_dtypes</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">activations</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">constraints</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">dtype_policies</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">initializers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">quantizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">regularizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">input_spec </span><span class="s0">import </span><span class="s1">InputSpec</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">layer </span><span class="s0">import </span><span class="s1">Layer</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.layers.Dense&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">Dense</span><span class="s2">(</span><span class="s1">Layer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Just your regular densely-connected NN layer. 
 
    `Dense` implements the operation: 
    `output = activation(dot(input, kernel) + bias)` 
    where `activation` is the element-wise activation function 
    passed as the `activation` argument, `kernel` is a weights matrix 
    created by the layer, and `bias` is a bias vector created by the layer 
    (only applicable if `use_bias` is `True`). 
 
    Note: If the input to the layer has a rank greater than 2, `Dense` 
    computes the dot product between the `inputs` and the `kernel` along the 
    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`). 
    For example, if input has dimensions `(batch_size, d0, d1)`, then we create 
    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2 
    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are 
    `batch_size * d0` such sub-tensors). The output in this case will have 
    shape `(batch_size, d0, units)`. 
 
    Args: 
        units: Positive integer, dimensionality of the output space. 
        activation: Activation function to use. 
            If you don't specify anything, no activation is applied 
            (ie. &quot;linear&quot; activation: `a(x) = x`). 
        use_bias: Boolean, whether the layer uses a bias vector. 
        kernel_initializer: Initializer for the `kernel` weights matrix. 
        bias_initializer: Initializer for the bias vector. 
        kernel_regularizer: Regularizer function applied to 
            the `kernel` weights matrix. 
        bias_regularizer: Regularizer function applied to the bias vector. 
        activity_regularizer: Regularizer function applied to 
            the output of the layer (its &quot;activation&quot;). 
        kernel_constraint: Constraint function applied to 
            the `kernel` weights matrix. 
        bias_constraint: Constraint function applied to the bias vector. 
        lora_rank: Optional integer. If set, the layer's forward pass 
            will implement LoRA (Low-Rank Adaptation) 
            with the provided rank. LoRA sets the layer's kernel 
            to non-trainable and replaces it with a delta over the 
            original kernel, obtained via multiplying two lower-rank 
            trainable matrices. This can be useful to reduce the 
            computation cost of fine-tuning large dense layers. 
            You can also enable LoRA on an existing 
            `Dense` layer by calling `layer.enable_lora(rank)`. 
 
    Input shape: 
        N-D tensor with shape: `(batch_size, ..., input_dim)`. 
        The most common situation would be 
        a 2D input with shape `(batch_size, input_dim)`. 
 
    Output shape: 
        N-D tensor with shape: `(batch_size, ..., units)`. 
        For instance, for a 2D input with shape `(batch_size, input_dim)`, 
        the output would have shape `(batch_size, units)`. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">units</span><span class="s2">,</span>
        <span class="s1">activation</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_bias</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">kernel_initializer</span><span class="s2">=</span><span class="s3">&quot;glorot_uniform&quot;</span><span class="s2">,</span>
        <span class="s1">bias_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">kernel_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">kernel_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">lora_rank</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s1">activity_regularizer</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">units </span><span class="s2">= </span><span class="s1">units</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">activation </span><span class="s2">= </span><span class="s1">activations</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">activation</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s2">= </span><span class="s1">use_bias</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_initializer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_initializer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint </span><span class="s2">= </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_constraint</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint </span><span class="s2">= </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_constraint</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank </span><span class="s2">= </span><span class="s1">lora_rank</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled </span><span class="s2">= </span><span class="s0">False</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">input_spec </span><span class="s2">= </span><span class="s1">InputSpec</span><span class="s2">(</span><span class="s1">min_ndim</span><span class="s2">=</span><span class="s5">2</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">supports_masking </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s1">input_dim </span><span class="s2">= </span><span class="s1">input_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">quantized_build</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">!= </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s6"># If the layer is quantized to int8, `self._kernel` will be added</span>
            <span class="s6"># in `self._int8_build`. Therefore, we skip it here.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kernel&quot;</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=(</span><span class="s1">input_dim</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span><span class="s2">),</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer</span><span class="s2">,</span>
                <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span><span class="s2">,</span>
                <span class="s1">constraint</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">bias </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;bias&quot;</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span><span class="s2">,),</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer</span><span class="s2">,</span>
                <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer</span><span class="s2">,</span>
                <span class="s1">constraint</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">bias </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">input_spec </span><span class="s2">= </span><span class="s1">InputSpec</span><span class="s2">(</span><span class="s1">min_ndim</span><span class="s2">=</span><span class="s5">2</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">={-</span><span class="s5">1</span><span class="s2">: </span><span class="s1">input_dim</span><span class="s2">})</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">enable_lora</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">property</span>
    <span class="s0">def </span><span class="s1">kernel</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">AttributeError</span><span class="s2">(</span>
                <span class="s3">&quot;You must build the layer before accessing `kernel`.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b</span>
            <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">def </span><span class="s1">compute_output_shape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s1">output_shape </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">)</span>
        <span class="s1">output_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">] = </span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span>
        <span class="s0">return </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">output_shape</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">enable_lora</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">rank</span><span class="s2">, </span><span class="s1">a_initializer</span><span class="s2">=</span><span class="s3">&quot;he_uniform&quot;</span><span class="s2">, </span><span class="s1">b_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span>
    <span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Lora is incompatible with kernel constraints. &quot;</span>
                <span class="s3">&quot;In order to enable lora on this layer, remove the &quot;</span>
                <span class="s3">&quot;`kernel_constraint` argument.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Cannot enable lora on a layer that isn't yet built.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;lora is already enabled. &quot;</span>
                <span class="s3">&quot;This can only be done once per layer.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">unlock</span><span class="s2">()</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;lora_kernel_a&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">0</span><span class="s2">], </span><span class="s1">rank</span><span class="s2">),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">a_initializer</span><span class="s2">),</span>
            <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;lora_kernel_b&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">rank</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">b_initializer</span><span class="s2">),</span>
            <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">.</span><span class="s1">trainable </span><span class="s2">= </span><span class="s0">False</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">lock</span><span class="s2">()</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank </span><span class="s2">= </span><span class="s1">rank</span>

    <span class="s0">def </span><span class="s1">save_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s6"># Do nothing if the layer isn't yet built</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s6"># The keys of the `store` will be saved as determined because the</span>
        <span class="s6"># default ordering will change after quantization</span>
        <span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_kernel_with_merged_lora</span><span class="s2">()</span>
        <span class="s1">target_variables </span><span class="s2">= [</span><span class="s1">kernel_value</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">:</span>
            <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">kernel_scale</span><span class="s2">)</span>
            <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;float8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_amax_history</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_amax_history</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_amax_history</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">target_variables</span><span class="s2">):</span>
            <span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)] = </span><span class="s1">variable</span>

    <span class="s0">def </span><span class="s1">load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_load_own_variables</span><span class="s2">(</span><span class="s1">store</span><span class="s2">)</span>
        <span class="s6"># Do nothing if the layer isn't yet built</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s6"># The keys of the `store` will be saved as determined because the</span>
        <span class="s6"># default ordering will change after quantization</span>
        <span class="s1">target_variables </span><span class="s2">= [</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">:</span>
            <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">)</span>
            <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;float8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_amax_history</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_amax_history</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_scale</span><span class="s2">)</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_amax_history</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">target_variables</span><span class="s2">):</span>
            <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)])</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">base_config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s1">config </span><span class="s2">= {</span>
            <span class="s3">&quot;units&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span><span class="s2">,</span>
            <span class="s3">&quot;activation&quot;</span><span class="s2">: </span><span class="s1">activations</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation</span><span class="s2">),</span>
            <span class="s3">&quot;use_bias&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">,</span>
            <span class="s3">&quot;kernel_initializer&quot;</span><span class="s2">: </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;bias_initializer&quot;</span><span class="s2">: </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer</span><span class="s2">),</span>
            <span class="s3">&quot;kernel_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;bias_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer</span><span class="s2">),</span>
            <span class="s3">&quot;kernel_constraint&quot;</span><span class="s2">: </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint</span><span class="s2">),</span>
            <span class="s3">&quot;bias_constraint&quot;</span><span class="s2">: </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint</span><span class="s2">),</span>
        <span class="s2">}</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">:</span>
            <span class="s1">config</span><span class="s2">[</span><span class="s3">&quot;lora_rank&quot;</span><span class="s2">] = </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span>
        <span class="s0">return </span><span class="s2">{**</span><span class="s1">base_config</span><span class="s2">, **</span><span class="s1">config</span><span class="s2">}</span>

    <span class="s0">def </span><span class="s1">_check_load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s1">all_vars </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables </span><span class="s2">+ </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_non_trainable_variables</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">()) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">):</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">) == </span><span class="s5">0 </span><span class="s0">and not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s3">f&quot;Layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">' was never built &quot;</span>
                    <span class="s3">&quot;and thus it doesn't have any variables. &quot;</span>
                    <span class="s3">f&quot;However the weights file lists </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">())</span><span class="s0">} </span><span class="s3">&quot;</span>
                    <span class="s3">&quot;variables for this layer.</span><span class="s0">\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;In most cases, this error indicates that either:</span><span class="s0">\n\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;1. The layer is owned by a parent layer that &quot;</span>
                    <span class="s3">&quot;implements a `build()` method, but calling the &quot;</span>
                    <span class="s3">&quot;parent's `build()` method did NOT create the state of &quot;</span>
                    <span class="s3">f&quot;the child layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">'. A `build()` method &quot;</span>
                    <span class="s3">&quot;must create ALL state for the layer, including &quot;</span>
                    <span class="s3">&quot;the state of any children layers.</span><span class="s0">\n\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;2. You need to implement &quot;</span>
                    <span class="s3">&quot;the `def build_from_config(self, config)` method &quot;</span>
                    <span class="s3">f&quot;on layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">', to specify how to rebuild &quot;</span>
                    <span class="s3">&quot;it during loading. &quot;</span>
                    <span class="s3">&quot;In this case, you might also want to implement the &quot;</span>
                    <span class="s3">&quot;method that generates the build config at saving time, &quot;</span>
                    <span class="s3">&quot;`def get_build_config(self)`. &quot;</span>
                    <span class="s3">&quot;The method `build_from_config()` is meant &quot;</span>
                    <span class="s3">&quot;to create the state &quot;</span>
                    <span class="s3">&quot;of the layer (i.e. its variables) upon deserialization.&quot;</span><span class="s2">,</span>
                <span class="s2">)</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">f&quot;Layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">' expected </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">)</span><span class="s0">} </span><span class="s3">variables, &quot;</span>
                <span class="s3">&quot;but received &quot;</span>
                <span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">())</span><span class="s0">} </span><span class="s3">variables during loading. &quot;</span>
                <span class="s3">f&quot;Expected: </span><span class="s0">{</span><span class="s2">[</span><span class="s1">v</span><span class="s2">.</span><span class="s1">name </span><span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">all_vars</span><span class="s2">]</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>

    <span class="s6"># Quantization-related (int8 and float8) methods</span>

    <span class="s0">def </span><span class="s1">quantized_build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s1">input_dim </span><span class="s2">= </span><span class="s1">input_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>
            <span class="s1">kernel_shape </span><span class="s2">= (</span><span class="s1">input_dim</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_int8_build</span><span class="s2">(</span><span class="s1">kernel_shape</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;float8&quot;</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_float8_build</span><span class="s2">()</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">mode</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_int8_build</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">kernel_shape</span><span class="s2">,</span>
        <span class="s1">kernel_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">kernel_scale_initializer</span><span class="s2">=</span><span class="s3">&quot;ones&quot;</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_quantizer </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">AbsMaxQuantizer</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kernel&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=</span><span class="s1">kernel_shape</span><span class="s2">,</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">kernel_initializer</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;int8&quot;</span><span class="s2">,</span>
            <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kernel_scale&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">units</span><span class="s2">,),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">kernel_scale_initializer</span><span class="s2">,</span>
            <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_is_quantized </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_float8_build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">dtype_policies </span><span class="s0">import </span><span class="s1">QuantizedFloat8DTypePolicy</span>

        <span class="s6"># If `self.dtype_policy` is not QuantizedFloat8DTypePolicy, then set</span>
        <span class="s6"># `amax_history_length` to its default value.</span>
        <span class="s1">amax_history_length </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">,</span>
            <span class="s3">&quot;amax_history_length&quot;</span><span class="s2">,</span>
            <span class="s1">QuantizedFloat8DTypePolicy</span><span class="s2">.</span><span class="s1">default_amax_history_length</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s6"># We set `trainable=True` because we will use the gradients to overwrite</span>
        <span class="s6"># these variables</span>
        <span class="s1">scale_kwargs </span><span class="s2">= {</span>
            <span class="s3">&quot;shape&quot;</span><span class="s2">: (),</span>
            <span class="s3">&quot;initializer&quot;</span><span class="s2">: </span><span class="s3">&quot;ones&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s3">&quot;float32&quot;</span><span class="s2">,  </span><span class="s6"># Always be float32</span>
            <span class="s3">&quot;trainable&quot;</span><span class="s2">: </span><span class="s0">True</span><span class="s2">,</span>
            <span class="s3">&quot;autocast&quot;</span><span class="s2">: </span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">}</span>
        <span class="s1">amax_history_kwargs </span><span class="s2">= {</span>
            <span class="s3">&quot;shape&quot;</span><span class="s2">: (</span><span class="s1">amax_history_length</span><span class="s2">,),</span>
            <span class="s3">&quot;initializer&quot;</span><span class="s2">: </span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s3">&quot;float32&quot;</span><span class="s2">,  </span><span class="s6"># Always be float32</span>
            <span class="s3">&quot;trainable&quot;</span><span class="s2">: </span><span class="s0">True</span><span class="s2">,</span>
            <span class="s3">&quot;autocast&quot;</span><span class="s2">: </span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">}</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;inputs_scale&quot;</span><span class="s2">, **</span><span class="s1">scale_kwargs</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_amax_history </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;inputs_amax_history&quot;</span><span class="s2">, **</span><span class="s1">amax_history_kwargs</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kernel_scale&quot;</span><span class="s2">, **</span><span class="s1">scale_kwargs</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_amax_history </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kernel_amax_history&quot;</span><span class="s2">, **</span><span class="s1">amax_history_kwargs</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;outputs_grad_scale&quot;</span><span class="s2">, **</span><span class="s1">scale_kwargs</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_amax_history </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;outputs_grad_amax_history&quot;</span><span class="s2">, **</span><span class="s1">amax_history_kwargs</span>
        <span class="s2">)</span>
        <span class="s6"># We need to set `overwrite_with_gradient=True` to instruct the</span>
        <span class="s6"># optimizer to directly overwrite these variables with their computed</span>
        <span class="s6"># gradients during training</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_scale</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_amax_history</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_amax_history</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_scale</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_amax_history</span><span class="s2">.</span><span class="s1">overwrite_with_gradient </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_is_quantized </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_int8_call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s2">@</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">custom_gradient</span>
        <span class="s0">def </span><span class="s1">matmul_with_inputs_gradient</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">kernel</span><span class="s2">, </span><span class="s1">kernel_scale</span><span class="s2">):</span>
            <span class="s0">def </span><span class="s1">grad_fn</span><span class="s2">(*</span><span class="s1">args</span><span class="s2">, </span><span class="s1">upstream</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
                <span class="s0">if </span><span class="s1">upstream </span><span class="s0">is None</span><span class="s2">:</span>
                    <span class="s2">(</span><span class="s1">upstream</span><span class="s2">,) = </span><span class="s1">args</span>
                <span class="s1">float_kernel </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">kernel</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">),</span>
                    <span class="s1">kernel_scale</span><span class="s2">,</span>
                <span class="s2">)</span>
                <span class="s1">inputs_grad </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">upstream</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">float_kernel</span><span class="s2">))</span>
                <span class="s0">return </span><span class="s2">(</span><span class="s1">inputs_grad</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">)</span>

            <span class="s1">inputs</span><span class="s2">, </span><span class="s1">inputs_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_quantizer</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">kernel</span><span class="s2">)</span>
            <span class="s6"># De-scale outputs</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span><span class="s1">inputs_scale</span><span class="s2">, </span><span class="s1">kernel_scale</span><span class="s2">))</span>
            <span class="s0">return </span><span class="s1">x</span><span class="s2">, </span><span class="s1">grad_fn</span>

        <span class="s1">x </span><span class="s2">= </span><span class="s1">matmul_with_inputs_gradient</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">),</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">),</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">lora_x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a</span><span class="s2">)</span>
            <span class="s1">lora_x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">lora_x</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">lora_x</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">def </span><span class="s1">_float8_call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">NotImplementedError</span><span class="s2">(</span>
                <span class="s3">&quot;Currently, `_float8_call` doesn't support LoRA&quot;</span>
            <span class="s2">)</span>

        <span class="s2">@</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">custom_gradient</span>
        <span class="s0">def </span><span class="s1">quantized_dequantize_inputs</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">amax_history</span><span class="s2">):</span>
            <span class="s0">if </span><span class="s1">training</span><span class="s2">:</span>
                <span class="s1">new_scale </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">compute_float8_scale</span><span class="s2">(</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">max</span><span class="s2">(</span><span class="s1">amax_history</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">),</span>
                    <span class="s1">scale</span><span class="s2">,</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span>
                        <span class="s1">float</span><span class="s2">(</span><span class="s1">ml_dtypes</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s3">&quot;float8_e4m3fn&quot;</span><span class="s2">).</span><span class="s1">max</span><span class="s2">), </span><span class="s3">&quot;float32&quot;</span>
                    <span class="s2">),</span>
                <span class="s2">)</span>
                <span class="s1">new_amax_history </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">compute_float8_amax_history</span><span class="s2">(</span>
                    <span class="s1">inputs</span><span class="s2">, </span><span class="s1">amax_history</span>
                <span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">new_scale </span><span class="s2">= </span><span class="s0">None</span>
                <span class="s1">new_amax_history </span><span class="s2">= </span><span class="s0">None</span>
            <span class="s1">qdq_inputs </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">quantize_and_dequantize</span><span class="s2">(</span>
                <span class="s1">inputs</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s3">&quot;float8_e4m3fn&quot;</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span>
            <span class="s2">)</span>

            <span class="s0">def </span><span class="s1">grad</span><span class="s2">(*</span><span class="s1">args</span><span class="s2">, </span><span class="s1">upstream</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
                <span class="s0">if </span><span class="s1">upstream </span><span class="s0">is None</span><span class="s2">:</span>
                    <span class="s2">(</span><span class="s1">upstream</span><span class="s2">,) = </span><span class="s1">args</span>
                <span class="s0">return </span><span class="s1">upstream</span><span class="s2">, </span><span class="s1">new_scale</span><span class="s2">, </span><span class="s1">new_amax_history</span>

            <span class="s0">return </span><span class="s1">qdq_inputs</span><span class="s2">, </span><span class="s1">grad</span>

        <span class="s2">@</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">custom_gradient</span>
        <span class="s0">def </span><span class="s1">quantized_dequantize_outputs</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s1">amax_history</span><span class="s2">):</span>
            <span class="s4">&quot;&quot;&quot;Quantize-dequantize the output gradient but not the output.&quot;&quot;&quot;</span>

            <span class="s0">def </span><span class="s1">grad</span><span class="s2">(*</span><span class="s1">args</span><span class="s2">, </span><span class="s1">upstream</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
                <span class="s0">if </span><span class="s1">upstream </span><span class="s0">is None</span><span class="s2">:</span>
                    <span class="s2">(</span><span class="s1">upstream</span><span class="s2">,) = </span><span class="s1">args</span>
                <span class="s1">new_scale </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">compute_float8_scale</span><span class="s2">(</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">max</span><span class="s2">(</span><span class="s1">amax_history</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">),</span>
                    <span class="s1">scale</span><span class="s2">,</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span>
                        <span class="s1">float</span><span class="s2">(</span><span class="s1">ml_dtypes</span><span class="s2">.</span><span class="s1">finfo</span><span class="s2">(</span><span class="s3">&quot;float8_e5m2&quot;</span><span class="s2">).</span><span class="s1">max</span><span class="s2">), </span><span class="s3">&quot;float32&quot;</span>
                    <span class="s2">),</span>
                <span class="s2">)</span>
                <span class="s1">qdq_upstream </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">quantize_and_dequantize</span><span class="s2">(</span>
                    <span class="s1">upstream</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">, </span><span class="s3">&quot;float8_e5m2&quot;</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span>
                <span class="s2">)</span>
                <span class="s1">new_amax_history </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">compute_float8_amax_history</span><span class="s2">(</span>
                    <span class="s1">upstream</span><span class="s2">, </span><span class="s1">amax_history</span>
                <span class="s2">)</span>
                <span class="s0">return </span><span class="s1">qdq_upstream</span><span class="s2">, </span><span class="s1">new_scale</span><span class="s2">, </span><span class="s1">new_amax_history</span>

            <span class="s0">return </span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">grad</span>

        <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span>
            <span class="s1">quantized_dequantize_inputs</span><span class="s2">(</span>
                <span class="s1">inputs</span><span class="s2">,</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_scale</span><span class="s2">),</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">inputs_amax_history</span><span class="s2">),</span>
            <span class="s2">),</span>
            <span class="s1">quantized_dequantize_inputs</span><span class="s2">(</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">),</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span><span class="s2">),</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_amax_history</span><span class="s2">),</span>
            <span class="s2">),</span>
        <span class="s2">)</span>
        <span class="s6"># `quantized_dequantize_outputs` is placed immediately after</span>
        <span class="s6"># `ops.matmul` for the sake of pattern matching in gemm_rewrite. That</span>
        <span class="s6"># way, the qdq will be adjacent to the corresponding matmul_bprop in the</span>
        <span class="s6"># bprop.</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">quantized_dequantize_outputs</span><span class="s2">(</span>
            <span class="s1">x</span><span class="s2">,</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_scale</span><span class="s2">),</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">outputs_grad_amax_history</span><span class="s2">),</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s6"># Under non-mixed precision cases, F32 bias has to be converted to</span>
            <span class="s6"># BF16 first to get the biasAdd fusion support. ref. PR</span>
            <span class="s6"># https://github.com/tensorflow/tensorflow/pull/60306</span>
            <span class="s1">bias </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">compute_dtype </span><span class="s2">== </span><span class="s3">&quot;float32&quot;</span><span class="s2">:</span>
                <span class="s1">bias_bf16 </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">bias</span><span class="s2">, </span><span class="s3">&quot;bfloat16&quot;</span><span class="s2">)</span>
                <span class="s1">bias </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">bias_bf16</span><span class="s2">, </span><span class="s1">bias</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">bias</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">x</span>

    <span class="s0">def </span><span class="s1">quantize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">, </span><span class="s1">type_check</span><span class="s2">=</span><span class="s0">True</span><span class="s2">):</span>
        <span class="s6"># Prevent quantization of the subclasses</span>
        <span class="s0">if </span><span class="s1">type_check </span><span class="s0">and </span><span class="s2">(</span><span class="s1">type</span><span class="s2">(</span><span class="s1">self</span><span class="s2">) </span><span class="s0">is not </span><span class="s1">Dense</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_not_implemented_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantize</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s6"># Quantize `self._kernel` to int8 and compute corresponding scale</span>
            <span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">abs_max_quantize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">to_numpy</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
            <span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">kernel_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
            <span class="s1">kernel_shape </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
            <span class="s0">del </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span>
            <span class="s6"># Utilize a lambda expression as an initializer to prevent adding a</span>
            <span class="s6"># large constant to the computation graph.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_int8_build</span><span class="s2">(</span>
                <span class="s1">kernel_shape</span><span class="s2">,</span>
                <span class="s0">lambda </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">kernel_value</span><span class="s2">,</span>
                <span class="s0">lambda </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">kernel_scale</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;float8&quot;</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_float8_build</span><span class="s2">()</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">mode</span><span class="s2">)</span>

        <span class="s6"># Set new dtype policy</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">policy </span><span class="s2">= </span><span class="s1">dtype_policies</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">mode</span><span class="s0">}</span><span class="s3">_from_</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">&quot;</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy </span><span class="s2">= </span><span class="s1">policy</span>

    <span class="s0">def </span><span class="s1">_get_kernel_with_merged_lora</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">kernel_value </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_kernel</span>
            <span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_scale</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
                <span class="s6"># Dequantize &amp; quantize to merge lora weights into int8 kernel</span>
                <span class="s6"># Note that this is a lossy compression</span>
                <span class="s1">kernel_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">kernel_scale</span><span class="s2">)</span>
                <span class="s1">kernel_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span>
                    <span class="s1">kernel_value</span><span class="s2">,</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_a</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_kernel_b</span><span class="s2">),</span>
                <span class="s2">)</span>
                <span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">abs_max_quantize</span><span class="s2">(</span>
                    <span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">, </span><span class="s1">to_numpy</span><span class="s2">=</span><span class="s0">True</span>
                <span class="s2">)</span>
                <span class="s1">kernel_scale </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">kernel_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">kernel_value</span><span class="s2">, </span><span class="s1">kernel_scale</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel</span><span class="s2">, </span><span class="s0">None</span>
</pre>
</body>
</html>