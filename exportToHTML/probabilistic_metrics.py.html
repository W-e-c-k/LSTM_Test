<html>
<head>
<title>probabilistic_metrics.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
probabilistic_metrics.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">binary_crossentropy</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">categorical_crossentropy</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">kl_divergence</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">poisson</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">sparse_categorical_crossentropy</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">metrics </span><span class="s0">import </span><span class="s1">reduction_metrics</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.KLDivergence&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">KLDivergence</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes Kullback-Leibler divergence metric between `y_true` and 
    `y_pred`. 
 
    Formula: 
 
    ```python 
    metric = y_true * log(y_true / y_pred) 
    ``` 
 
    `y_true` and `y_pred` are expected to be probability 
    distributions, with values between 0 and 1. They will get 
    clipped to the `[0, 1]` range. 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Examples: 
 
    &gt;&gt;&gt; m = keras.metrics.KLDivergence() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) 
    &gt;&gt;&gt; m.result() 
    0.45814306 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.9162892 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile(optimizer='sgd', 
                  loss='mse', 
                  metrics=[keras.metrics.KLDivergence()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;kl_divergence&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">fn</span><span class="s2">=</span><span class="s1">kl_divergence</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.Poisson&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">Poisson</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the Poisson metric between `y_true` and `y_pred`. 
 
    Formula: 
 
    ```python 
    metric = y_pred - y_true * log(y_pred) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.Poisson() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.49999997 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.99999994 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile(optimizer='sgd', 
                  loss='mse', 
                  metrics=[keras.metrics.Poisson()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;poisson&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">fn</span><span class="s2">=</span><span class="s1">poisson</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.BinaryCrossentropy&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">BinaryCrossentropy</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the crossentropy metric between the labels and predictions. 
 
    This is the crossentropy metric class to be used when there are only two 
    label classes (0 and 1). 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
        from_logits: (Optional) Whether output is expected 
            to be a logits tensor. By default, we consider 
            that output encodes a probability distribution. 
        label_smoothing: (Optional) Float in `[0, 1]`. 
            When &gt; 0, label values are smoothed, 
            meaning the confidence on label values are relaxed. 
            e.g. `label_smoothing=0.2` means that we will use 
            a value of 0.1 for label &quot;0&quot; and 0.9 for label &quot;1&quot;. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.BinaryCrossentropy() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]]) 
    &gt;&gt;&gt; m.result() 
    0.81492424 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.9162905 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.BinaryCrossentropy()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;binary_crossentropy&quot;</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">label_smoothing</span><span class="s2">=</span><span class="s5">0</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">binary_crossentropy</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s1">from_logits</span><span class="s2">=</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s1">label_smoothing</span><span class="s2">=</span><span class="s1">label_smoothing</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">from_logits</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">label_smoothing </span><span class="s2">= </span><span class="s1">label_smoothing</span>
        <span class="s6"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span>
            <span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s3">&quot;from_logits&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s3">&quot;label_smoothing&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">label_smoothing</span><span class="s2">,</span>
        <span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.CategoricalCrossentropy&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">CategoricalCrossentropy</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the crossentropy metric between the labels and predictions. 
 
    This is the crossentropy metric class to be used when there are multiple 
    label classes (2 or more). It assumes that labels are one-hot encoded, 
    e.g., when labels values are `[2, 0, 1]`, then 
    `y_true` is `[[0, 0, 1], [1, 0, 0], [0, 1, 0]]`. 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
        from_logits: (Optional) Whether output is expected to be 
            a logits tensor. By default, we consider that output 
            encodes a probability distribution. 
        label_smoothing: (Optional) Float in `[0, 1]`. 
            When &gt; 0, label values are smoothed, meaning the confidence 
            on label values are relaxed. e.g. `label_smoothing=0.2` means 
            that we will use a value of 0.1 for label 
            &quot;0&quot; and 0.9 for label &quot;1&quot;. 
        axis: (Optional) Defaults to `-1`. 
            The dimension along which entropy is computed. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; # EPSILON = 1e-7, y = y_true, y` = y_pred 
    &gt;&gt;&gt; # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON) 
    &gt;&gt;&gt; # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]] 
    &gt;&gt;&gt; # xent = -sum(y * log(y'), axis = -1) 
    &gt;&gt;&gt; #      = -((log 0.95), (log 0.1)) 
    &gt;&gt;&gt; #      = [0.051, 2.302] 
    &gt;&gt;&gt; # Reduced xent = (0.051 + 2.302) / 2 
    &gt;&gt;&gt; m = keras.metrics.CategoricalCrossentropy() 
    &gt;&gt;&gt; m.update_state([[0, 1, 0], [0, 0, 1]], 
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) 
    &gt;&gt;&gt; m.result() 
    1.1769392 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1, 0], [0, 0, 1]], 
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], 
    ...                sample_weight=np.array([0.3, 0.7])) 
    &gt;&gt;&gt; m.result() 
    1.6271976 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.CategoricalCrossentropy()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;categorical_crossentropy&quot;</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">label_smoothing</span><span class="s2">=</span><span class="s5">0</span><span class="s2">,</span>
        <span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">categorical_crossentropy</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s1">from_logits</span><span class="s2">=</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s1">label_smoothing</span><span class="s2">=</span><span class="s1">label_smoothing</span><span class="s2">,</span>
            <span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">from_logits</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">label_smoothing </span><span class="s2">= </span><span class="s1">label_smoothing</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">axis </span><span class="s2">= </span><span class="s1">axis</span>
        <span class="s6"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span>
            <span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s3">&quot;from_logits&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s3">&quot;label_smoothing&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">label_smoothing</span><span class="s2">,</span>
            <span class="s3">&quot;axis&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s2">,</span>
        <span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.SparseCategoricalCrossentropy&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">SparseCategoricalCrossentropy</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the crossentropy metric between the labels and predictions. 
 
    Use this crossentropy metric when there are two or more label classes. 
    It expects labels to be provided as integers. If you want to provide labels 
    that are one-hot encoded, please use the `CategoricalCrossentropy` 
    metric instead. 
 
    There should be `num_classes` floating point values per feature for `y_pred` 
    and a single floating point value per feature for `y_true`. 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
        from_logits: (Optional) Whether output is expected 
            to be a logits tensor. By default, we consider that output 
            encodes a probability distribution. 
        axis: (Optional) Defaults to `-1`. 
            The dimension along which entropy is computed. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]] 
    &gt;&gt;&gt; # logits = log(y_pred) 
    &gt;&gt;&gt; # softmax = exp(logits) / sum(exp(logits), axis=-1) 
    &gt;&gt;&gt; # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]] 
    &gt;&gt;&gt; # xent = -sum(y * log(softmax), 1) 
    &gt;&gt;&gt; # log(softmax) = [[-2.9957, -0.0513, -16.1181], 
    &gt;&gt;&gt; #                [-2.3026, -0.2231, -2.3026]] 
    &gt;&gt;&gt; # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]] 
    &gt;&gt;&gt; # xent = [0.0513, 2.3026] 
    &gt;&gt;&gt; # Reduced xent = (0.0513 + 2.3026) / 2 
    &gt;&gt;&gt; m = keras.metrics.SparseCategoricalCrossentropy() 
    &gt;&gt;&gt; m.update_state([1, 2], 
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]) 
    &gt;&gt;&gt; m.result() 
    1.1769392 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([1, 2], 
    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]], 
    ...                sample_weight=np.array([0.3, 0.7])) 
    &gt;&gt;&gt; m.result() 
    1.6271976 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.SparseCategoricalCrossentropy()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;sparse_categorical_crossentropy&quot;</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">sparse_categorical_crossentropy</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s1">from_logits</span><span class="s2">=</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">from_logits</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">axis </span><span class="s2">= </span><span class="s1">axis</span>
        <span class="s6"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span>
            <span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s3">&quot;from_logits&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">from_logits</span><span class="s2">,</span>
            <span class="s3">&quot;axis&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s2">,</span>
        <span class="s2">}</span>
</pre>
</body>
</html>