<html>
<head>
<title>embedding.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
embedding.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">constraints</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">dtype_policies</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">initializers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">quantizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">regularizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">layer </span><span class="s0">import </span><span class="s1">Layer</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.layers.Embedding&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">Embedding</span><span class="s2">(</span><span class="s1">Layer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Turns positive integers (indexes) into dense vectors of fixed size. 
 
    e.g. `[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]` 
 
    This layer can only be used on positive integer inputs of a fixed range. 
 
    Example: 
 
    &gt;&gt;&gt; model = keras.Sequential() 
    &gt;&gt;&gt; model.add(keras.layers.Embedding(1000, 64)) 
    &gt;&gt;&gt; # The model will take as input an integer matrix of size (batch, 
    &gt;&gt;&gt; # input_length), and the largest integer (i.e. word index) in the input 
    &gt;&gt;&gt; # should be no larger than 999 (vocabulary size). 
    &gt;&gt;&gt; # Now model.output_shape is (None, 10, 64), where `None` is the batch 
    &gt;&gt;&gt; # dimension. 
    &gt;&gt;&gt; input_array = np.random.randint(1000, size=(32, 10)) 
    &gt;&gt;&gt; model.compile('rmsprop', 'mse') 
    &gt;&gt;&gt; output_array = model.predict(input_array) 
    &gt;&gt;&gt; print(output_array.shape) 
    (32, 10, 64) 
 
    Args: 
        input_dim: Integer. Size of the vocabulary, 
            i.e. maximum integer index + 1. 
        output_dim: Integer. Dimension of the dense embedding. 
        embeddings_initializer: Initializer for the `embeddings` 
            matrix (see `keras.initializers`). 
        embeddings_regularizer: Regularizer function applied to 
            the `embeddings` matrix (see `keras.regularizers`). 
        embeddings_constraint: Constraint function applied to 
            the `embeddings` matrix (see `keras.constraints`). 
        mask_zero: Boolean, whether or not the input value 0 is a special 
            &quot;padding&quot; value that should be masked out. 
            This is useful when using recurrent layers which 
            may take variable length input. If this is `True`, 
            then all subsequent layers in the model need 
            to support masking or an exception will be raised. 
            If `mask_zero` is set to `True`, as a consequence, 
            index 0 cannot be used in the vocabulary (`input_dim` should 
            equal size of vocabulary + 1). 
        weights: Optional floating-point matrix of size 
            `(input_dim, output_dim)`. The initial embeddings values 
            to use. 
        lora_rank: Optional integer. If set, the layer's forward pass 
            will implement LoRA (Low-Rank Adaptation) 
            with the provided rank. LoRA sets the layer's embeddings 
            matrix to non-trainable and replaces it with a delta over the 
            original matrix, obtained via multiplying two lower-rank 
            trainable matrices. This can be useful to reduce the 
            computation cost of fine-tuning large embedding layers. 
            You can also enable LoRA on an existing 
            `Embedding` layer by calling `layer.enable_lora(rank)`. 
 
    Input shape: 
        2D tensor with shape: `(batch_size, input_length)`. 
 
    Output shape: 
        3D tensor with shape: `(batch_size, input_length, output_dim)`. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">input_dim</span><span class="s2">,</span>
        <span class="s1">output_dim</span><span class="s2">,</span>
        <span class="s1">embeddings_initializer</span><span class="s2">=</span><span class="s3">&quot;uniform&quot;</span><span class="s2">,</span>
        <span class="s1">embeddings_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">embeddings_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">mask_zero</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">weights</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">lora_rank</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">input_length </span><span class="s2">= </span><span class="s1">kwargs</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s3">&quot;input_length&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">input_length </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
                <span class="s3">&quot;Argument `input_length` is deprecated. Just remove it.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(**</span><span class="s1">kwargs</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">input_dim </span><span class="s2">= </span><span class="s1">input_dim</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">output_dim </span><span class="s2">= </span><span class="s1">output_dim</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">embeddings_initializer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">embeddings_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_constraint </span><span class="s2">= </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">embeddings_constraint</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mask_zero </span><span class="s2">= </span><span class="s1">mask_zero</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">supports_masking </span><span class="s2">= </span><span class="s1">mask_zero</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">autocast </span><span class="s2">= </span><span class="s0">False</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank </span><span class="s2">= </span><span class="s1">lora_rank</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled </span><span class="s2">= </span><span class="s0">False</span>

        <span class="s0">if </span><span class="s1">weights </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">build</span><span class="s2">()</span>
            <span class="s0">if not </span><span class="s2">(</span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">weights</span><span class="s2">, </span><span class="s1">list</span><span class="s2">) </span><span class="s0">and </span><span class="s1">len</span><span class="s2">(</span><span class="s1">weights</span><span class="s2">) == </span><span class="s5">1</span><span class="s2">):</span>
                <span class="s1">weights </span><span class="s2">= [</span><span class="s1">weights</span><span class="s2">]</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">set_weights</span><span class="s2">(</span><span class="s1">weights</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">quantized_build</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">!= </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_dim</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">output_dim</span><span class="s2">),</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_initializer</span><span class="s2">,</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;embeddings&quot;</span><span class="s2">,</span>
                <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_regularizer</span><span class="s2">,</span>
                <span class="s1">constraint</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_constraint</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">enable_lora</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">property</span>
    <span class="s0">def </span><span class="s1">embeddings</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b</span>
            <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">dtype </span><span class="s2">!= </span><span class="s3">&quot;int32&quot; </span><span class="s0">and </span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">dtype </span><span class="s2">!= </span><span class="s3">&quot;int64&quot;</span><span class="s2">:</span>
            <span class="s1">inputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s3">&quot;int32&quot;</span><span class="s2">)</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">take</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_mask</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">mask_zero</span><span class="s2">:</span>
            <span class="s0">return None</span>
        <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">not_equal</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s5">0</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_output_shape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">input_shape </span><span class="s2">+ (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">output_dim</span><span class="s2">,)</span>

    <span class="s0">def </span><span class="s1">enable_lora</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">rank</span><span class="s2">, </span><span class="s1">a_initializer</span><span class="s2">=</span><span class="s3">&quot;he_uniform&quot;</span><span class="s2">, </span><span class="s1">b_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span>
    <span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_constraint</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Lora is incompatible with embedding constraints. &quot;</span>
                <span class="s3">&quot;In order to enable lora on this layer, remove the &quot;</span>
                <span class="s3">&quot;`embeddings_constraint` argument.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Cannot enable lora on a layer that isn't yet built.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;lora is already enabled. &quot;</span>
                <span class="s3">&quot;This can only be done once per layer.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">unlock</span><span class="s2">()</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;lora_embeddings_a&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">0</span><span class="s2">], </span><span class="s1">rank</span><span class="s2">),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">a_initializer</span><span class="s2">),</span>
            <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_regularizer</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;lora_embeddings_b&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">rank</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s5">1</span><span class="s2">]),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">b_initializer</span><span class="s2">),</span>
            <span class="s1">regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_regularizer</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings</span><span class="s2">.</span><span class="s1">trainable </span><span class="s2">= </span><span class="s0">False</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">lock</span><span class="s2">()</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank </span><span class="s2">= </span><span class="s1">rank</span>

    <span class="s0">def </span><span class="s1">save_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s6"># Do nothing if the layer isn't yet built</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s6"># The keys of the `store` will be saved as determined because the</span>
        <span class="s6"># default ordering will change after quantization</span>
        <span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">embeddings_scale </span><span class="s2">= (</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_get_embeddings_with_merged_lora</span><span class="s2">()</span>
        <span class="s2">)</span>
        <span class="s1">target_variables </span><span class="s2">= [</span><span class="s1">embeddings_value</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">embeddings_scale</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">target_variables</span><span class="s2">):</span>
            <span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)] = </span><span class="s1">variable</span>

    <span class="s0">def </span><span class="s1">load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_load_own_variables</span><span class="s2">(</span><span class="s1">store</span><span class="s2">)</span>
        <span class="s6"># Do nothing if the layer isn't yet built</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s6"># The keys of the `store` will be saved as determined because the</span>
        <span class="s6"># default ordering will change after quantization</span>
        <span class="s1">target_variables </span><span class="s2">= [</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span><span class="s2">]</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
                <span class="s1">target_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_scale</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">target_variables</span><span class="s2">):</span>
            <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)])</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
            <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">base_config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s1">config </span><span class="s2">= {</span>
            <span class="s3">&quot;input_dim&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_dim</span><span class="s2">,</span>
            <span class="s3">&quot;output_dim&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">output_dim</span><span class="s2">,</span>
            <span class="s3">&quot;embeddings_initializer&quot;</span><span class="s2">: </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_initializer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;embeddings_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_regularizer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;activity_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">activity_regularizer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;embeddings_constraint&quot;</span><span class="s2">: </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_constraint</span>
            <span class="s2">),</span>
            <span class="s3">&quot;mask_zero&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">mask_zero</span><span class="s2">,</span>
        <span class="s2">}</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span><span class="s2">:</span>
            <span class="s1">config</span><span class="s2">[</span><span class="s3">&quot;lora_rank&quot;</span><span class="s2">] = </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_rank</span>
        <span class="s0">return </span><span class="s2">{**</span><span class="s1">base_config</span><span class="s2">, **</span><span class="s1">config</span><span class="s2">}</span>

    <span class="s0">def </span><span class="s1">_check_load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s1">all_vars </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables </span><span class="s2">+ </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_non_trainable_variables</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">()) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">):</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">) == </span><span class="s5">0 </span><span class="s0">and not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s3">f&quot;Layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">' was never built &quot;</span>
                    <span class="s3">&quot;and thus it doesn't have any variables. &quot;</span>
                    <span class="s3">f&quot;However the weights file lists </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">())</span><span class="s0">} </span><span class="s3">&quot;</span>
                    <span class="s3">&quot;variables for this layer.</span><span class="s0">\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;In most cases, this error indicates that either:</span><span class="s0">\n\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;1. The layer is owned by a parent layer that &quot;</span>
                    <span class="s3">&quot;implements a `build()` method, but calling the &quot;</span>
                    <span class="s3">&quot;parent's `build()` method did NOT create the state of &quot;</span>
                    <span class="s3">f&quot;the child layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">'. A `build()` method &quot;</span>
                    <span class="s3">&quot;must create ALL state for the layer, including &quot;</span>
                    <span class="s3">&quot;the state of any children layers.</span><span class="s0">\n\n</span><span class="s3">&quot;</span>
                    <span class="s3">&quot;2. You need to implement &quot;</span>
                    <span class="s3">&quot;the `def build_from_config(self, config)` method &quot;</span>
                    <span class="s3">f&quot;on layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">', to specify how to rebuild &quot;</span>
                    <span class="s3">&quot;it during loading. &quot;</span>
                    <span class="s3">&quot;In this case, you might also want to implement the &quot;</span>
                    <span class="s3">&quot;method that generates the build config at saving time, &quot;</span>
                    <span class="s3">&quot;`def get_build_config(self)`. &quot;</span>
                    <span class="s3">&quot;The method `build_from_config()` is meant &quot;</span>
                    <span class="s3">&quot;to create the state &quot;</span>
                    <span class="s3">&quot;of the layer (i.e. its variables) upon deserialization.&quot;</span><span class="s2">,</span>
                <span class="s2">)</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">f&quot;Layer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">' expected </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">all_vars</span><span class="s2">)</span><span class="s0">} </span><span class="s3">variables, &quot;</span>
                <span class="s3">&quot;but received &quot;</span>
                <span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">())</span><span class="s0">} </span><span class="s3">variables during loading. &quot;</span>
                <span class="s3">f&quot;Expected: </span><span class="s0">{</span><span class="s2">[</span><span class="s1">v</span><span class="s2">.</span><span class="s1">name </span><span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">all_vars</span><span class="s2">]</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>

    <span class="s3">&quot;&quot;&quot;Quantization-related (int8) methods&quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">NotImplementedError</span><span class="s2">(</span>
            <span class="s3">&quot;Invalid quantization mode. Expected 'int8'. &quot;</span>
            <span class="s3">f&quot;Received: quantization_mode=</span><span class="s0">{</span><span class="s1">mode</span><span class="s0">}</span><span class="s3">&quot;</span>
        <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">quantized_build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_int8_build</span><span class="s2">()</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">mode</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_int8_build</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">embeddings_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">embeddings_scale_initializer</span><span class="s2">=</span><span class="s3">&quot;ones&quot;</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;embeddings&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_dim</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">output_dim</span><span class="s2">),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">embeddings_initializer</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;int8&quot;</span><span class="s2">,</span>
            <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s6"># We choose to reduce the axis of `output_dim` because, typically,</span>
        <span class="s6"># `input_dim` is larger than `output_dim`. This reduces quantization</span>
        <span class="s6"># error.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;embeddings_scale&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_dim</span><span class="s2">,),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">embeddings_scale_initializer</span><span class="s2">,</span>
            <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_is_quantized </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">quantized_call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, *</span><span class="s1">args</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s2">!= </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantization_mode</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">super</span><span class="s2">().</span><span class="s1">quantized_call</span><span class="s2">(*</span><span class="s1">args</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_int8_call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s6"># We cannot update quantized self._embeddings, so the custom gradient is</span>
        <span class="s6"># not needed</span>
        <span class="s0">if </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_dtype</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">) </span><span class="s0">not in </span><span class="s2">(</span><span class="s3">&quot;int32&quot;</span><span class="s2">, </span><span class="s3">&quot;int64&quot;</span><span class="s2">):</span>
            <span class="s1">inputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s3">&quot;int32&quot;</span><span class="s2">)</span>
        <span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">take</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_scale</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">take</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
        <span class="s6"># De-scale outputs</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">),</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">embeddings_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">),</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
            <span class="s1">lora_outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">take</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">0</span><span class="s2">)</span>
            <span class="s1">lora_outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">lora_outputs</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b</span><span class="s2">)</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">lora_outputs</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">outputs</span>

    <span class="s0">def </span><span class="s1">quantize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">mode</span><span class="s2">, </span><span class="s1">type_check</span><span class="s2">=</span><span class="s0">True</span><span class="s2">):</span>
        <span class="s6"># Prevent quantization of the subclasses</span>
        <span class="s0">if </span><span class="s1">type_check </span><span class="s0">and </span><span class="s2">(</span><span class="s1">type</span><span class="s2">(</span><span class="s1">self</span><span class="s2">) </span><span class="s0">is not </span><span class="s1">Embedding</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_not_implemented_error</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">quantize</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">mode </span><span class="s2">== </span><span class="s3">&quot;int8&quot;</span><span class="s2">:</span>
            <span class="s6"># Quantize `self._embeddings` to int8 and compute corresponding</span>
            <span class="s6"># scale</span>
            <span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">quantizers</span><span class="s2">.</span><span class="s1">abs_max_quantize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">to_numpy</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
            <span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">embeddings_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
            <span class="s0">del </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span>
            <span class="s6"># Utilize a lambda expression as an initializer to prevent adding a</span>
            <span class="s6"># large constant to the computation graph.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_int8_build</span><span class="s2">(</span>
                <span class="s0">lambda </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">embeddings_value</span><span class="s2">,</span>
                <span class="s0">lambda </span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">: </span><span class="s1">embeddings_scale</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_quantization_mode_error</span><span class="s2">(</span><span class="s1">mode</span><span class="s2">)</span>

        <span class="s6"># Set new dtype policy</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">policy </span><span class="s2">= </span><span class="s1">dtype_policies</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s3">f&quot;</span><span class="s0">{</span><span class="s1">mode</span><span class="s0">}</span><span class="s3">_from_</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s3">&quot;</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy </span><span class="s2">= </span><span class="s1">policy</span>

    <span class="s0">def </span><span class="s1">_get_embeddings_with_merged_lora</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">.</span><span class="s1">quantization_mode </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">embeddings_value </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_embeddings</span>
            <span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings_scale</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_enabled</span><span class="s2">:</span>
                <span class="s6"># Dequantize &amp; quantize to merge lora weights into embeddings</span>
                <span class="s6"># Note that this is a lossy compression</span>
                <span class="s1">embeddings_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span>
                    <span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">embeddings_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
                <span class="s2">)</span>
                <span class="s1">embeddings_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span>
                    <span class="s1">embeddings_value</span><span class="s2">,</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">matmul</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_a</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">lora_embeddings_b</span><span class="s2">),</span>
                <span class="s2">)</span>
                <span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">embeddings_scale </span><span class="s2">= (</span>
                    <span class="s1">quantizers</span><span class="s2">.</span><span class="s1">abs_max_quantize</span><span class="s2">(</span>
                        <span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">to_numpy</span><span class="s2">=</span><span class="s0">True</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
                <span class="s1">embeddings_scale </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">embeddings_scale</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">embeddings_value</span><span class="s2">, </span><span class="s1">embeddings_scale</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">embeddings</span><span class="s2">, </span><span class="s0">None</span>
</pre>
</body>
</html>