<html>
<head>
<title>_classification.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #7a7e85;}
.s3 { color: #cf8e6d;}
.s4 { color: #bcbec4;}
.s5 { color: #6aab73;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
_classification.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Metrics to assess performance on classification task given class prediction. 
 
Functions named as ``*_score`` return a scalar value to maximize: the higher 
the better. 
 
Function named as ``*_error`` or ``*_loss`` return a scalar value to minimize: 
the lower the better. 
&quot;&quot;&quot;</span>

<span class="s2"># Authors: Alexandre Gramfort &lt;alexandre.gramfort@inria.fr&gt;</span>
<span class="s2">#          Mathieu Blondel &lt;mathieu@mblondel.org&gt;</span>
<span class="s2">#          Olivier Grisel &lt;olivier.grisel@ensta.org&gt;</span>
<span class="s2">#          Arnaud Joly &lt;a.joly@ulg.ac.be&gt;</span>
<span class="s2">#          Jochen Wersdorfer &lt;jochen@wersdoerfer.de&gt;</span>
<span class="s2">#          Lars Buitinck</span>
<span class="s2">#          Joel Nothman &lt;joel.nothman@gmail.com&gt;</span>
<span class="s2">#          Noel Dawe &lt;noel@dawe.me&gt;</span>
<span class="s2">#          Jatin Shah &lt;jatindshah@gmail.com&gt;</span>
<span class="s2">#          Saurabh Jha &lt;saurabh.jhaa@gmail.com&gt;</span>
<span class="s2">#          Bernardo Stein &lt;bernardovstein@gmail.com&gt;</span>
<span class="s2">#          Shangwu Yao &lt;shangwuyao@gmail.com&gt;</span>
<span class="s2">#          Michal Karbownik &lt;michakarbownik@gmail.com&gt;</span>
<span class="s2"># License: BSD 3 clause</span>


<span class="s3">import </span><span class="s1">warnings</span>
<span class="s3">from </span><span class="s1">numbers </span><span class="s3">import </span><span class="s1">Integral</span><span class="s4">, </span><span class="s1">Real</span>

<span class="s3">import </span><span class="s1">numpy </span><span class="s3">as </span><span class="s1">np</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">sparse </span><span class="s3">import </span><span class="s1">coo_matrix</span><span class="s4">, </span><span class="s1">csr_matrix</span>
<span class="s3">from </span><span class="s1">scipy</span><span class="s4">.</span><span class="s1">special </span><span class="s3">import </span><span class="s1">xlogy</span>

<span class="s3">from </span><span class="s4">..</span><span class="s1">exceptions </span><span class="s3">import </span><span class="s1">UndefinedMetricWarning</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">preprocessing </span><span class="s3">import </span><span class="s1">LabelBinarizer</span><span class="s4">, </span><span class="s1">LabelEncoder</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">assert_all_finite</span><span class="s4">,</span>
    <span class="s1">check_array</span><span class="s4">,</span>
    <span class="s1">check_consistent_length</span><span class="s4">,</span>
    <span class="s1">column_or_1d</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_array_api </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_average</span><span class="s4">,</span>
    <span class="s1">_count_nonzero</span><span class="s4">,</span>
    <span class="s1">_is_numpy_namespace</span><span class="s4">,</span>
    <span class="s1">_union1d</span><span class="s4">,</span>
    <span class="s1">get_namespace</span><span class="s4">,</span>
    <span class="s1">get_namespace_and_device</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">_param_validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">Hidden</span><span class="s4">,</span>
    <span class="s1">Interval</span><span class="s4">,</span>
    <span class="s1">Options</span><span class="s4">,</span>
    <span class="s1">StrOptions</span><span class="s4">,</span>
    <span class="s1">validate_params</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">extmath </span><span class="s3">import </span><span class="s1">_nanaverage</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">multiclass </span><span class="s3">import </span><span class="s1">type_of_target</span><span class="s4">, </span><span class="s1">unique_labels</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">sparsefuncs </span><span class="s3">import </span><span class="s1">count_nonzero</span>
<span class="s3">from </span><span class="s4">..</span><span class="s1">utils</span><span class="s4">.</span><span class="s1">validation </span><span class="s3">import </span><span class="s4">(</span>
    <span class="s1">_check_pos_label_consistency</span><span class="s4">,</span>
    <span class="s1">_check_sample_weight</span><span class="s4">,</span>
    <span class="s1">_num_samples</span><span class="s4">,</span>
<span class="s4">)</span>


<span class="s3">def </span><span class="s1">_check_zero_division</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">):</span>
    <span class="s3">if </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">, </span><span class="s1">str</span><span class="s4">) </span><span class="s3">and </span><span class="s1">zero_division </span><span class="s4">== </span><span class="s5">&quot;warn&quot;</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">(</span><span class="s6">0.0</span><span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">, (</span><span class="s1">int</span><span class="s4">, </span><span class="s1">float</span><span class="s4">)) </span><span class="s3">and </span><span class="s1">zero_division </span><span class="s3">in </span><span class="s4">[</span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">]:</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:  </span><span class="s2"># np.isnan(zero_division)</span>
        <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>


<span class="s3">def </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Check that y_true and y_pred belong to the same classification task. 
 
    This converts multiclass or binary types to a common shape, and raises a 
    ValueError for a mix of multilabel and multiclass targets, a mix of 
    multilabel formats, for the presence of continuous-valued or multioutput 
    targets, or for targets of different lengths. 
 
    Column vectors are squeezed to 1d, while multilabel formats are returned 
    as CSR sparse label indicators. 
 
    Parameters 
    ---------- 
    y_true : array-like 
 
    y_pred : array-like 
 
    Returns 
    ------- 
    type_true : one of {'multilabel-indicator', 'multiclass', 'binary'} 
        The type of the true target data, as output by 
        ``utils.multiclass.type_of_target``. 
 
    y_true : array or indicator matrix 
 
    y_pred : array or indicator matrix 
    &quot;&quot;&quot;</span>
    <span class="s1">xp</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">get_namespace</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">type_true </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s1">type_pred </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_pred&quot;</span><span class="s4">)</span>

    <span class="s1">y_type </span><span class="s4">= {</span><span class="s1">type_true</span><span class="s4">, </span><span class="s1">type_pred</span><span class="s4">}</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== {</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">}:</span>
        <span class="s1">y_type </span><span class="s4">= {</span><span class="s5">&quot;multiclass&quot;</span><span class="s4">}</span>

    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">) &gt; </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Classification metrics can't handle a mix of {0} and {1} targets&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                <span class="s1">type_true</span><span class="s4">, </span><span class="s1">type_pred</span>
            <span class="s4">)</span>
        <span class="s4">)</span>

    <span class="s2"># We can't have more than one value on y_type =&gt; The set is no more needed</span>
    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">y_type</span><span class="s4">.</span><span class="s1">pop</span><span class="s4">()</span>

    <span class="s2"># No metrics support &quot;multiclass-multioutput&quot; format</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">[</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">, </span><span class="s5">&quot;multilabel-indicator&quot;</span><span class="s4">]:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>

    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">in </span><span class="s4">[</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">]:</span>
        <span class="s1">xp</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">get_namespace</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
            <span class="s3">try</span><span class="s4">:</span>
                <span class="s1">unique_values </span><span class="s4">= </span><span class="s1">_union1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">xp</span><span class="s4">)</span>
            <span class="s3">except </span><span class="s1">TypeError </span><span class="s3">as </span><span class="s1">e</span><span class="s4">:</span>
                <span class="s2"># We expect y_true and y_pred to be of the same data type.</span>
                <span class="s2"># If `y_true` was provided to the classifier as strings,</span>
                <span class="s2"># `y_pred` given by the classifier will also be encoded with</span>
                <span class="s2"># strings. So we raise a meaningful error</span>
                <span class="s3">raise </span><span class="s1">TypeError</span><span class="s4">(</span>
                    <span class="s5">&quot;Labels in y_true and y_pred should be of the same type. &quot;</span>
                    <span class="s5">f&quot;Got y_true=</span><span class="s3">{</span><span class="s1">xp</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span><span class="s3">} </span><span class="s5">and &quot;</span>
                    <span class="s5">f&quot;y_pred=</span><span class="s3">{</span><span class="s1">xp</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span><span class="s3">}</span><span class="s5">. Make sure that the &quot;</span>
                    <span class="s5">&quot;predictions provided by the classifier coincides with &quot;</span>
                    <span class="s5">&quot;the true labels.&quot;</span>
                <span class="s4">) </span><span class="s3">from </span><span class="s1">e</span>
            <span class="s3">if </span><span class="s1">unique_values</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] &gt; </span><span class="s6">2</span><span class="s4">:</span>
                <span class="s1">y_type </span><span class="s4">= </span><span class="s5">&quot;multiclass&quot;</span>

    <span class="s3">if </span><span class="s1">y_type</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s5">&quot;multilabel&quot;</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">_is_numpy_namespace</span><span class="s4">(</span><span class="s1">xp</span><span class="s4">):</span>
            <span class="s2"># XXX: do we really want to sparse-encode multilabel indicators when</span>
            <span class="s2"># they are passed as a dense arrays? This is not possible for array</span>
            <span class="s2"># API inputs in general hence we only do it for NumPy inputs. But even</span>
            <span class="s2"># for NumPy the usefulness is questionable.</span>
            <span class="s1">y_true </span><span class="s4">= </span><span class="s1">csr_matrix</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
            <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">csr_matrix</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s1">y_type </span><span class="s4">= </span><span class="s5">&quot;multilabel-indicator&quot;</span>

    <span class="s3">return </span><span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;normalize&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">accuracy_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Accuracy classification score. 
 
    In multilabel classification, this function computes subset accuracy: 
    the set of labels predicted for a sample must *exactly* match the 
    corresponding set of labels in y_true. 
 
    Read more in the :ref:`User Guide &lt;accuracy_score&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) labels. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Predicted labels, as returned by a classifier. 
 
    normalize : bool, default=True 
        If ``False``, return the number of correctly classified samples. 
        Otherwise, return the fraction of correctly classified samples. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    score : float or int 
        If ``normalize == True``, return the fraction of correctly 
        classified samples (float), else returns the number of correctly 
        classified samples (int). 
 
        The best performance is 1 with ``normalize == True`` and the number 
        of samples with ``normalize == False``. 
 
    See Also 
    -------- 
    balanced_accuracy_score : Compute the balanced accuracy to deal with 
        imbalanced datasets. 
    jaccard_score : Compute the Jaccard similarity coefficient score. 
    hamming_loss : Compute the average Hamming loss or Hamming distance between 
        two sets of samples. 
    zero_one_loss : Compute the Zero-one classification loss. By default, the 
        function will return the percentage of imperfectly predicted subsets. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import accuracy_score 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 3] 
    &gt;&gt;&gt; y_true = [0, 1, 2, 3] 
    &gt;&gt;&gt; accuracy_score(y_true, y_pred) 
    0.5 
    &gt;&gt;&gt; accuracy_score(y_true, y_pred, normalize=False) 
    2.0 
 
    In the multilabel case with binary label indicators: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 
    0.5 
    &quot;&quot;&quot;</span>
    <span class="s1">xp</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">device </span><span class="s4">= </span><span class="s1">get_namespace_and_device</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s2"># Compute accuracy for each possible representation</span>
    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s5">&quot;multilabel&quot;</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">_is_numpy_namespace</span><span class="s4">(</span><span class="s1">xp</span><span class="s4">):</span>
            <span class="s1">differing_labels </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">- </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">differing_labels </span><span class="s4">= </span><span class="s1">_count_nonzero</span><span class="s4">(</span>
                <span class="s1">y_true </span><span class="s4">- </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">xp</span><span class="s4">=</span><span class="s1">xp</span><span class="s4">, </span><span class="s1">device</span><span class="s4">=</span><span class="s1">device</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span>
            <span class="s4">)</span>
        <span class="s1">score </span><span class="s4">= </span><span class="s1">xp</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">differing_labels </span><span class="s4">== </span><span class="s6">0</span><span class="s4">, </span><span class="s1">device</span><span class="s4">=</span><span class="s1">device</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">score </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">y_pred</span>

    <span class="s3">return </span><span class="s1">float</span><span class="s4">(</span><span class="s1">_average</span><span class="s4">(</span><span class="s1">score</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s1">normalize</span><span class="s4">))</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;normalize&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;true&quot;</span><span class="s4">, </span><span class="s5">&quot;pred&quot;</span><span class="s4">, </span><span class="s5">&quot;all&quot;</span><span class="s4">}), </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">confusion_matrix</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">None</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute confusion matrix to evaluate the accuracy of a classification. 
 
    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}` 
    is equal to the number of observations known to be in group :math:`i` and 
    predicted to be in group :math:`j`. 
 
    Thus in binary classification, the count of true negatives is 
    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is 
    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`. 
 
    Read more in the :ref:`User Guide &lt;confusion_matrix&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        Ground truth (correct) target values. 
 
    y_pred : array-like of shape (n_samples,) 
        Estimated targets as returned by a classifier. 
 
    labels : array-like of shape (n_classes), default=None 
        List of labels to index the matrix. This may be used to reorder 
        or select a subset of labels. 
        If ``None`` is given, those that appear at least once 
        in ``y_true`` or ``y_pred`` are used in sorted order. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
        .. versionadded:: 0.18 
 
    normalize : {'true', 'pred', 'all'}, default=None 
        Normalizes confusion matrix over the true (rows), predicted (columns) 
        conditions or all the population. If None, confusion matrix will not be 
        normalized. 
 
    Returns 
    ------- 
    C : ndarray of shape (n_classes, n_classes) 
        Confusion matrix whose i-th row and j-th 
        column entry indicates the number of 
        samples with true label being i-th class 
        and predicted label being j-th class. 
 
    See Also 
    -------- 
    ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix 
        given an estimator, the data, and the label. 
    ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix 
        given the true and predicted labels. 
    ConfusionMatrixDisplay : Confusion Matrix visualization. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Confusion matrix 
           &lt;https://en.wikipedia.org/wiki/Confusion_matrix&gt;`_ 
           (Wikipedia and other references may use a different 
           convention for axes). 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import confusion_matrix 
    &gt;&gt;&gt; y_true = [2, 0, 2, 2, 0, 1] 
    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 0, 2] 
    &gt;&gt;&gt; confusion_matrix(y_true, y_pred) 
    array([[2, 0, 0], 
           [0, 0, 1], 
           [1, 0, 2]]) 
 
    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;] 
    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;] 
    &gt;&gt;&gt; confusion_matrix(y_true, y_pred, labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;]) 
    array([[2, 0, 0], 
           [0, 0, 1], 
           [1, 0, 2]]) 
 
    In the binary case, we can extract true positives, etc. as follows: 
 
    &gt;&gt;&gt; tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel() 
    &gt;&gt;&gt; (tn, fp, fn, tp) 
    (np.int64(0), np.int64(2), np.int64(1), np.int64(1)) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">(</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;%s is not supported&quot; </span><span class="s4">% </span><span class="s1">y_type</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">unique_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">n_labels </span><span class="s4">= </span><span class="s1">labels</span><span class="s4">.</span><span class="s1">size</span>
        <span class="s3">if </span><span class="s1">n_labels </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;'labels' should contains at least one label.&quot;</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">size </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">((</span><span class="s1">n_labels</span><span class="s4">, </span><span class="s1">n_labels</span><span class="s4">), </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">int</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">len</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">intersect1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">)) == </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;At least one label specified must be in y_true&quot;</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">int64</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">n_labels </span><span class="s4">= </span><span class="s1">labels</span><span class="s4">.</span><span class="s1">size</span>
    <span class="s2"># If labels are not consecutive integers starting from zero, then</span>
    <span class="s2"># y_true and y_pred must be converted into index form</span>
    <span class="s1">need_index_conversion </span><span class="s4">= </span><span class="s3">not </span><span class="s4">(</span>
        <span class="s1">labels</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">.</span><span class="s1">kind </span><span class="s3">in </span><span class="s4">{</span><span class="s5">&quot;i&quot;</span><span class="s4">, </span><span class="s5">&quot;u&quot;</span><span class="s4">, </span><span class="s5">&quot;b&quot;</span><span class="s4">}</span>
        <span class="s3">and </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">labels </span><span class="s4">== </span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">n_labels</span><span class="s4">))</span>
        <span class="s3">and </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &gt;= </span><span class="s6">0</span>
        <span class="s3">and </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &gt;= </span><span class="s6">0</span>
    <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">need_index_conversion</span><span class="s4">:</span>
        <span class="s1">label_to_ind </span><span class="s4">= {</span><span class="s1">y</span><span class="s4">: </span><span class="s1">x </span><span class="s3">for </span><span class="s1">x</span><span class="s4">, </span><span class="s1">y </span><span class="s3">in </span><span class="s1">enumerate</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)}</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">label_to_ind</span><span class="s4">.</span><span class="s1">get</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s1">n_labels </span><span class="s4">+ </span><span class="s6">1</span><span class="s4">) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">y_pred</span><span class="s4">])</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">label_to_ind</span><span class="s4">.</span><span class="s1">get</span><span class="s4">(</span><span class="s1">x</span><span class="s4">, </span><span class="s1">n_labels </span><span class="s4">+ </span><span class="s6">1</span><span class="s4">) </span><span class="s3">for </span><span class="s1">x </span><span class="s3">in </span><span class="s1">y_true</span><span class="s4">])</span>

    <span class="s2"># intersect y_pred, y_true with labels, eliminate items not in labels</span>
    <span class="s1">ind </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">logical_and</span><span class="s4">(</span><span class="s1">y_pred </span><span class="s4">&lt; </span><span class="s1">n_labels</span><span class="s4">, </span><span class="s1">y_true </span><span class="s4">&lt; </span><span class="s1">n_labels</span><span class="s4">)</span>
    <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">all</span><span class="s4">(</span><span class="s1">ind</span><span class="s4">):</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">y_pred</span><span class="s4">[</span><span class="s1">ind</span><span class="s4">]</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[</span><span class="s1">ind</span><span class="s4">]</span>
        <span class="s2"># also eliminate weights of eliminated items</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">sample_weight</span><span class="s4">[</span><span class="s1">ind</span><span class="s4">]</span>

    <span class="s2"># Choose the accumulator dtype to always have high precision</span>
    <span class="s3">if </span><span class="s1">sample_weight</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">.</span><span class="s1">kind </span><span class="s3">in </span><span class="s4">{</span><span class="s5">&quot;i&quot;</span><span class="s4">, </span><span class="s5">&quot;u&quot;</span><span class="s4">, </span><span class="s5">&quot;b&quot;</span><span class="s4">}:</span>
        <span class="s1">dtype </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">int64</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">dtype </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span>

    <span class="s1">cm </span><span class="s4">= </span><span class="s1">coo_matrix</span><span class="s4">(</span>
        <span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, (</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)),</span>
        <span class="s1">shape</span><span class="s4">=(</span><span class="s1">n_labels</span><span class="s4">, </span><span class="s1">n_labels</span><span class="s4">),</span>
        <span class="s1">dtype</span><span class="s4">=</span><span class="s1">dtype</span><span class="s4">,</span>
    <span class="s4">).</span><span class="s1">toarray</span><span class="s4">()</span>

    <span class="s3">with </span><span class="s1">np</span><span class="s4">.</span><span class="s1">errstate</span><span class="s4">(</span><span class="s1">all</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">normalize </span><span class="s4">== </span><span class="s5">&quot;true&quot;</span><span class="s4">:</span>
            <span class="s1">cm </span><span class="s4">= </span><span class="s1">cm </span><span class="s4">/ </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">keepdims</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">normalize </span><span class="s4">== </span><span class="s5">&quot;pred&quot;</span><span class="s4">:</span>
            <span class="s1">cm </span><span class="s4">= </span><span class="s1">cm </span><span class="s4">/ </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">, </span><span class="s1">keepdims</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
        <span class="s3">elif </span><span class="s1">normalize </span><span class="s4">== </span><span class="s5">&quot;all&quot;</span><span class="s4">:</span>
            <span class="s1">cm </span><span class="s4">= </span><span class="s1">cm </span><span class="s4">/ </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
        <span class="s1">cm </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan_to_num</span><span class="s4">(</span><span class="s1">cm</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">== (</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">):</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s4">(</span>
                <span class="s5">&quot;A single label was found in 'y_true' and 'y_pred'. For the confusion &quot;</span>
                <span class="s5">&quot;matrix to have the correct shape, use the 'labels' parameter to pass &quot;</span>
                <span class="s5">&quot;all known labels.&quot;</span>
            <span class="s4">),</span>
            <span class="s1">UserWarning</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s3">return </span><span class="s1">cm</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;samplewise&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">multilabel_confusion_matrix</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">samplewise</span><span class="s4">=</span><span class="s3">False</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute a confusion matrix for each class or sample. 
 
    .. versionadded:: 0.21 
 
    Compute class-wise (default) or sample-wise (samplewise=True) multilabel 
    confusion matrix to evaluate the accuracy of a classification, and output 
    confusion matrices for each class or sample. 
 
    In multilabel confusion matrix :math:`MCM`, the count of true negatives 
    is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`, 
    true positives is :math:`MCM_{:,1,1}` and false positives is 
    :math:`MCM_{:,0,1}`. 
 
    Multiclass data will be treated as if binarized under a one-vs-rest 
    transformation. Returned confusion matrices will be in the order of 
    sorted unique labels in the union of (y_true, y_pred). 
 
    Read more in the :ref:`User Guide &lt;multilabel_confusion_matrix&gt;`. 
 
    Parameters 
    ---------- 
    y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or \ 
            (n_samples,) 
        Ground truth (correct) target values. 
 
    y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or \ 
            (n_samples,) 
        Estimated targets as returned by a classifier. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    labels : array-like of shape (n_classes,), default=None 
        A list of classes or column indices to select some (or to force 
        inclusion of classes absent from the data). 
 
    samplewise : bool, default=False 
        In the multilabel case, this calculates a confusion matrix per sample. 
 
    Returns 
    ------- 
    multi_confusion : ndarray of shape (n_outputs, 2, 2) 
        A 2x2 confusion matrix corresponding to each output in the input. 
        When calculating class-wise multi_confusion (default), then 
        n_outputs = n_labels; when calculating sample-wise multi_confusion 
        (samplewise=True), n_outputs = n_samples. If ``labels`` is defined, 
        the results will be returned in the order specified in ``labels``, 
        otherwise the results will be returned in sorted order by default. 
 
    See Also 
    -------- 
    confusion_matrix : Compute confusion matrix to evaluate the accuracy of a 
        classifier. 
 
    Notes 
    ----- 
    The `multilabel_confusion_matrix` calculates class-wise or sample-wise 
    multilabel confusion matrices, and in multiclass tasks, labels are 
    binarized under a one-vs-rest way; while 
    :func:`~sklearn.metrics.confusion_matrix` calculates one confusion matrix 
    for confusion between every two classes. 
 
    Examples 
    -------- 
    Multilabel-indicator case: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import multilabel_confusion_matrix 
    &gt;&gt;&gt; y_true = np.array([[1, 0, 1], 
    ...                    [0, 1, 0]]) 
    &gt;&gt;&gt; y_pred = np.array([[1, 0, 0], 
    ...                    [0, 1, 1]]) 
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred) 
    array([[[1, 0], 
            [0, 1]], 
    &lt;BLANKLINE&gt; 
           [[1, 0], 
            [0, 1]], 
    &lt;BLANKLINE&gt; 
           [[0, 1], 
            [1, 0]]]) 
 
    Multiclass case: 
 
    &gt;&gt;&gt; y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;] 
    &gt;&gt;&gt; y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;] 
    &gt;&gt;&gt; multilabel_confusion_matrix(y_true, y_pred, 
    ...                             labels=[&quot;ant&quot;, &quot;bird&quot;, &quot;cat&quot;]) 
    array([[[3, 1], 
            [0, 2]], 
    &lt;BLANKLINE&gt; 
           [[5, 0], 
            [1, 0]], 
    &lt;BLANKLINE&gt; 
           [[2, 1], 
            [1, 2]]]) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">(</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">, </span><span class="s5">&quot;multilabel-indicator&quot;</span><span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;%s is not supported&quot; </span><span class="s4">% </span><span class="s1">y_type</span><span class="s4">)</span>

    <span class="s1">present_labels </span><span class="s4">= </span><span class="s1">unique_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">present_labels</span>
        <span class="s1">n_labels </span><span class="s4">= </span><span class="s3">None</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">n_labels </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">(</span>
            <span class="s4">[</span><span class="s1">labels</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">setdiff1d</span><span class="s4">(</span><span class="s1">present_labels</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">assume_unique</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)]</span>
        <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">samplewise</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Samplewise metrics are not available outside of &quot;</span>
                <span class="s5">&quot;multilabel classification.&quot;</span>
            <span class="s4">)</span>

        <span class="s1">le </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">()</span>
        <span class="s1">le</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">le</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">le</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s1">sorted_labels </span><span class="s4">= </span><span class="s1">le</span><span class="s4">.</span><span class="s1">classes_</span>

        <span class="s2"># labels are now from 0 to len(labels) - 1 -&gt; use bincount</span>
        <span class="s1">tp </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">y_pred</span>
        <span class="s1">tp_bins </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[</span><span class="s1">tp</span><span class="s4">]</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">tp_bins_weights </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)[</span><span class="s1">tp</span><span class="s4">]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">tp_bins_weights </span><span class="s4">= </span><span class="s3">None</span>

        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">tp_bins</span><span class="s4">):</span>
            <span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span>
                <span class="s1">tp_bins</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">tp_bins_weights</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s2"># Pathological case</span>
            <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">))</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">):</span>
            <span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">))</span>
        <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">):</span>
            <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">minlength</span><span class="s4">=</span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">))</span>

        <span class="s2"># Retain only selected labels</span>
        <span class="s1">indices </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">searchsorted</span><span class="s4">(</span><span class="s1">sorted_labels</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">[:</span><span class="s1">n_labels</span><span class="s4">])</span>
        <span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">tp_sum</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>
        <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">true_sum</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>
        <span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">pred_sum</span><span class="s4">[</span><span class="s1">indices</span><span class="s4">]</span>

    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">sum_axis </span><span class="s4">= </span><span class="s6">1 </span><span class="s3">if </span><span class="s1">samplewise </span><span class="s3">else </span><span class="s6">0</span>

        <span class="s2"># All labels are index integers for multilabel.</span>
        <span class="s2"># Select labels:</span>
        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array_equal</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">, </span><span class="s1">present_labels</span><span class="s4">):</span>
            <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) &gt; </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">present_labels</span><span class="s4">):</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;All labels must be in [0, n labels) for &quot;</span>
                    <span class="s5">&quot;multilabel targets. &quot;</span>
                    <span class="s5">&quot;Got %d &gt; %d&quot; </span><span class="s4">% (</span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">), </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">present_labels</span><span class="s4">))</span>
                <span class="s4">)</span>
            <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">min</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) &lt; </span><span class="s6">0</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;All labels must be in [0, n labels) for &quot;</span>
                    <span class="s5">&quot;multilabel targets. &quot;</span>
                    <span class="s5">&quot;Got %d &lt; 0&quot; </span><span class="s4">% </span><span class="s1">np</span><span class="s4">.</span><span class="s1">min</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
                <span class="s4">)</span>

        <span class="s3">if </span><span class="s1">n_labels </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">y_true </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">[:, </span><span class="s1">labels</span><span class="s4">[:</span><span class="s1">n_labels</span><span class="s4">]]</span>
            <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">y_pred</span><span class="s4">[:, </span><span class="s1">labels</span><span class="s4">[:</span><span class="s1">n_labels</span><span class="s4">]]</span>

        <span class="s2"># calculate weighted counts</span>
        <span class="s1">true_and_pred </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">multiply</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span>
            <span class="s1">true_and_pred</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s1">sum_axis</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
        <span class="s4">)</span>
        <span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s1">sum_axis</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s1">sum_axis</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">fp </span><span class="s4">= </span><span class="s1">pred_sum </span><span class="s4">- </span><span class="s1">tp_sum</span>
    <span class="s1">fn </span><span class="s4">= </span><span class="s1">true_sum </span><span class="s4">- </span><span class="s1">tp_sum</span>
    <span class="s1">tp </span><span class="s4">= </span><span class="s1">tp_sum</span>

    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None and </span><span class="s1">samplewise</span><span class="s4">:</span>
        <span class="s1">sample_weight </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s1">tp </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">tp</span><span class="s4">)</span>
        <span class="s1">fp </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">fp</span><span class="s4">)</span>
        <span class="s1">fn </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">fn</span><span class="s4">)</span>
        <span class="s1">tn </span><span class="s4">= </span><span class="s1">sample_weight </span><span class="s4">* </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] - </span><span class="s1">tp </span><span class="s4">- </span><span class="s1">fp </span><span class="s4">- </span><span class="s1">fn</span>
    <span class="s3">elif </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">tn </span><span class="s4">= </span><span class="s1">sum</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">) - </span><span class="s1">tp </span><span class="s4">- </span><span class="s1">fp </span><span class="s4">- </span><span class="s1">fn</span>
    <span class="s3">elif </span><span class="s1">samplewise</span><span class="s4">:</span>
        <span class="s1">tn </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] - </span><span class="s1">tp </span><span class="s4">- </span><span class="s1">fp </span><span class="s4">- </span><span class="s1">fn</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">tn </span><span class="s4">= </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] - </span><span class="s1">tp </span><span class="s4">- </span><span class="s1">fp </span><span class="s4">- </span><span class="s1">fn</span>

    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">tn</span><span class="s4">, </span><span class="s1">fp</span><span class="s4">, </span><span class="s1">fn</span><span class="s4">, </span><span class="s1">tp</span><span class="s4">]).</span><span class="s1">T</span><span class="s4">.</span><span class="s1">reshape</span><span class="s4">(-</span><span class="s6">1</span><span class="s4">, </span><span class="s6">2</span><span class="s4">, </span><span class="s6">2</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y1&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y2&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;weights&quot;</span><span class="s4">: [</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;linear&quot;</span><span class="s4">, </span><span class="s5">&quot;quadratic&quot;</span><span class="s4">}), </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">cohen_kappa_score</span><span class="s4">(</span><span class="s1">y1</span><span class="s4">, </span><span class="s1">y2</span><span class="s4">, *, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">r&quot;&quot;&quot;Compute Cohen's kappa: a statistic that measures inter-annotator agreement. 
 
    This function computes Cohen's kappa [1]_, a score that expresses the level 
    of agreement between two annotators on a classification problem. It is 
    defined as 
 
    .. math:: 
        \kappa = (p_o - p_e) / (1 - p_e) 
 
    where :math:`p_o` is the empirical probability of agreement on the label 
    assigned to any sample (the observed agreement ratio), and :math:`p_e` is 
    the expected agreement when both annotators assign labels randomly. 
    :math:`p_e` is estimated using a per-annotator empirical prior over the 
    class labels [2]_. 
 
    Read more in the :ref:`User Guide &lt;cohen_kappa&gt;`. 
 
    Parameters 
    ---------- 
    y1 : array-like of shape (n_samples,) 
        Labels assigned by the first annotator. 
 
    y2 : array-like of shape (n_samples,) 
        Labels assigned by the second annotator. The kappa statistic is 
        symmetric, so swapping ``y1`` and ``y2`` doesn't change the value. 
 
    labels : array-like of shape (n_classes,), default=None 
        List of labels to index the matrix. This may be used to select a 
        subset of labels. If `None`, all labels that appear at least once in 
        ``y1`` or ``y2`` are used. 
 
    weights : {'linear', 'quadratic'}, default=None 
        Weighting type to calculate the score. `None` means no weighted; 
        &quot;linear&quot; means linear weighted; &quot;quadratic&quot; means quadratic weighted. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    kappa : float 
        The kappa statistic, which is a number between -1 and 1. The maximum 
        value means complete agreement; zero or lower means chance agreement. 
 
    References 
    ---------- 
    .. [1] :doi:`J. Cohen (1960). &quot;A coefficient of agreement for nominal scales&quot;. 
           Educational and Psychological Measurement 20(1):37-46. 
           &lt;10.1177/001316446002000104&gt;` 
    .. [2] `R. Artstein and M. Poesio (2008). &quot;Inter-coder agreement for 
           computational linguistics&quot;. Computational Linguistics 34(4):555-596 
           &lt;https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2&gt;`_. 
    .. [3] `Wikipedia entry for the Cohen's kappa 
            &lt;https://en.wikipedia.org/wiki/Cohen%27s_kappa&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import cohen_kappa_score 
    &gt;&gt;&gt; y1 = [&quot;negative&quot;, &quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;, &quot;positive&quot;] 
    &gt;&gt;&gt; y2 = [&quot;negative&quot;, &quot;positive&quot;, &quot;negative&quot;, &quot;neutral&quot;, &quot;negative&quot;] 
    &gt;&gt;&gt; cohen_kappa_score(y1, y2) 
    np.float64(0.6875) 
    &quot;&quot;&quot;</span>
    <span class="s1">confusion </span><span class="s4">= </span><span class="s1">confusion_matrix</span><span class="s4">(</span><span class="s1">y1</span><span class="s4">, </span><span class="s1">y2</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">confusion</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span>
    <span class="s1">sum0 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">confusion</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">)</span>
    <span class="s1">sum1 </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">confusion</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s1">expected </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">outer</span><span class="s4">(</span><span class="s1">sum0</span><span class="s4">, </span><span class="s1">sum1</span><span class="s4">) / </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">sum0</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">weights </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">w_mat </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones</span><span class="s4">([</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">], </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">int</span><span class="s4">)</span>
        <span class="s1">w_mat</span><span class="s4">.</span><span class="s1">flat</span><span class="s4">[:: </span><span class="s1">n_classes </span><span class="s4">+ </span><span class="s6">1</span><span class="s4">] = </span><span class="s6">0</span>
    <span class="s3">else</span><span class="s4">:  </span><span class="s2"># &quot;linear&quot; or &quot;quadratic&quot;</span>
        <span class="s1">w_mat </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">zeros</span><span class="s4">([</span><span class="s1">n_classes</span><span class="s4">, </span><span class="s1">n_classes</span><span class="s4">], </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">int</span><span class="s4">)</span>
        <span class="s1">w_mat </span><span class="s4">+= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">n_classes</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">weights </span><span class="s4">== </span><span class="s5">&quot;linear&quot;</span><span class="s4">:</span>
            <span class="s1">w_mat </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">abs</span><span class="s4">(</span><span class="s1">w_mat </span><span class="s4">- </span><span class="s1">w_mat</span><span class="s4">.</span><span class="s1">T</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">w_mat </span><span class="s4">= (</span><span class="s1">w_mat </span><span class="s4">- </span><span class="s1">w_mat</span><span class="s4">.</span><span class="s1">T</span><span class="s4">) ** </span><span class="s6">2</span>

    <span class="s1">k </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">w_mat </span><span class="s4">* </span><span class="s1">confusion</span><span class="s4">) / </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">w_mat </span><span class="s4">* </span><span class="s1">expected</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">k</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">}),</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">jaccard_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Jaccard similarity coefficient score. 
 
    The Jaccard index [1], or Jaccard similarity coefficient, defined as 
    the size of the intersection divided by the size of the union of two label 
    sets, is used to compare set of predicted labels for a sample to the 
    corresponding set of labels in ``y_true``. 
 
    Support beyond term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return the 
    Jaccard similarity coefficient for `pos_label`. If `average` is not `'binary'`, 
    `pos_label` is ignored and scores for both classes are computed, then averaged or 
    both returned (when `average=None`). Similarly, for :term:`multiclass` and 
    :term:`multilabel` targets, scores for all `labels` are either returned or 
    averaged depending on the `average` parameter. Use `labels` specify the set of 
    labels to calculate the score for. 
 
    Read more in the :ref:`User Guide &lt;jaccard_similarity_score&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) labels. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Predicted labels, as returned by a classifier. 
 
    labels : array-like of shape (n_classes,), default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'micro', 'macro', 'samples', 'weighted', \ 
            'binary'} or None, default='binary' 
        If ``None``, the scores for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average, weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : &quot;warn&quot;, {0.0, 1.0}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division, i.e. when there 
        there are no negative values in predictions and labels. If set to 
        &quot;warn&quot;, this acts like 0, but a warning is also raised. 
 
    Returns 
    ------- 
    score : float or ndarray of shape (n_unique_labels,), dtype=np.float64 
        The Jaccard score. When `average` is not `None`, a single scalar is 
        returned. 
 
    See Also 
    -------- 
    accuracy_score : Function for calculating the accuracy score. 
    f1_score : Function for calculating the F1 score. 
    multilabel_confusion_matrix : Function for computing a confusion matrix\ 
                                  for each class or sample. 
 
    Notes 
    ----- 
    :func:`jaccard_score` may be a poor metric if there are no 
    positives for some samples or classes. Jaccard is undefined if there are 
    no true or predicted labels, and our implementation will return a score 
    of 0 with a warning. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Jaccard index 
           &lt;https://en.wikipedia.org/wiki/Jaccard_index&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import jaccard_score 
    &gt;&gt;&gt; y_true = np.array([[0, 1, 1], 
    ...                    [1, 1, 0]]) 
    &gt;&gt;&gt; y_pred = np.array([[1, 1, 1], 
    ...                    [1, 0, 0]]) 
 
    In the binary case: 
 
    &gt;&gt;&gt; jaccard_score(y_true[0], y_pred[0]) 
    np.float64(0.6666...) 
 
    In the 2D comparison case (e.g. image similarity): 
 
    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=&quot;micro&quot;) 
    np.float64(0.6) 
 
    In the multilabel case: 
 
    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average='samples') 
    np.float64(0.5833...) 
    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average='macro') 
    np.float64(0.6666...) 
    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=None) 
    array([0.5, 0.5, 1. ]) 
 
    In the multiclass case: 
 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 2] 
    &gt;&gt;&gt; y_true = [0, 1, 2, 2] 
    &gt;&gt;&gt; jaccard_score(y_true, y_pred, average=None) 
    array([1. , 0. , 0.33...]) 
    &quot;&quot;&quot;</span>
    <span class="s1">labels </span><span class="s4">= </span><span class="s1">_check_set_wise_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">)</span>
    <span class="s1">samplewise </span><span class="s4">= </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;samples&quot;</span>
    <span class="s1">MCM </span><span class="s4">= </span><span class="s1">multilabel_confusion_matrix</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">samplewise</span><span class="s4">=</span><span class="s1">samplewise</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s1">numerator </span><span class="s4">= </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">]</span>
    <span class="s1">denominator </span><span class="s4">= </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">] + </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">] + </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">0</span><span class="s4">]</span>

    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;micro&quot;</span><span class="s4">:</span>
        <span class="s1">numerator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">numerator</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()])</span>
        <span class="s1">denominator </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">denominator</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()])</span>

    <span class="s1">jaccard </span><span class="s4">= </span><span class="s1">_prf_divide</span><span class="s4">(</span>
        <span class="s1">numerator</span><span class="s4">,</span>
        <span class="s1">denominator</span><span class="s4">,</span>
        <span class="s5">&quot;jaccard&quot;</span><span class="s4">,</span>
        <span class="s5">&quot;true or predicted&quot;</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">,</span>
        <span class="s4">(</span><span class="s5">&quot;jaccard&quot;</span><span class="s4">,),</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">jaccard</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;weighted&quot;</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">0</span><span class="s4">] + </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">]</span>
        <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">weights</span><span class="s4">):</span>
            <span class="s2"># numerator is 0, and warning should have already been issued</span>
            <span class="s1">weights </span><span class="s4">= </span><span class="s3">None</span>
    <span class="s3">elif </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;samples&quot; </span><span class="s3">and </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s1">sample_weight</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s3">None</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">jaccard</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">matthews_corrcoef</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Matthews correlation coefficient (MCC). 
 
    The Matthews correlation coefficient is used in machine learning as a 
    measure of the quality of binary and multiclass classifications. It takes 
    into account true and false positives and negatives and is generally 
    regarded as a balanced measure which can be used even if the classes are of 
    very different sizes. The MCC is in essence a correlation coefficient value 
    between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 
    an average random prediction and -1 an inverse prediction.  The statistic 
    is also known as the phi coefficient. [source: Wikipedia] 
 
    Binary and multiclass labels are supported.  Only in the binary case does 
    this relate to information about true and false positives and negatives. 
    See references below. 
 
    Read more in the :ref:`User Guide &lt;matthews_corrcoef&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        Ground truth (correct) target values. 
 
    y_pred : array-like of shape (n_samples,) 
        Estimated targets as returned by a classifier. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
        .. versionadded:: 0.18 
 
    Returns 
    ------- 
    mcc : float 
        The Matthews correlation coefficient (+1 represents a perfect 
        prediction, 0 an average random prediction and -1 and inverse 
        prediction). 
 
    References 
    ---------- 
    .. [1] :doi:`Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the 
       accuracy of prediction algorithms for classification: an overview. 
       &lt;10.1093/bioinformatics/16.5.412&gt;` 
 
    .. [2] `Wikipedia entry for the Matthews Correlation Coefficient (phi coefficient) 
       &lt;https://en.wikipedia.org/wiki/Phi_coefficient&gt;`_. 
 
    .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a 
        K-category correlation coefficient 
        &lt;https://www.sciencedirect.com/science/article/pii/S1476927104000799&gt;`_. 
 
    .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN 
        Error Measures in MultiClass Prediction 
        &lt;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import matthews_corrcoef 
    &gt;&gt;&gt; y_true = [+1, +1, +1, -1] 
    &gt;&gt;&gt; y_pred = [+1, -1, +1, +1] 
    &gt;&gt;&gt; matthews_corrcoef(y_true, y_pred) 
    np.float64(-0.33...) 
    &quot;&quot;&quot;</span>
    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s3">not in </span><span class="s4">{</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">}:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;%s is not supported&quot; </span><span class="s4">% </span><span class="s1">y_type</span><span class="s4">)</span>

    <span class="s1">lb </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">()</span>
    <span class="s1">lb</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">hstack</span><span class="s4">([</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">]))</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">lb</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">lb</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">)</span>

    <span class="s1">C </span><span class="s4">= </span><span class="s1">confusion_matrix</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">t_sum </span><span class="s4">= </span><span class="s1">C</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
    <span class="s1">p_sum </span><span class="s4">= </span><span class="s1">C</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">0</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
    <span class="s1">n_correct </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">trace</span><span class="s4">(</span><span class="s1">C</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">)</span>
    <span class="s1">n_samples </span><span class="s4">= </span><span class="s1">p_sum</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
    <span class="s1">cov_ytyp </span><span class="s4">= </span><span class="s1">n_correct </span><span class="s4">* </span><span class="s1">n_samples </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">t_sum</span><span class="s4">, </span><span class="s1">p_sum</span><span class="s4">)</span>
    <span class="s1">cov_ypyp </span><span class="s4">= </span><span class="s1">n_samples</span><span class="s4">**</span><span class="s6">2 </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">p_sum</span><span class="s4">, </span><span class="s1">p_sum</span><span class="s4">)</span>
    <span class="s1">cov_ytyt </span><span class="s4">= </span><span class="s1">n_samples</span><span class="s4">**</span><span class="s6">2 </span><span class="s4">- </span><span class="s1">np</span><span class="s4">.</span><span class="s1">dot</span><span class="s4">(</span><span class="s1">t_sum</span><span class="s4">, </span><span class="s1">t_sum</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">cov_ypyp </span><span class="s4">* </span><span class="s1">cov_ytyt </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s6">0.0</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">cov_ytyp </span><span class="s4">/ </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">cov_ytyt </span><span class="s4">* </span><span class="s1">cov_ypyp</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;normalize&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">zero_one_loss</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Zero-one classification loss. 
 
    If normalize is ``True``, return the fraction of misclassifications 
    (float), else it returns the number of misclassifications (int). The best 
    performance is 0. 
 
    Read more in the :ref:`User Guide &lt;zero_one_loss&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) labels. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Predicted labels, as returned by a classifier. 
 
    normalize : bool, default=True 
        If ``False``, return the number of misclassifications. 
        Otherwise, return the fraction of misclassifications. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    loss : float or int, 
        If ``normalize == True``, return the fraction of misclassifications 
        (float), else it returns the number of misclassifications (int). 
 
    See Also 
    -------- 
    accuracy_score : Compute the accuracy score. By default, the function will 
        return the fraction of correct predictions divided by the total number 
        of predictions. 
    hamming_loss : Compute the average Hamming loss or Hamming distance between 
        two sets of samples. 
    jaccard_score : Compute the Jaccard similarity coefficient score. 
 
    Notes 
    ----- 
    In multilabel classification, the zero_one_loss function corresponds to 
    the subset zero-one loss: for each sample, the entire set of labels must be 
    correctly predicted, otherwise the loss for that sample is equal to one. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import zero_one_loss 
    &gt;&gt;&gt; y_pred = [1, 2, 3, 4] 
    &gt;&gt;&gt; y_true = [2, 2, 3, 4] 
    &gt;&gt;&gt; zero_one_loss(y_true, y_pred) 
    0.25 
    &gt;&gt;&gt; zero_one_loss(y_true, y_pred, normalize=False) 
    1.0 
 
    In the multilabel case with binary label indicators: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 
    0.5 
    &quot;&quot;&quot;</span>
    <span class="s1">xp</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">get_namespace</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">score </span><span class="s4">= </span><span class="s1">accuracy_score</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s1">normalize</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">normalize</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">score</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is not None</span><span class="s4">:</span>
            <span class="s1">n_samples </span><span class="s4">= </span><span class="s1">xp</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">n_samples </span><span class="s4">= </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">n_samples </span><span class="s4">- </span><span class="s1">score</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">f1_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the F1 score, also known as balanced F-score or F-measure. 
 
    The F1 score can be interpreted as a harmonic mean of the precision and 
    recall, where an F1 score reaches its best value at 1 and worst score at 0. 
    The relative contribution of precision and recall to the F1 score are 
    equal. The formula for the F1 score is: 
 
    .. math:: 
        \\text{F1} = \\frac{2 * \\text{TP}}{2 * \\text{TP} + \\text{FP} + \\text{FN}} 
 
    Where :math:`\\text{TP}` is the number of true positives, :math:`\\text{FN}` is the 
    number of false negatives, and :math:`\\text{FP}` is the number of false positives. 
    F1 is by default 
    calculated as 0.0 when there are no true positives, false negatives, or 
    false positives. 
 
    Support beyond :term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return 
    F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored 
    and F1 score for both classes are computed, then averaged or both returned (when 
    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets, 
    F1 score for all `labels` are either returned or averaged depending on the 
    `average` parameter. Use `labels` specify the set of labels to calculate F1 score 
    for. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    labels : array-like, default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
        .. versionchanged:: 0.17 
           Parameter `labels` improved for multiclass problem. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \ 
            default='binary' 
        This parameter is required for multiclass/multilabel targets. 
        If ``None``, the scores for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance; it can result in an 
            F-score that is not between precision and recall. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification where this differs from 
            :func:`accuracy_score`). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division, i.e. when all 
        predictions and labels are negative. 
 
        Notes: 
        - If set to &quot;warn&quot;, this acts like 0, but a warning is also raised. 
        - If set to `np.nan`, such values will be excluded from the average. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    f1_score : float or array of float, shape = [n_unique_labels] 
        F1 score of the positive class in binary classification or weighted 
        average of the F1 scores of each class for the multiclass task. 
 
    See Also 
    -------- 
    fbeta_score : Compute the F-beta score. 
    precision_recall_fscore_support : Compute the precision, recall, F-score, 
        and support. 
    jaccard_score : Compute the Jaccard similarity coefficient score. 
    multilabel_confusion_matrix : Compute a confusion matrix for each class or 
        sample. 
 
    Notes 
    ----- 
    When ``true positive + false positive + false negative == 0`` (i.e. a class 
    is completely absent from both ``y_true`` or ``y_pred``), f-score is 
    undefined. In such cases, by default f-score will be set to 0.0, and 
    ``UndefinedMetricWarning`` will be raised. This behavior can be modified by 
    setting the ``zero_division`` parameter. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the F1-score 
           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import f1_score 
    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2] 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1] 
    &gt;&gt;&gt; f1_score(y_true, y_pred, average='macro') 
    np.float64(0.26...) 
    &gt;&gt;&gt; f1_score(y_true, y_pred, average='micro') 
    np.float64(0.33...) 
    &gt;&gt;&gt; f1_score(y_true, y_pred, average='weighted') 
    np.float64(0.26...) 
    &gt;&gt;&gt; f1_score(y_true, y_pred, average=None) 
    array([0.8, 0. , 0. ]) 
 
    &gt;&gt;&gt; # binary classification 
    &gt;&gt;&gt; y_true_empty = [0, 0, 0, 0, 0, 0] 
    &gt;&gt;&gt; y_pred_empty = [0, 0, 0, 0, 0, 0] 
    &gt;&gt;&gt; f1_score(y_true_empty, y_pred_empty) 
    np.float64(0.0...) 
    &gt;&gt;&gt; f1_score(y_true_empty, y_pred_empty, zero_division=1.0) 
    np.float64(1.0...) 
    &gt;&gt;&gt; f1_score(y_true_empty, y_pred_empty, zero_division=np.nan) 
    nan... 
 
    &gt;&gt;&gt; # multilabel classification 
    &gt;&gt;&gt; y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]] 
    &gt;&gt;&gt; y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]] 
    &gt;&gt;&gt; f1_score(y_true, y_pred, average=None) 
    array([0.66666667, 1.        , 0.66666667]) 
    &quot;&quot;&quot;</span>
    <span class="s3">return </span><span class="s1">fbeta_score</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">beta</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">=</span><span class="s1">average</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;beta&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">fbeta_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">beta</span><span class="s4">,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the F-beta score. 
 
    The F-beta score is the weighted harmonic mean of precision and recall, 
    reaching its optimal value at 1 and its worst value at 0. 
 
    The `beta` parameter represents the ratio of recall importance to 
    precision importance. `beta &gt; 1` gives more weight to recall, while 
    `beta &lt; 1` favors precision. For example, `beta = 2` makes recall twice 
    as important as precision, while `beta = 0.5` does the opposite. 
    Asymptotically, `beta -&gt; +inf` considers only recall, and `beta -&gt; 0` 
    only precision. 
 
    The formula for F-beta score is: 
 
    .. math:: 
 
       F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}} 
                        {(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}} 
 
    Where :math:`\\text{tp}` is the number of true positives, :math:`\\text{fp}` is the 
    number of false positives, and :math:`\\text{fn}` is the number of false negatives. 
 
    Support beyond term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return 
    F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is 
    ignored and F-beta score for both classes are computed, then averaged or both 
    returned (when `average=None`). Similarly, for :term:`multiclass` and 
    :term:`multilabel` targets, F-beta score for all `labels` are either returned or 
    averaged depending on the `average` parameter. Use `labels` specify the set of 
    labels to calculate F-beta score for. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    beta : float 
        Determines the weight of recall in the combined score. 
 
    labels : array-like, default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
        .. versionchanged:: 0.17 
           Parameter `labels` improved for multiclass problem. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \ 
            default='binary' 
        This parameter is required for multiclass/multilabel targets. 
        If ``None``, the scores for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance; it can result in an 
            F-score that is not between precision and recall. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification where this differs from 
            :func:`accuracy_score`). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division, i.e. when all 
        predictions and labels are negative. 
 
        Notes: 
        - If set to &quot;warn&quot;, this acts like 0, but a warning is also raised. 
        - If set to `np.nan`, such values will be excluded from the average. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    fbeta_score : float (if average is not None) or array of float, shape =\ 
        [n_unique_labels] 
        F-beta score of the positive class in binary classification or weighted 
        average of the F-beta score of each class for the multiclass task. 
 
    See Also 
    -------- 
    precision_recall_fscore_support : Compute the precision, recall, F-score, 
        and support. 
    multilabel_confusion_matrix : Compute a confusion matrix for each class or 
        sample. 
 
    Notes 
    ----- 
    When ``true positive + false positive + false negative == 0``, f-score 
    returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be 
    modified by setting ``zero_division``. 
 
    References 
    ---------- 
    .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011). 
           Modern Information Retrieval. Addison Wesley, pp. 327-328. 
 
    .. [2] `Wikipedia entry for the F1-score 
           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import fbeta_score 
    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2] 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1] 
    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average='macro', beta=0.5) 
    np.float64(0.23...) 
    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average='micro', beta=0.5) 
    np.float64(0.33...) 
    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average='weighted', beta=0.5) 
    np.float64(0.23...) 
    &gt;&gt;&gt; fbeta_score(y_true, y_pred, average=None, beta=0.5) 
    array([0.71..., 0.        , 0.        ]) 
    &gt;&gt;&gt; y_pred_empty = [0, 0, 0, 0, 0, 0] 
    &gt;&gt;&gt; fbeta_score(y_true, y_pred_empty, 
    ...             average=&quot;macro&quot;, zero_division=np.nan, beta=0.5) 
    np.float64(0.12...) 
    &quot;&quot;&quot;</span>

    <span class="s1">_</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">f</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">beta</span><span class="s4">=</span><span class="s1">beta</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">=</span><span class="s1">average</span><span class="s4">,</span>
        <span class="s1">warn_for</span><span class="s4">=(</span><span class="s5">&quot;f-score&quot;</span><span class="s4">,),</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">f</span>


<span class="s3">def </span><span class="s1">_prf_divide</span><span class="s4">(</span>
    <span class="s1">numerator</span><span class="s4">, </span><span class="s1">denominator</span><span class="s4">, </span><span class="s1">metric</span><span class="s4">, </span><span class="s1">modifier</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">warn_for</span><span class="s4">, </span><span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Performs division and handles divide-by-zero. 
 
    On zero-division, sets the corresponding result elements equal to 
    0, 1 or np.nan (according to ``zero_division``). Plus, if 
    ``zero_division != &quot;warn&quot;`` raises a warning. 
 
    The metric, modifier and average arguments are used only for determining 
    an appropriate warning. 
    &quot;&quot;&quot;</span>
    <span class="s1">mask </span><span class="s4">= </span><span class="s1">denominator </span><span class="s4">== </span><span class="s6">0.0</span>
    <span class="s1">denominator </span><span class="s4">= </span><span class="s1">denominator</span><span class="s4">.</span><span class="s1">copy</span><span class="s4">()</span>
    <span class="s1">denominator</span><span class="s4">[</span><span class="s1">mask</span><span class="s4">] = </span><span class="s6">1  </span><span class="s2"># avoid infs/nans</span>
    <span class="s1">result </span><span class="s4">= </span><span class="s1">numerator </span><span class="s4">/ </span><span class="s1">denominator</span>

    <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">mask</span><span class="s4">):</span>
        <span class="s3">return </span><span class="s1">result</span>

    <span class="s2"># set those with 0 denominator to `zero_division`, and 0 when &quot;warn&quot;</span>
    <span class="s1">zero_division_value </span><span class="s4">= </span><span class="s1">_check_zero_division</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">)</span>
    <span class="s1">result</span><span class="s4">[</span><span class="s1">mask</span><span class="s4">] = </span><span class="s1">zero_division_value</span>

    <span class="s2"># we assume the user will be removing warnings if zero_division is set</span>
    <span class="s2"># to something different than &quot;warn&quot;. If we are computing only f-score</span>
    <span class="s2"># the warning will be raised only if precision and recall are ill-defined</span>
    <span class="s3">if </span><span class="s1">zero_division </span><span class="s4">!= </span><span class="s5">&quot;warn&quot; </span><span class="s3">or </span><span class="s1">metric </span><span class="s3">not in </span><span class="s1">warn_for</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">result</span>

    <span class="s2"># build appropriate warning</span>
    <span class="s3">if </span><span class="s1">metric </span><span class="s3">in </span><span class="s1">warn_for</span><span class="s4">:</span>
        <span class="s1">_warn_prf</span><span class="s4">(</span><span class="s1">average</span><span class="s4">, </span><span class="s1">modifier</span><span class="s4">, </span><span class="s5">f&quot;</span><span class="s3">{</span><span class="s1">metric</span><span class="s4">.</span><span class="s1">capitalize</span><span class="s4">()</span><span class="s3">} </span><span class="s5">is&quot;</span><span class="s4">, </span><span class="s1">len</span><span class="s4">(</span><span class="s1">result</span><span class="s4">))</span>

    <span class="s3">return </span><span class="s1">result</span>


<span class="s3">def </span><span class="s1">_warn_prf</span><span class="s4">(</span><span class="s1">average</span><span class="s4">, </span><span class="s1">modifier</span><span class="s4">, </span><span class="s1">msg_start</span><span class="s4">, </span><span class="s1">result_size</span><span class="s4">):</span>
    <span class="s1">axis0</span><span class="s4">, </span><span class="s1">axis1 </span><span class="s4">= </span><span class="s5">&quot;sample&quot;</span><span class="s4">, </span><span class="s5">&quot;label&quot;</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;samples&quot;</span><span class="s4">:</span>
        <span class="s1">axis0</span><span class="s4">, </span><span class="s1">axis1 </span><span class="s4">= </span><span class="s1">axis1</span><span class="s4">, </span><span class="s1">axis0</span>
    <span class="s1">msg </span><span class="s4">= (</span>
        <span class="s5">&quot;{0} ill-defined and being set to 0.0 {{0}} &quot;</span>
        <span class="s5">&quot;no {1} {2}s. Use `zero_division` parameter to control&quot;</span>
        <span class="s5">&quot; this behavior.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">msg_start</span><span class="s4">, </span><span class="s1">modifier</span><span class="s4">, </span><span class="s1">axis0</span><span class="s4">)</span>
    <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">result_size </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s1">msg </span><span class="s4">= </span><span class="s1">msg</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s5">&quot;due to&quot;</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">msg </span><span class="s4">= </span><span class="s1">msg</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s5">&quot;in {0}s with&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">axis1</span><span class="s4">))</span>
    <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UndefinedMetricWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s6">2</span><span class="s4">)</span>


<span class="s3">def </span><span class="s1">_check_set_wise_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Validation associated with set-wise metrics. 
 
    Returns identified labels. 
    &quot;&quot;&quot;</span>
    <span class="s1">average_options </span><span class="s4">= (</span><span class="s3">None</span><span class="s4">, </span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s3">not in </span><span class="s1">average_options </span><span class="s3">and </span><span class="s1">average </span><span class="s4">!= </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;average has to be one of &quot; </span><span class="s4">+ </span><span class="s1">str</span><span class="s4">(</span><span class="s1">average_options</span><span class="s4">))</span>

    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s2"># Convert to Python primitive type to avoid NumPy type / Python str</span>
    <span class="s2"># comparison. See https://github.com/numpy/numpy/issues/6784</span>
    <span class="s1">present_labels </span><span class="s4">= </span><span class="s1">unique_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">).</span><span class="s1">tolist</span><span class="s4">()</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">pos_label </span><span class="s3">not in </span><span class="s1">present_labels</span><span class="s4">:</span>
                <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">present_labels</span><span class="s4">) &gt;= </span><span class="s6">2</span><span class="s4">:</span>
                    <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                        <span class="s5">f&quot;pos_label=</span><span class="s3">{</span><span class="s1">pos_label</span><span class="s3">} </span><span class="s5">is not a valid label. It &quot;</span>
                        <span class="s5">f&quot;should be one of </span><span class="s3">{</span><span class="s1">present_labels</span><span class="s3">}</span><span class="s5">&quot;</span>
                    <span class="s4">)</span>
            <span class="s1">labels </span><span class="s4">= [</span><span class="s1">pos_label</span><span class="s4">]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">average_options </span><span class="s4">= </span><span class="s1">list</span><span class="s4">(</span><span class="s1">average_options</span><span class="s4">)</span>
            <span class="s3">if </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">:</span>
                <span class="s1">average_options</span><span class="s4">.</span><span class="s1">remove</span><span class="s4">(</span><span class="s5">&quot;samples&quot;</span><span class="s4">)</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Target is %s but average='binary'. Please &quot;</span>
                <span class="s5">&quot;choose another average setting, one of %r.&quot; </span><span class="s4">% (</span><span class="s1">y_type</span><span class="s4">, </span><span class="s1">average_options</span><span class="s4">)</span>
            <span class="s4">)</span>
    <span class="s3">elif </span><span class="s1">pos_label </span><span class="s3">not in </span><span class="s4">(</span><span class="s3">None</span><span class="s4">, </span><span class="s6">1</span><span class="s4">):</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s5">&quot;Note that pos_label (set to %r) is ignored when &quot;</span>
            <span class="s5">&quot;average != 'binary' (got %r). You may use &quot;</span>
            <span class="s5">&quot;labels=[pos_label] to specify a single positive class.&quot;</span>
            <span class="s4">% (</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">average</span><span class="s4">),</span>
            <span class="s1">UserWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">labels</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;beta&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, </span><span class="s6">0.0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;both&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;warn_for&quot;</span><span class="s4">: [</span><span class="s1">list</span><span class="s4">, </span><span class="s1">tuple</span><span class="s4">, </span><span class="s1">set</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">beta</span><span class="s4">=</span><span class="s6">1.0</span><span class="s4">,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">warn_for</span><span class="s4">=(</span><span class="s5">&quot;precision&quot;</span><span class="s4">, </span><span class="s5">&quot;recall&quot;</span><span class="s4">, </span><span class="s5">&quot;f-score&quot;</span><span class="s4">),</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute precision, recall, F-measure and support for each class. 
 
    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of 
    true positives and ``fp`` the number of false positives. The precision is 
    intuitively the ability of the classifier not to label a negative sample as 
    positive. 
 
    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of 
    true positives and ``fn`` the number of false negatives. The recall is 
    intuitively the ability of the classifier to find all the positive samples. 
 
    The F-beta score can be interpreted as a weighted harmonic mean of 
    the precision and recall, where an F-beta score reaches its best 
    value at 1 and worst score at 0. 
 
    The F-beta score weights recall more than precision by a factor of 
    ``beta``. ``beta == 1.0`` means recall and precision are equally important. 
 
    The support is the number of occurrences of each class in ``y_true``. 
 
    Support beyond term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return 
    metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored 
    and metrics for both classes are computed, then averaged or both returned (when 
    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets, 
    metrics for all `labels` are either returned or averaged depending on the `average` 
    parameter. Use `labels` specify the set of labels to calculate metrics for. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    beta : float, default=1.0 
        The strength of recall versus precision in the F-score. 
 
    labels : array-like, default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'binary', 'micro', 'macro', 'samples', 'weighted'}, \ 
            default=None 
        If ``None``, the metrics for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance; it can result in an 
            F-score that is not between precision and recall. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification where this differs from 
            :func:`accuracy_score`). 
 
    warn_for : list, tuple or set, for internal use 
        This determines which warnings will be made in the case that this 
        function is being used to return only one of its metrics. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division: 
           - recall: when there are no positive labels 
           - precision: when there are no positive predictions 
           - f-score: both 
 
        Notes: 
        - If set to &quot;warn&quot;, this acts like 0, but a warning is also raised. 
        - If set to `np.nan`, such values will be excluded from the average. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    precision : float (if average is not None) or array of float, shape =\ 
        [n_unique_labels] 
        Precision score. 
 
    recall : float (if average is not None) or array of float, shape =\ 
        [n_unique_labels] 
        Recall score. 
 
    fbeta_score : float (if average is not None) or array of float, shape =\ 
        [n_unique_labels] 
        F-beta score. 
 
    support : None (if average is not None) or array of int, shape =\ 
        [n_unique_labels] 
        The number of occurrences of each label in ``y_true``. 
 
    Notes 
    ----- 
    When ``true positive + false positive == 0``, precision is undefined. 
    When ``true positive + false negative == 0``, recall is undefined. When 
    ``true positive + false negative + false positive == 0``, f-score is 
    undefined. In such cases, by default the metric will be set to 0, and 
    ``UndefinedMetricWarning`` will be raised. This behavior can be modified 
    with ``zero_division``. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Precision and recall 
           &lt;https://en.wikipedia.org/wiki/Precision_and_recall&gt;`_. 
 
    .. [2] `Wikipedia entry for the F1-score 
           &lt;https://en.wikipedia.org/wiki/F1_score&gt;`_. 
 
    .. [3] `Discriminative Methods for Multi-labeled Classification Advances 
           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu 
           Godbole, Sunita Sarawagi 
           &lt;http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import precision_recall_fscore_support 
    &gt;&gt;&gt; y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig']) 
    &gt;&gt;&gt; y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog']) 
    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='macro') 
    (np.float64(0.22...), np.float64(0.33...), np.float64(0.26...), None) 
    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='micro') 
    (np.float64(0.33...), np.float64(0.33...), np.float64(0.33...), None) 
    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average='weighted') 
    (np.float64(0.22...), np.float64(0.33...), np.float64(0.26...), None) 
 
    It is possible to compute per-label precisions, recalls, F1-scores and 
    supports instead of averaging: 
 
    &gt;&gt;&gt; precision_recall_fscore_support(y_true, y_pred, average=None, 
    ... labels=['pig', 'dog', 'cat']) 
    (array([0.        , 0.        , 0.66...]), 
     array([0., 0., 1.]), array([0. , 0. , 0.8]), 
     array([2, 2, 2])) 
    &quot;&quot;&quot;</span>
    <span class="s1">_check_zero_division</span><span class="s4">(</span><span class="s1">zero_division</span><span class="s4">)</span>
    <span class="s1">labels </span><span class="s4">= </span><span class="s1">_check_set_wise_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">)</span>

    <span class="s2"># Calculate tp_sum, pred_sum, true_sum ###</span>
    <span class="s1">samplewise </span><span class="s4">= </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;samples&quot;</span>
    <span class="s1">MCM </span><span class="s4">= </span><span class="s1">multilabel_confusion_matrix</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">samplewise</span><span class="s4">=</span><span class="s1">samplewise</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">]</span>
    <span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">tp_sum </span><span class="s4">+ </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">0</span><span class="s4">, </span><span class="s6">1</span><span class="s4">]</span>
    <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">tp_sum </span><span class="s4">+ </span><span class="s1">MCM</span><span class="s4">[:, </span><span class="s6">1</span><span class="s4">, </span><span class="s6">0</span><span class="s4">]</span>

    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;micro&quot;</span><span class="s4">:</span>
        <span class="s1">tp_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">tp_sum</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()])</span>
        <span class="s1">pred_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">pred_sum</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()])</span>
        <span class="s1">true_sum </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">([</span><span class="s1">true_sum</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()])</span>

    <span class="s2"># Finally, we have all our sufficient statistics. Divide! #</span>
    <span class="s1">beta2 </span><span class="s4">= </span><span class="s1">beta</span><span class="s4">**</span><span class="s6">2</span>

    <span class="s2"># Divide, and on zero-division, set scores and/or warn according to</span>
    <span class="s2"># zero_division:</span>
    <span class="s1">precision </span><span class="s4">= </span><span class="s1">_prf_divide</span><span class="s4">(</span>
        <span class="s1">tp_sum</span><span class="s4">, </span><span class="s1">pred_sum</span><span class="s4">, </span><span class="s5">&quot;precision&quot;</span><span class="s4">, </span><span class="s5">&quot;predicted&quot;</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">warn_for</span><span class="s4">, </span><span class="s1">zero_division</span>
    <span class="s4">)</span>
    <span class="s1">recall </span><span class="s4">= </span><span class="s1">_prf_divide</span><span class="s4">(</span>
        <span class="s1">tp_sum</span><span class="s4">, </span><span class="s1">true_sum</span><span class="s4">, </span><span class="s5">&quot;recall&quot;</span><span class="s4">, </span><span class="s5">&quot;true&quot;</span><span class="s4">, </span><span class="s1">average</span><span class="s4">, </span><span class="s1">warn_for</span><span class="s4">, </span><span class="s1">zero_division</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">isposinf</span><span class="s4">(</span><span class="s1">beta</span><span class="s4">):</span>
        <span class="s1">f_score </span><span class="s4">= </span><span class="s1">recall</span>
    <span class="s3">elif </span><span class="s1">beta </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s1">f_score </span><span class="s4">= </span><span class="s1">precision</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s2"># The score is defined as:</span>
        <span class="s2"># score = (1 + beta**2) * precision * recall / (beta**2 * precision + recall)</span>
        <span class="s2"># Therefore, we can express the score in terms of confusion matrix entries as:</span>
        <span class="s2"># score = (1 + beta**2) * tp / ((1 + beta**2) * tp + beta**2 * fn + fp)</span>
        <span class="s1">denom </span><span class="s4">= </span><span class="s1">beta2 </span><span class="s4">* </span><span class="s1">true_sum </span><span class="s4">+ </span><span class="s1">pred_sum</span>
        <span class="s1">f_score </span><span class="s4">= </span><span class="s1">_prf_divide</span><span class="s4">(</span>
            <span class="s4">(</span><span class="s6">1 </span><span class="s4">+ </span><span class="s1">beta2</span><span class="s4">) * </span><span class="s1">tp_sum</span><span class="s4">,</span>
            <span class="s1">denom</span><span class="s4">,</span>
            <span class="s5">&quot;f-score&quot;</span><span class="s4">,</span>
            <span class="s5">&quot;true nor predicted&quot;</span><span class="s4">,</span>
            <span class="s1">average</span><span class="s4">,</span>
            <span class="s1">warn_for</span><span class="s4">,</span>
            <span class="s1">zero_division</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s2"># Average the results</span>
    <span class="s3">if </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;weighted&quot;</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s1">true_sum</span>
    <span class="s3">elif </span><span class="s1">average </span><span class="s4">== </span><span class="s5">&quot;samples&quot;</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s1">sample_weight</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">weights </span><span class="s4">= </span><span class="s3">None</span>

    <span class="s3">if </span><span class="s1">average </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s3">assert </span><span class="s1">average </span><span class="s4">!= </span><span class="s5">&quot;binary&quot; </span><span class="s3">or </span><span class="s1">len</span><span class="s4">(</span><span class="s1">precision</span><span class="s4">) == </span><span class="s6">1</span>
        <span class="s1">precision </span><span class="s4">= </span><span class="s1">_nanaverage</span><span class="s4">(</span><span class="s1">precision</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>
        <span class="s1">recall </span><span class="s4">= </span><span class="s1">_nanaverage</span><span class="s4">(</span><span class="s1">recall</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>
        <span class="s1">f_score </span><span class="s4">= </span><span class="s1">_nanaverage</span><span class="s4">(</span><span class="s1">f_score</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>
        <span class="s1">true_sum </span><span class="s4">= </span><span class="s3">None  </span><span class="s2"># return no support</span>

    <span class="s3">return </span><span class="s1">precision</span><span class="s4">, </span><span class="s1">recall</span><span class="s4">, </span><span class="s1">f_score</span><span class="s4">, </span><span class="s1">true_sum</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;raise_warning&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">class_likelihood_ratios</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">raise_warning</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute binary classification positive and negative likelihood ratios. 
 
    The positive likelihood ratio is `LR+ = sensitivity / (1 - specificity)` 
    where the sensitivity or recall is the ratio `tp / (tp + fn)` and the 
    specificity is `tn / (tn + fp)`. The negative likelihood ratio is `LR- = (1 
    - sensitivity) / specificity`. Here `tp` is the number of true positives, 
    `fp` the number of false positives, `tn` is the number of true negatives and 
    `fn` the number of false negatives. Both class likelihood ratios can be used 
    to obtain post-test probabilities given a pre-test probability. 
 
    `LR+` ranges from 1 to infinity. A `LR+` of 1 indicates that the probability 
    of predicting the positive class is the same for samples belonging to either 
    class; therefore, the test is useless. The greater `LR+` is, the more a 
    positive prediction is likely to be a true positive when compared with the 
    pre-test probability. A value of `LR+` lower than 1 is invalid as it would 
    indicate that the odds of a sample being a true positive decrease with 
    respect to the pre-test odds. 
 
    `LR-` ranges from 0 to 1. The closer it is to 0, the lower the probability 
    of a given sample to be a false negative. A `LR-` of 1 means the test is 
    useless because the odds of having the condition did not change after the 
    test. A value of `LR-` greater than 1 invalidates the classifier as it 
    indicates an increase in the odds of a sample belonging to the positive 
    class after being classified as negative. This is the case when the 
    classifier systematically predicts the opposite of the true label. 
 
    A typical application in medicine is to identify the positive/negative class 
    to the presence/absence of a disease, respectively; the classifier being a 
    diagnostic test; the pre-test probability of an individual having the 
    disease can be the prevalence of such disease (proportion of a particular 
    population found to be affected by a medical condition); and the post-test 
    probabilities would be the probability that the condition is truly present 
    given a positive test result. 
 
    Read more in the :ref:`User Guide &lt;class_likelihood_ratios&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    labels : array-like, default=None 
        List of labels to index the matrix. This may be used to select the 
        positive and negative classes with the ordering `labels=[negative_class, 
        positive_class]`. If `None` is given, those that appear at least once in 
        `y_true` or `y_pred` are used in sorted order. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    raise_warning : bool, default=True 
        Whether or not a case-specific warning message is raised when there is a 
        zero division. Even if the error is not raised, the function will return 
        nan in such cases. 
 
    Returns 
    ------- 
    (positive_likelihood_ratio, negative_likelihood_ratio) : tuple 
        A tuple of two float, the first containing the Positive likelihood ratio 
        and the second the Negative likelihood ratio. 
 
    Warns 
    ----- 
    When `false positive == 0`, the positive likelihood ratio is undefined. 
    When `true negative == 0`, the negative likelihood ratio is undefined. 
    When `true positive + false negative == 0` both ratios are undefined. 
    In such cases, `UserWarning` will be raised if raise_warning=True. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Likelihood ratios in diagnostic testing 
           &lt;https://en.wikipedia.org/wiki/Likelihood_ratios_in_diagnostic_testing&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import class_likelihood_ratios 
    &gt;&gt;&gt; class_likelihood_ratios([0, 1, 0, 1, 0], [1, 1, 0, 0, 0]) 
    (np.float64(1.5), np.float64(0.75)) 
    &gt;&gt;&gt; y_true = np.array([&quot;non-cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;]) 
    &gt;&gt;&gt; y_pred = np.array([&quot;cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;, &quot;non-cat&quot;, &quot;non-cat&quot;]) 
    &gt;&gt;&gt; class_likelihood_ratios(y_true, y_pred) 
    (np.float64(1.33...), np.float64(0.66...)) 
    &gt;&gt;&gt; y_true = np.array([&quot;non-zebra&quot;, &quot;zebra&quot;, &quot;non-zebra&quot;, &quot;zebra&quot;, &quot;non-zebra&quot;]) 
    &gt;&gt;&gt; y_pred = np.array([&quot;zebra&quot;, &quot;zebra&quot;, &quot;non-zebra&quot;, &quot;non-zebra&quot;, &quot;non-zebra&quot;]) 
    &gt;&gt;&gt; class_likelihood_ratios(y_true, y_pred) 
    (np.float64(1.5), np.float64(0.75)) 
 
    To avoid ambiguities, use the notation `labels=[negative_class, 
    positive_class]` 
 
    &gt;&gt;&gt; y_true = np.array([&quot;non-cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;]) 
    &gt;&gt;&gt; y_pred = np.array([&quot;cat&quot;, &quot;cat&quot;, &quot;non-cat&quot;, &quot;non-cat&quot;, &quot;non-cat&quot;]) 
    &gt;&gt;&gt; class_likelihood_ratios(y_true, y_pred, labels=[&quot;non-cat&quot;, &quot;cat&quot;]) 
    (np.float64(1.5), np.float64(0.75)) 
    &quot;&quot;&quot;</span>

    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">!= </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;class_likelihood_ratios only supports binary classification &quot;</span>
            <span class="s5">f&quot;problems, got targets of type: </span><span class="s3">{</span><span class="s1">y_type</span><span class="s3">}</span><span class="s5">&quot;</span>
        <span class="s4">)</span>

    <span class="s1">cm </span><span class="s4">= </span><span class="s1">confusion_matrix</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
    <span class="s4">)</span>

    <span class="s2"># Case when `y_test` contains a single class and `y_test == y_pred`.</span>
    <span class="s2"># This may happen when cross-validating imbalanced data and should</span>
    <span class="s2"># not be interpreted as a perfect score.</span>
    <span class="s3">if </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">shape </span><span class="s4">== (</span><span class="s6">1</span><span class="s4">, </span><span class="s6">1</span><span class="s4">):</span>
        <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;samples of only one class were seen during testing &quot;</span>
        <span class="s3">if </span><span class="s1">raise_warning</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UserWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s6">2</span><span class="s4">)</span>
        <span class="s1">positive_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
        <span class="s1">negative_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">tn</span><span class="s4">, </span><span class="s1">fp</span><span class="s4">, </span><span class="s1">fn</span><span class="s4">, </span><span class="s1">tp </span><span class="s4">= </span><span class="s1">cm</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">()</span>
        <span class="s1">support_pos </span><span class="s4">= </span><span class="s1">tp </span><span class="s4">+ </span><span class="s1">fn</span>
        <span class="s1">support_neg </span><span class="s4">= </span><span class="s1">tn </span><span class="s4">+ </span><span class="s1">fp</span>
        <span class="s1">pos_num </span><span class="s4">= </span><span class="s1">tp </span><span class="s4">* </span><span class="s1">support_neg</span>
        <span class="s1">pos_denom </span><span class="s4">= </span><span class="s1">fp </span><span class="s4">* </span><span class="s1">support_pos</span>
        <span class="s1">neg_num </span><span class="s4">= </span><span class="s1">fn </span><span class="s4">* </span><span class="s1">support_neg</span>
        <span class="s1">neg_denom </span><span class="s4">= </span><span class="s1">tn </span><span class="s4">* </span><span class="s1">support_pos</span>

        <span class="s2"># If zero division warn and set scores to nan, else divide</span>
        <span class="s3">if </span><span class="s1">support_pos </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;no samples of the positive class were present in the testing set &quot;</span>
            <span class="s3">if </span><span class="s1">raise_warning</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UserWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s6">2</span><span class="s4">)</span>
            <span class="s1">positive_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
            <span class="s1">negative_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
        <span class="s3">if </span><span class="s1">fp </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">tp </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
                <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;no samples predicted for the positive class&quot;</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;positive_likelihood_ratio ill-defined and being set to nan &quot;</span>
            <span class="s3">if </span><span class="s1">raise_warning</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UserWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s6">2</span><span class="s4">)</span>
            <span class="s1">positive_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">positive_likelihood_ratio </span><span class="s4">= </span><span class="s1">pos_num </span><span class="s4">/ </span><span class="s1">pos_denom</span>
        <span class="s3">if </span><span class="s1">tn </span><span class="s4">== </span><span class="s6">0</span><span class="s4">:</span>
            <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;negative_likelihood_ratio ill-defined and being set to nan &quot;</span>
            <span class="s3">if </span><span class="s1">raise_warning</span><span class="s4">:</span>
                <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UserWarning</span><span class="s4">, </span><span class="s1">stacklevel</span><span class="s4">=</span><span class="s6">2</span><span class="s4">)</span>
            <span class="s1">negative_likelihood_ratio </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">nan</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">negative_likelihood_ratio </span><span class="s4">= </span><span class="s1">neg_num </span><span class="s4">/ </span><span class="s1">neg_denom</span>

    <span class="s3">return </span><span class="s1">positive_likelihood_ratio</span><span class="s4">, </span><span class="s1">negative_likelihood_ratio</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">precision_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the precision. 
 
    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of 
    true positives and ``fp`` the number of false positives. The precision is 
    intuitively the ability of the classifier not to label as positive a sample 
    that is negative. 
 
    The best value is 1 and the worst value is 0. 
 
    Support beyond term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return 
    precision for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored 
    and precision for both classes are computed, then averaged or both returned (when 
    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets, 
    precision for all `labels` are either returned or averaged depending on the 
    `average` parameter. Use `labels` specify the set of labels to calculate precision 
    for. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    labels : array-like, default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
        .. versionchanged:: 0.17 
           Parameter `labels` improved for multiclass problem. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \ 
            default='binary' 
        This parameter is required for multiclass/multilabel targets. 
        If ``None``, the scores for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance; it can result in an 
            F-score that is not between precision and recall. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification where this differs from 
            :func:`accuracy_score`). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division. 
 
        Notes: 
        - If set to &quot;warn&quot;, this acts like 0, but a warning is also raised. 
        - If set to `np.nan`, such values will be excluded from the average. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    precision : float (if average is not None) or array of float of shape \ 
                (n_unique_labels,) 
        Precision of the positive class in binary classification or weighted 
        average of the precision of each class for the multiclass task. 
 
    See Also 
    -------- 
    precision_recall_fscore_support : Compute precision, recall, F-measure and 
        support for each class. 
    recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the 
        number of true positives and ``fn`` the number of false negatives. 
    PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given 
        an estimator and some data. 
    PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given 
        binary class predictions. 
    multilabel_confusion_matrix : Compute a confusion matrix for each class or 
        sample. 
 
    Notes 
    ----- 
    When ``true positive + false positive == 0``, precision returns 0 and 
    raises ``UndefinedMetricWarning``. This behavior can be 
    modified with ``zero_division``. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import precision_score 
    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2] 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1] 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average='macro') 
    np.float64(0.22...) 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average='micro') 
    np.float64(0.33...) 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average='weighted') 
    np.float64(0.22...) 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None) 
    array([0.66..., 0.        , 0.        ]) 
    &gt;&gt;&gt; y_pred = [0, 0, 0, 0, 0, 0] 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None) 
    array([0.33..., 0.        , 0.        ]) 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None, zero_division=1) 
    array([0.33..., 1.        , 1.        ]) 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None, zero_division=np.nan) 
    array([0.33...,        nan,        nan]) 
 
    &gt;&gt;&gt; # multilabel classification 
    &gt;&gt;&gt; y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]] 
    &gt;&gt;&gt; y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]] 
    &gt;&gt;&gt; precision_score(y_true, y_pred, average=None) 
    array([0.5, 1. , 1. ]) 
    &quot;&quot;&quot;</span>
    <span class="s1">p</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">=</span><span class="s1">average</span><span class="s4">,</span>
        <span class="s1">warn_for</span><span class="s4">=(</span><span class="s5">&quot;precision&quot;</span><span class="s4">,),</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">p</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;average&quot;</span><span class="s4">: [</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;binary&quot;</span><span class="s4">}),</span>
            <span class="s3">None</span><span class="s4">,</span>
        <span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">recall_score</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">pos_label</span><span class="s4">=</span><span class="s6">1</span><span class="s4">,</span>
    <span class="s1">average</span><span class="s4">=</span><span class="s5">&quot;binary&quot;</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the recall. 
 
    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of 
    true positives and ``fn`` the number of false negatives. The recall is 
    intuitively the ability of the classifier to find all the positive samples. 
 
    The best value is 1 and the worst value is 0. 
 
    Support beyond term:`binary` targets is achieved by treating :term:`multiclass` 
    and :term:`multilabel` data as a collection of binary problems, one for each 
    label. For the :term:`binary` case, setting `average='binary'` will return 
    recall for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored 
    and recall for both classes are computed then averaged or both returned (when 
    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets, 
    recall for all `labels` are either returned or averaged depending on the `average` 
    parameter. Use `labels` specify the set of labels to calculate recall for. 
 
    Read more in the :ref:`User Guide &lt;precision_recall_f_measure_metrics&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    labels : array-like, default=None 
        The set of labels to include when `average != 'binary'`, and their 
        order if `average is None`. Labels present in the data can be 
        excluded, for example in multiclass classification to exclude a &quot;negative 
        class&quot;. Labels not present in the data can be included and will be 
        &quot;assigned&quot; 0 samples. For multilabel targets, labels are column indices. 
        By default, all labels in `y_true` and `y_pred` are used in sorted order. 
 
        .. versionchanged:: 0.17 
           Parameter `labels` improved for multiclass problem. 
 
    pos_label : int, float, bool or str, default=1 
        The class to report if `average='binary'` and the data is binary, 
        otherwise this parameter is ignored. 
        For multiclass or multilabel targets, set `labels=[pos_label]` and 
        `average != 'binary'` to report metrics for one label only. 
 
    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \ 
            default='binary' 
        This parameter is required for multiclass/multilabel targets. 
        If ``None``, the scores for each class are returned. Otherwise, this 
        determines the type of averaging performed on the data: 
 
        ``'binary'``: 
            Only report results for the class specified by ``pos_label``. 
            This is applicable only if targets (``y_{true,pred}``) are binary. 
        ``'micro'``: 
            Calculate metrics globally by counting the total true positives, 
            false negatives and false positives. 
        ``'macro'``: 
            Calculate metrics for each label, and find their unweighted 
            mean.  This does not take label imbalance into account. 
        ``'weighted'``: 
            Calculate metrics for each label, and find their average weighted 
            by support (the number of true instances for each label). This 
            alters 'macro' to account for label imbalance; it can result in an 
            F-score that is not between precision and recall. Weighted recall 
            is equal to accuracy. 
        ``'samples'``: 
            Calculate metrics for each instance, and find their average (only 
            meaningful for multilabel classification where this differs from 
            :func:`accuracy_score`). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division. 
 
        Notes: 
        - If set to &quot;warn&quot;, this acts like 0, but a warning is also raised. 
        - If set to `np.nan`, such values will be excluded from the average. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    recall : float (if average is not None) or array of float of shape \ 
             (n_unique_labels,) 
        Recall of the positive class in binary classification or weighted 
        average of the recall of each class for the multiclass task. 
 
    See Also 
    -------- 
    precision_recall_fscore_support : Compute precision, recall, F-measure and 
        support for each class. 
    precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the 
        number of true positives and ``fp`` the number of false positives. 
    balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced 
        datasets. 
    multilabel_confusion_matrix : Compute a confusion matrix for each class or 
        sample. 
    PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given 
        an estimator and some data. 
    PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given 
        binary class predictions. 
 
    Notes 
    ----- 
    When ``true positive + false negative == 0``, recall returns 0 and raises 
    ``UndefinedMetricWarning``. This behavior can be modified with 
    ``zero_division``. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import recall_score 
    &gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2] 
    &gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1] 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average='macro') 
    np.float64(0.33...) 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average='micro') 
    np.float64(0.33...) 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average='weighted') 
    np.float64(0.33...) 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None) 
    array([1., 0., 0.]) 
    &gt;&gt;&gt; y_true = [0, 0, 0, 0, 0, 0] 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None) 
    array([0.5, 0. , 0. ]) 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None, zero_division=1) 
    array([0.5, 1. , 1. ]) 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None, zero_division=np.nan) 
    array([0.5, nan, nan]) 
 
    &gt;&gt;&gt; # multilabel classification 
    &gt;&gt;&gt; y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]] 
    &gt;&gt;&gt; y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]] 
    &gt;&gt;&gt; recall_score(y_true, y_pred, average=None) 
    array([1. , 1. , 0.5]) 
    &quot;&quot;&quot;</span>
    <span class="s1">_</span><span class="s4">, </span><span class="s1">r</span><span class="s4">, </span><span class="s1">_</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">pos_label</span><span class="s4">=</span><span class="s1">pos_label</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">=</span><span class="s1">average</span><span class="s4">,</span>
        <span class="s1">warn_for</span><span class="s4">=(</span><span class="s5">&quot;recall&quot;</span><span class="s4">,),</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s3">return </span><span class="s1">r</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;adjusted&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">balanced_accuracy_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">adjusted</span><span class="s4">=</span><span class="s3">False</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the balanced accuracy. 
 
    The balanced accuracy in binary and multiclass classification problems to 
    deal with imbalanced datasets. It is defined as the average of recall 
    obtained on each class. 
 
    The best value is 1 and the worst value is 0 when ``adjusted=False``. 
 
    Read more in the :ref:`User Guide &lt;balanced_accuracy_score&gt;`. 
 
    .. versionadded:: 0.20 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        Ground truth (correct) target values. 
 
    y_pred : array-like of shape (n_samples,) 
        Estimated targets as returned by a classifier. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    adjusted : bool, default=False 
        When true, the result is adjusted for chance, so that random 
        performance would score 0, while keeping perfect performance at a score 
        of 1. 
 
    Returns 
    ------- 
    balanced_accuracy : float 
        Balanced accuracy score. 
 
    See Also 
    -------- 
    average_precision_score : Compute average precision (AP) from prediction 
        scores. 
    precision_score : Compute the precision score. 
    recall_score : Compute the recall score. 
    roc_auc_score : Compute Area Under the Receiver Operating Characteristic 
        Curve (ROC AUC) from prediction scores. 
 
    Notes 
    ----- 
    Some literature promotes alternative definitions of balanced accuracy. Our 
    definition is equivalent to :func:`accuracy_score` with class-balanced 
    sample weights, and shares desirable properties with the binary case. 
    See the :ref:`User Guide &lt;balanced_accuracy_score&gt;`. 
 
    References 
    ---------- 
    .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010). 
           The balanced accuracy and its posterior distribution. 
           Proceedings of the 20th International Conference on Pattern 
           Recognition, 3121-24. 
    .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015). 
           `Fundamentals of Machine Learning for Predictive Data Analytics: 
           Algorithms, Worked Examples, and Case Studies 
           &lt;https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import balanced_accuracy_score 
    &gt;&gt;&gt; y_true = [0, 1, 0, 0, 1, 0] 
    &gt;&gt;&gt; y_pred = [0, 1, 0, 0, 0, 1] 
    &gt;&gt;&gt; balanced_accuracy_score(y_true, y_pred) 
    np.float64(0.625) 
    &quot;&quot;&quot;</span>
    <span class="s1">C </span><span class="s4">= </span><span class="s1">confusion_matrix</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">with </span><span class="s1">np</span><span class="s4">.</span><span class="s1">errstate</span><span class="s4">(</span><span class="s1">divide</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">, </span><span class="s1">invalid</span><span class="s4">=</span><span class="s5">&quot;ignore&quot;</span><span class="s4">):</span>
        <span class="s1">per_class </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">diag</span><span class="s4">(</span><span class="s1">C</span><span class="s4">) / </span><span class="s1">C</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">np</span><span class="s4">.</span><span class="s1">any</span><span class="s4">(</span><span class="s1">np</span><span class="s4">.</span><span class="s1">isnan</span><span class="s4">(</span><span class="s1">per_class</span><span class="s4">)):</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s5">&quot;y_pred contains classes not in y_true&quot;</span><span class="s4">)</span>
        <span class="s1">per_class </span><span class="s4">= </span><span class="s1">per_class</span><span class="s4">[~</span><span class="s1">np</span><span class="s4">.</span><span class="s1">isnan</span><span class="s4">(</span><span class="s1">per_class</span><span class="s4">)]</span>
    <span class="s1">score </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">(</span><span class="s1">per_class</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">adjusted</span><span class="s4">:</span>
        <span class="s1">n_classes </span><span class="s4">= </span><span class="s1">len</span><span class="s4">(</span><span class="s1">per_class</span><span class="s4">)</span>
        <span class="s1">chance </span><span class="s4">= </span><span class="s6">1 </span><span class="s4">/ </span><span class="s1">n_classes</span>
        <span class="s1">score </span><span class="s4">-= </span><span class="s1">chance</span>
        <span class="s1">score </span><span class="s4">/= </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">chance</span>
    <span class="s3">return </span><span class="s1">score</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;target_names&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;digits&quot;</span><span class="s4">: [</span><span class="s1">Interval</span><span class="s4">(</span><span class="s1">Integral</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">closed</span><span class="s4">=</span><span class="s5">&quot;left&quot;</span><span class="s4">)],</span>
        <span class="s5">&quot;output_dict&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;zero_division&quot;</span><span class="s4">: [</span>
            <span class="s1">Options</span><span class="s4">(</span><span class="s1">Real</span><span class="s4">, {</span><span class="s6">0.0</span><span class="s4">, </span><span class="s6">1.0</span><span class="s4">}),</span>
            <span class="s5">&quot;nan&quot;</span><span class="s4">,</span>
            <span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;warn&quot;</span><span class="s4">}),</span>
        <span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">classification_report</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">,</span>
    <span class="s1">y_pred</span><span class="s4">,</span>
    <span class="s4">*,</span>
    <span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">target_names</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
    <span class="s1">digits</span><span class="s4">=</span><span class="s6">2</span><span class="s4">,</span>
    <span class="s1">output_dict</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
    <span class="s1">zero_division</span><span class="s4">=</span><span class="s5">&quot;warn&quot;</span><span class="s4">,</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Build a text report showing the main classification metrics. 
 
    Read more in the :ref:`User Guide &lt;classification_report&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) target values. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Estimated targets as returned by a classifier. 
 
    labels : array-like of shape (n_labels,), default=None 
        Optional list of label indices to include in the report. 
 
    target_names : array-like of shape (n_labels,), default=None 
        Optional display names matching the labels (same order). 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    digits : int, default=2 
        Number of digits for formatting output floating point values. 
        When ``output_dict`` is ``True``, this will be ignored and the 
        returned values will not be rounded. 
 
    output_dict : bool, default=False 
        If True, return output as dict. 
 
        .. versionadded:: 0.20 
 
    zero_division : {&quot;warn&quot;, 0.0, 1.0, np.nan}, default=&quot;warn&quot; 
        Sets the value to return when there is a zero division. If set to 
        &quot;warn&quot;, this acts as 0, but warnings are also raised. 
 
        .. versionadded:: 1.3 
           `np.nan` option was added. 
 
    Returns 
    ------- 
    report : str or dict 
        Text summary of the precision, recall, F1 score for each class. 
        Dictionary returned if output_dict is True. Dictionary has the 
        following structure:: 
 
            {'label 1': {'precision':0.5, 
                         'recall':1.0, 
                         'f1-score':0.67, 
                         'support':1}, 
             'label 2': { ... }, 
              ... 
            } 
 
        The reported averages include macro average (averaging the unweighted 
        mean per label), weighted average (averaging the support-weighted mean 
        per label), and sample average (only for multilabel classification). 
        Micro average (averaging the total true positives, false negatives and 
        false positives) is only shown for multi-label or multi-class 
        with a subset of classes, because it corresponds to accuracy 
        otherwise and would be the same for all metrics. 
        See also :func:`precision_recall_fscore_support` for more details 
        on averages. 
 
        Note that in binary classification, recall of the positive class 
        is also known as &quot;sensitivity&quot;; recall of the negative class is 
        &quot;specificity&quot;. 
 
    See Also 
    -------- 
    precision_recall_fscore_support: Compute precision, recall, F-measure and 
        support for each class. 
    confusion_matrix: Compute confusion matrix to evaluate the accuracy of a 
        classification. 
    multilabel_confusion_matrix: Compute a confusion matrix for each class or sample. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import classification_report 
    &gt;&gt;&gt; y_true = [0, 1, 2, 2, 2] 
    &gt;&gt;&gt; y_pred = [0, 0, 2, 2, 1] 
    &gt;&gt;&gt; target_names = ['class 0', 'class 1', 'class 2'] 
    &gt;&gt;&gt; print(classification_report(y_true, y_pred, target_names=target_names)) 
                  precision    recall  f1-score   support 
    &lt;BLANKLINE&gt; 
         class 0       0.50      1.00      0.67         1 
         class 1       0.00      0.00      0.00         1 
         class 2       1.00      0.67      0.80         3 
    &lt;BLANKLINE&gt; 
        accuracy                           0.60         5 
       macro avg       0.50      0.56      0.49         5 
    weighted avg       0.70      0.60      0.61         5 
    &lt;BLANKLINE&gt; 
    &gt;&gt;&gt; y_pred = [1, 1, 0] 
    &gt;&gt;&gt; y_true = [1, 1, 1] 
    &gt;&gt;&gt; print(classification_report(y_true, y_pred, labels=[1, 2, 3])) 
                  precision    recall  f1-score   support 
    &lt;BLANKLINE&gt; 
               1       1.00      0.67      0.80         3 
               2       0.00      0.00      0.00         0 
               3       0.00      0.00      0.00         0 
    &lt;BLANKLINE&gt; 
       micro avg       1.00      0.67      0.80         3 
       macro avg       0.33      0.22      0.27         3 
    weighted avg       1.00      0.67      0.80         3 
    &lt;BLANKLINE&gt; 
    &quot;&quot;&quot;</span>

    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">unique_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
        <span class="s1">labels_given </span><span class="s4">= </span><span class="s3">False</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">asarray</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">labels_given </span><span class="s4">= </span><span class="s3">True</span>

    <span class="s2"># labelled micro average</span>
    <span class="s1">micro_is_accuracy </span><span class="s4">= (</span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;multiclass&quot; </span><span class="s3">or </span><span class="s1">y_type </span><span class="s4">== </span><span class="s5">&quot;binary&quot;</span><span class="s4">) </span><span class="s3">and </span><span class="s4">(</span>
        <span class="s3">not </span><span class="s1">labels_given </span><span class="s3">or </span><span class="s4">(</span><span class="s1">set</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) &gt;= </span><span class="s1">set</span><span class="s4">(</span><span class="s1">unique_labels</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)))</span>
    <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">target_names </span><span class="s3">is not None and </span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">) != </span><span class="s1">len</span><span class="s4">(</span><span class="s1">target_names</span><span class="s4">):</span>
        <span class="s3">if </span><span class="s1">labels_given</span><span class="s4">:</span>
            <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
                <span class="s5">&quot;labels size, {0}, does not match size of target_names, {1}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                    <span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">), </span><span class="s1">len</span><span class="s4">(</span><span class="s1">target_names</span><span class="s4">)</span>
                <span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;Number of classes, {0}, does not match size of &quot;</span>
                <span class="s5">&quot;target_names, {1}. Try specifying the labels &quot;</span>
                <span class="s5">&quot;parameter&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">), </span><span class="s1">len</span><span class="s4">(</span><span class="s1">target_names</span><span class="s4">))</span>
            <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">target_names </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">target_names </span><span class="s4">= [</span><span class="s5">&quot;%s&quot; </span><span class="s4">% </span><span class="s1">l </span><span class="s3">for </span><span class="s1">l </span><span class="s3">in </span><span class="s1">labels</span><span class="s4">]</span>

    <span class="s1">headers </span><span class="s4">= [</span><span class="s5">&quot;precision&quot;</span><span class="s4">, </span><span class="s5">&quot;recall&quot;</span><span class="s4">, </span><span class="s5">&quot;f1-score&quot;</span><span class="s4">, </span><span class="s5">&quot;support&quot;</span><span class="s4">]</span>
    <span class="s2"># compute per-class results without averaging</span>
    <span class="s1">p</span><span class="s4">, </span><span class="s1">r</span><span class="s4">, </span><span class="s1">f1</span><span class="s4">, </span><span class="s1">s </span><span class="s4">= </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
        <span class="s1">average</span><span class="s4">=</span><span class="s3">None</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
    <span class="s4">)</span>
    <span class="s1">rows </span><span class="s4">= </span><span class="s1">zip</span><span class="s4">(</span><span class="s1">target_names</span><span class="s4">, </span><span class="s1">p</span><span class="s4">, </span><span class="s1">r</span><span class="s4">, </span><span class="s1">f1</span><span class="s4">, </span><span class="s1">s</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_type</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s5">&quot;multilabel&quot;</span><span class="s4">):</span>
        <span class="s1">average_options </span><span class="s4">= (</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">, </span><span class="s5">&quot;samples&quot;</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">average_options </span><span class="s4">= (</span><span class="s5">&quot;micro&quot;</span><span class="s4">, </span><span class="s5">&quot;macro&quot;</span><span class="s4">, </span><span class="s5">&quot;weighted&quot;</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">output_dict</span><span class="s4">:</span>
        <span class="s1">report_dict </span><span class="s4">= {</span><span class="s1">label</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]: </span><span class="s1">label</span><span class="s4">[</span><span class="s6">1</span><span class="s4">:] </span><span class="s3">for </span><span class="s1">label </span><span class="s3">in </span><span class="s1">rows</span><span class="s4">}</span>
        <span class="s3">for </span><span class="s1">label</span><span class="s4">, </span><span class="s1">scores </span><span class="s3">in </span><span class="s1">report_dict</span><span class="s4">.</span><span class="s1">items</span><span class="s4">():</span>
            <span class="s1">report_dict</span><span class="s4">[</span><span class="s1">label</span><span class="s4">] = </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">zip</span><span class="s4">(</span><span class="s1">headers</span><span class="s4">, [</span><span class="s1">float</span><span class="s4">(</span><span class="s1">i</span><span class="s4">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">scores</span><span class="s4">]))</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">longest_last_line_heading </span><span class="s4">= </span><span class="s5">&quot;weighted avg&quot;</span>
        <span class="s1">name_width </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s1">len</span><span class="s4">(</span><span class="s1">cn</span><span class="s4">) </span><span class="s3">for </span><span class="s1">cn </span><span class="s3">in </span><span class="s1">target_names</span><span class="s4">)</span>
        <span class="s1">width </span><span class="s4">= </span><span class="s1">max</span><span class="s4">(</span><span class="s1">name_width</span><span class="s4">, </span><span class="s1">len</span><span class="s4">(</span><span class="s1">longest_last_line_heading</span><span class="s4">), </span><span class="s1">digits</span><span class="s4">)</span>
        <span class="s1">head_fmt </span><span class="s4">= </span><span class="s5">&quot;{:&gt;{width}s} &quot; </span><span class="s4">+ </span><span class="s5">&quot; {:&gt;9}&quot; </span><span class="s4">* </span><span class="s1">len</span><span class="s4">(</span><span class="s1">headers</span><span class="s4">)</span>
        <span class="s1">report </span><span class="s4">= </span><span class="s1">head_fmt</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s5">&quot;&quot;</span><span class="s4">, *</span><span class="s1">headers</span><span class="s4">, </span><span class="s1">width</span><span class="s4">=</span><span class="s1">width</span><span class="s4">)</span>
        <span class="s1">report </span><span class="s4">+= </span><span class="s5">&quot;</span><span class="s3">\n\n</span><span class="s5">&quot;</span>
        <span class="s1">row_fmt </span><span class="s4">= </span><span class="s5">&quot;{:&gt;{width}s} &quot; </span><span class="s4">+ </span><span class="s5">&quot; {:&gt;9.{digits}f}&quot; </span><span class="s4">* </span><span class="s6">3 </span><span class="s4">+ </span><span class="s5">&quot; {:&gt;9}</span><span class="s3">\n</span><span class="s5">&quot;</span>
        <span class="s3">for </span><span class="s1">row </span><span class="s3">in </span><span class="s1">rows</span><span class="s4">:</span>
            <span class="s1">report </span><span class="s4">+= </span><span class="s1">row_fmt</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(*</span><span class="s1">row</span><span class="s4">, </span><span class="s1">width</span><span class="s4">=</span><span class="s1">width</span><span class="s4">, </span><span class="s1">digits</span><span class="s4">=</span><span class="s1">digits</span><span class="s4">)</span>
        <span class="s1">report </span><span class="s4">+= </span><span class="s5">&quot;</span><span class="s3">\n</span><span class="s5">&quot;</span>

    <span class="s2"># compute all applicable averages</span>
    <span class="s3">for </span><span class="s1">average </span><span class="s3">in </span><span class="s1">average_options</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">average</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s5">&quot;micro&quot;</span><span class="s4">) </span><span class="s3">and </span><span class="s1">micro_is_accuracy</span><span class="s4">:</span>
            <span class="s1">line_heading </span><span class="s4">= </span><span class="s5">&quot;accuracy&quot;</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s1">line_heading </span><span class="s4">= </span><span class="s1">average </span><span class="s4">+ </span><span class="s5">&quot; avg&quot;</span>

        <span class="s2"># compute averages with specified averaging method</span>
        <span class="s1">avg_p</span><span class="s4">, </span><span class="s1">avg_r</span><span class="s4">, </span><span class="s1">avg_f1</span><span class="s4">, </span><span class="s1">_ </span><span class="s4">= </span><span class="s1">precision_recall_fscore_support</span><span class="s4">(</span>
            <span class="s1">y_true</span><span class="s4">,</span>
            <span class="s1">y_pred</span><span class="s4">,</span>
            <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
            <span class="s1">average</span><span class="s4">=</span><span class="s1">average</span><span class="s4">,</span>
            <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
            <span class="s1">zero_division</span><span class="s4">=</span><span class="s1">zero_division</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">avg </span><span class="s4">= [</span><span class="s1">avg_p</span><span class="s4">, </span><span class="s1">avg_r</span><span class="s4">, </span><span class="s1">avg_f1</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">s</span><span class="s4">)]</span>

        <span class="s3">if </span><span class="s1">output_dict</span><span class="s4">:</span>
            <span class="s1">report_dict</span><span class="s4">[</span><span class="s1">line_heading</span><span class="s4">] = </span><span class="s1">dict</span><span class="s4">(</span><span class="s1">zip</span><span class="s4">(</span><span class="s1">headers</span><span class="s4">, [</span><span class="s1">float</span><span class="s4">(</span><span class="s1">i</span><span class="s4">) </span><span class="s3">for </span><span class="s1">i </span><span class="s3">in </span><span class="s1">avg</span><span class="s4">]))</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">if </span><span class="s1">line_heading </span><span class="s4">== </span><span class="s5">&quot;accuracy&quot;</span><span class="s4">:</span>
                <span class="s1">row_fmt_accuracy </span><span class="s4">= (</span>
                    <span class="s5">&quot;{:&gt;{width}s} &quot;</span>
                    <span class="s4">+ </span><span class="s5">&quot; {:&gt;9.{digits}}&quot; </span><span class="s4">* </span><span class="s6">2</span>
                    <span class="s4">+ </span><span class="s5">&quot; {:&gt;9.{digits}f}&quot;</span>
                    <span class="s4">+ </span><span class="s5">&quot; {:&gt;9}</span><span class="s3">\n</span><span class="s5">&quot;</span>
                <span class="s4">)</span>
                <span class="s1">report </span><span class="s4">+= </span><span class="s1">row_fmt_accuracy</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                    <span class="s1">line_heading</span><span class="s4">, </span><span class="s5">&quot;&quot;</span><span class="s4">, </span><span class="s5">&quot;&quot;</span><span class="s4">, *</span><span class="s1">avg</span><span class="s4">[</span><span class="s6">2</span><span class="s4">:], </span><span class="s1">width</span><span class="s4">=</span><span class="s1">width</span><span class="s4">, </span><span class="s1">digits</span><span class="s4">=</span><span class="s1">digits</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s1">report </span><span class="s4">+= </span><span class="s1">row_fmt</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">line_heading</span><span class="s4">, *</span><span class="s1">avg</span><span class="s4">, </span><span class="s1">width</span><span class="s4">=</span><span class="s1">width</span><span class="s4">, </span><span class="s1">digits</span><span class="s4">=</span><span class="s1">digits</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">output_dict</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s5">&quot;accuracy&quot; </span><span class="s3">in </span><span class="s1">report_dict</span><span class="s4">.</span><span class="s1">keys</span><span class="s4">():</span>
            <span class="s1">report_dict</span><span class="s4">[</span><span class="s5">&quot;accuracy&quot;</span><span class="s4">] = </span><span class="s1">report_dict</span><span class="s4">[</span><span class="s5">&quot;accuracy&quot;</span><span class="s4">][</span><span class="s5">&quot;precision&quot;</span><span class="s4">]</span>
        <span class="s3">return </span><span class="s1">report_dict</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">return </span><span class="s1">report</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s5">&quot;sparse matrix&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">hamming_loss</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the average Hamming loss. 
 
    The Hamming loss is the fraction of labels that are incorrectly predicted. 
 
    Read more in the :ref:`User Guide &lt;hamming_loss&gt;`. 
 
    Parameters 
    ---------- 
    y_true : 1d array-like, or label indicator array / sparse matrix 
        Ground truth (correct) labels. 
 
    y_pred : 1d array-like, or label indicator array / sparse matrix 
        Predicted labels, as returned by a classifier. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
        .. versionadded:: 0.18 
 
    Returns 
    ------- 
    loss : float or int 
        Return the average Hamming loss between element of ``y_true`` and 
        ``y_pred``. 
 
    See Also 
    -------- 
    accuracy_score : Compute the accuracy score. By default, the function will 
        return the fraction of correct predictions divided by the total number 
        of predictions. 
    jaccard_score : Compute the Jaccard similarity coefficient score. 
    zero_one_loss : Compute the Zero-one classification loss. By default, the 
        function will return the percentage of imperfectly predicted subsets. 
 
    Notes 
    ----- 
    In multiclass classification, the Hamming loss corresponds to the Hamming 
    distance between ``y_true`` and ``y_pred`` which is equivalent to the 
    subset ``zero_one_loss`` function, when `normalize` parameter is set to 
    True. 
 
    In multilabel classification, the Hamming loss is different from the 
    subset zero-one loss. The zero-one loss considers the entire set of labels 
    for a given sample incorrect if it does not entirely match the true set of 
    labels. Hamming loss is more forgiving in that it penalizes only the 
    individual labels. 
 
    The Hamming loss is upperbounded by the subset zero-one loss, when 
    `normalize` parameter is set to True. It is always between 0 and 1, 
    lower being better. 
 
    References 
    ---------- 
    .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: 
           An Overview. International Journal of Data Warehousing &amp; Mining, 
           3(3), 1-13, July-September 2007. 
 
    .. [2] `Wikipedia entry on the Hamming distance 
           &lt;https://en.wikipedia.org/wiki/Hamming_distance&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import hamming_loss 
    &gt;&gt;&gt; y_pred = [1, 2, 3, 4] 
    &gt;&gt;&gt; y_true = [2, 2, 3, 4] 
    &gt;&gt;&gt; hamming_loss(y_true, y_pred) 
    0.25 
 
    In the multilabel case with binary label indicators: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2))) 
    0.75 
    &quot;&quot;&quot;</span>

    <span class="s1">y_type</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred </span><span class="s4">= </span><span class="s1">_check_targets</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">sample_weight </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">weight_average </span><span class="s4">= </span><span class="s6">1.0</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">weight_average </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">mean</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_type</span><span class="s4">.</span><span class="s1">startswith</span><span class="s4">(</span><span class="s5">&quot;multilabel&quot;</span><span class="s4">):</span>
        <span class="s1">n_differences </span><span class="s4">= </span><span class="s1">count_nonzero</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">- </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">n_differences </span><span class="s4">/ (</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">] * </span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] * </span><span class="s1">weight_average</span><span class="s4">)</span>

    <span class="s3">elif </span><span class="s1">y_type </span><span class="s3">in </span><span class="s4">[</span><span class="s5">&quot;binary&quot;</span><span class="s4">, </span><span class="s5">&quot;multiclass&quot;</span><span class="s4">]:</span>
        <span class="s3">return </span><span class="s1">float</span><span class="s4">(</span><span class="s1">_average</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">!= </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">True</span><span class="s4">))</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;{0} is not supported&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">y_type</span><span class="s4">))</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;normalize&quot;</span><span class="s4">: [</span><span class="s5">&quot;boolean&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">log_loss</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s3">True</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">r&quot;&quot;&quot;Log loss, aka logistic loss or cross-entropy loss. 
 
    This is the loss function used in (multinomial) logistic regression 
    and extensions of it such as neural networks, defined as the negative 
    log-likelihood of a logistic model that returns ``y_pred`` probabilities 
    for its training data ``y_true``. 
    The log loss is only defined for two or more labels. 
    For a single sample with true label :math:`y \in \{0,1\}` and 
    a probability estimate :math:`p = \operatorname{Pr}(y = 1)`, the log 
    loss is: 
 
    .. math:: 
        L_{\log}(y, p) = -(y \log (p) + (1 - y) \log (1 - p)) 
 
    Read more in the :ref:`User Guide &lt;log_loss&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like or label indicator matrix 
        Ground truth (correct) labels for n_samples samples. 
 
    y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,) 
        Predicted probabilities, as returned by a classifier's 
        predict_proba method. If ``y_pred.shape = (n_samples,)`` 
        the probabilities provided are assumed to be that of the 
        positive class. The labels in ``y_pred`` are assumed to be 
        ordered alphabetically, as done by 
        :class:`~sklearn.preprocessing.LabelBinarizer`. 
 
        `y_pred` values are clipped to `[eps, 1-eps]` where `eps` is the machine 
        precision for `y_pred`'s dtype. 
 
    normalize : bool, default=True 
        If true, return the mean loss per sample. 
        Otherwise, return the sum of the per-sample losses. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    labels : array-like, default=None 
        If not provided, labels will be inferred from y_true. If ``labels`` 
        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are 
        assumed to be binary and are inferred from ``y_true``. 
 
        .. versionadded:: 0.18 
 
    Returns 
    ------- 
    loss : float 
        Log loss, aka logistic loss or cross-entropy loss. 
 
    Notes 
    ----- 
    The logarithm used is the natural logarithm (base-e). 
 
    References 
    ---------- 
    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, 
    p. 209. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn.metrics import log_loss 
    &gt;&gt;&gt; log_loss([&quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;], 
    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]]) 
    0.21616... 
    &quot;&quot;&quot;</span>
    <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span>
        <span class="s1">y_pred</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">float64</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float32</span><span class="s4">, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">float16</span><span class="s4">]</span>
    <span class="s4">)</span>

    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">lb </span><span class="s4">= </span><span class="s1">LabelBinarizer</span><span class="s4">()</span>

    <span class="s3">if </span><span class="s1">labels </span><span class="s3">is not None</span><span class="s4">:</span>
        <span class="s1">lb</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
    <span class="s3">else</span><span class="s4">:</span>
        <span class="s1">lb</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) == </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;y_true contains only one label ({0}). Please &quot;</span>
                <span class="s5">&quot;provide the true labels explicitly through the &quot;</span>
                <span class="s5">&quot;labels argument.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">[</span><span class="s6">0</span><span class="s4">])</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;The labels array needs to contain at least two &quot;</span>
                <span class="s5">&quot;labels for log_loss, &quot;</span>
                <span class="s5">&quot;got {0}.&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
            <span class="s4">)</span>

    <span class="s1">transformed_labels </span><span class="s4">= </span><span class="s1">lb</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">transformed_labels</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] == </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s1">transformed_labels </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span>
            <span class="s6">1 </span><span class="s4">- </span><span class="s1">transformed_labels</span><span class="s4">, </span><span class="s1">transformed_labels</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span>
        <span class="s4">)</span>

    <span class="s2"># If y_pred is of single dimension, assume y_true to be binary</span>
    <span class="s2"># and then check.</span>
    <span class="s3">if </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">== </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">y_pred</span><span class="s4">[:, </span><span class="s1">np</span><span class="s4">.</span><span class="s1">newaxis</span><span class="s4">]</span>
    <span class="s3">if </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">] == </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">append</span><span class="s4">(</span><span class="s6">1 </span><span class="s4">- </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>

    <span class="s1">eps </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">finfo</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">).</span><span class="s1">eps</span>

    <span class="s2"># Make sure y_pred is normalized</span>
    <span class="s1">y_pred_sum </span><span class="s4">= </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>
    <span class="s3">if not </span><span class="s1">np</span><span class="s4">.</span><span class="s1">allclose</span><span class="s4">(</span><span class="s1">y_pred_sum</span><span class="s4">, </span><span class="s6">1</span><span class="s4">, </span><span class="s1">rtol</span><span class="s4">=</span><span class="s1">np</span><span class="s4">.</span><span class="s1">sqrt</span><span class="s4">(</span><span class="s1">eps</span><span class="s4">)):</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s5">&quot;The y_pred values do not sum to one. Make sure to pass probabilities.&quot;</span><span class="s4">,</span>
            <span class="s1">UserWarning</span><span class="s4">,</span>
        <span class="s4">)</span>

    <span class="s2"># Clipping</span>
    <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">clip</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">eps</span><span class="s4">, </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">eps</span><span class="s4">)</span>

    <span class="s2"># Check if dimensions are consistent.</span>
    <span class="s1">transformed_labels </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">transformed_labels</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">len</span><span class="s4">(</span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">) != </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
        <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;y_true and y_pred contain different number of &quot;</span>
                <span class="s5">&quot;classes {0}, {1}. Please provide the true &quot;</span>
                <span class="s5">&quot;labels explicitly through the labels argument. &quot;</span>
                <span class="s5">&quot;Classes found in &quot;</span>
                <span class="s5">&quot;y_true: {2}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span>
                    <span class="s1">transformed_labels</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">], </span><span class="s1">y_pred</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">], </span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span>
                <span class="s4">)</span>
            <span class="s4">)</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;The number of classes in labels is different &quot;</span>
                <span class="s5">&quot;from that in y_pred. Classes found in &quot;</span>
                <span class="s5">&quot;labels: {0}&quot;</span><span class="s4">.</span><span class="s1">format</span><span class="s4">(</span><span class="s1">lb</span><span class="s4">.</span><span class="s1">classes_</span><span class="s4">)</span>
            <span class="s4">)</span>

    <span class="s1">loss </span><span class="s4">= -</span><span class="s1">xlogy</span><span class="s4">(</span><span class="s1">transformed_labels</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">).</span><span class="s1">sum</span><span class="s4">(</span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>

    <span class="s3">return </span><span class="s1">float</span><span class="s4">(</span><span class="s1">_average</span><span class="s4">(</span><span class="s1">loss</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">normalize</span><span class="s4">=</span><span class="s1">normalize</span><span class="s4">))</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;pred_decision&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">hinge_loss</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">pred_decision</span><span class="s4">, *, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Average hinge loss (non-regularized). 
 
    In binary class case, assuming labels in y_true are encoded with +1 and -1, 
    when a prediction mistake is made, ``margin = y_true * pred_decision`` is 
    always negative (since the signs disagree), implying ``1 - margin`` is 
    always greater than 1.  The cumulated hinge loss is therefore an upper 
    bound of the number of mistakes made by the classifier. 
 
    In multiclass case, the function expects that either all the labels are 
    included in y_true or an optional labels argument is provided which 
    contains all the labels. The multilabel margin is calculated according 
    to Crammer-Singer's method. As in the binary case, the cumulated hinge loss 
    is an upper bound of the number of mistakes made by the classifier. 
 
    Read more in the :ref:`User Guide &lt;hinge_loss&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True target, consisting of integers of two values. The positive label 
        must be greater than the negative label. 
 
    pred_decision : array-like of shape (n_samples,) or (n_samples, n_classes) 
        Predicted decisions, as output by decision_function (floats). 
 
    labels : array-like, default=None 
        Contains all the labels for the problem. Used in multiclass hinge loss. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    Returns 
    ------- 
    loss : float 
        Average hinge loss. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry on the Hinge loss 
           &lt;https://en.wikipedia.org/wiki/Hinge_loss&gt;`_. 
 
    .. [2] Koby Crammer, Yoram Singer. On the Algorithmic 
           Implementation of Multiclass Kernel-based Vector 
           Machines. Journal of Machine Learning Research 2, 
           (2001), 265-292. 
 
    .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models 
           by Robert C. Moore, John DeNero 
           &lt;https://storage.googleapis.com/pub-tools-public-publication-data/pdf/37362.pdf&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; from sklearn import svm 
    &gt;&gt;&gt; from sklearn.metrics import hinge_loss 
    &gt;&gt;&gt; X = [[0], [1]] 
    &gt;&gt;&gt; y = [-1, 1] 
    &gt;&gt;&gt; est = svm.LinearSVC(random_state=0) 
    &gt;&gt;&gt; est.fit(X, y) 
    LinearSVC(random_state=0) 
    &gt;&gt;&gt; pred_decision = est.decision_function([[-2], [3], [0.5]]) 
    &gt;&gt;&gt; pred_decision 
    array([-2.18...,  2.36...,  0.09...]) 
    &gt;&gt;&gt; hinge_loss([-1, 1, 1], pred_decision) 
    np.float64(0.30...) 
 
    In the multiclass case: 
 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; X = np.array([[0], [1], [2], [3]]) 
    &gt;&gt;&gt; Y = np.array([0, 1, 2, 3]) 
    &gt;&gt;&gt; labels = np.array([0, 1, 2, 3]) 
    &gt;&gt;&gt; est = svm.LinearSVC() 
    &gt;&gt;&gt; est.fit(X, Y) 
    LinearSVC() 
    &gt;&gt;&gt; pred_decision = est.decision_function([[-1], [2], [3]]) 
    &gt;&gt;&gt; y_true = [0, 2, 3] 
    &gt;&gt;&gt; hinge_loss(y_true, pred_decision, labels=labels) 
    np.float64(0.56...) 
    &quot;&quot;&quot;</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">pred_decision</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s1">pred_decision </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">pred_decision</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">)</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_true_unique </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">labels </span><span class="s3">if </span><span class="s1">labels </span><span class="s3">is not None else </span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_true_unique</span><span class="s4">.</span><span class="s1">size </span><span class="s4">&gt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s3">if </span><span class="s1">pred_decision</span><span class="s4">.</span><span class="s1">ndim </span><span class="s4">&lt;= </span><span class="s6">1</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                <span class="s5">&quot;The shape of pred_decision cannot be 1d array&quot;</span>
                <span class="s5">&quot;with a multiclass target. pred_decision shape &quot;</span>
                <span class="s5">&quot;must be (n_samples, n_classes), that is &quot;</span>
                <span class="s5">f&quot;(</span><span class="s3">{</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">y_true_unique</span><span class="s4">.</span><span class="s1">size</span><span class="s3">}</span><span class="s5">).&quot;</span>
                <span class="s5">f&quot; Got: </span><span class="s3">{</span><span class="s1">pred_decision</span><span class="s4">.</span><span class="s1">shape</span><span class="s3">}</span><span class="s5">&quot;</span>
            <span class="s4">)</span>

        <span class="s2"># pred_decision.ndim &gt; 1 is true</span>
        <span class="s3">if </span><span class="s1">y_true_unique</span><span class="s4">.</span><span class="s1">size </span><span class="s4">!= </span><span class="s1">pred_decision</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">1</span><span class="s4">]:</span>
            <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;Please include all labels in y_true &quot;</span>
                    <span class="s5">&quot;or pass labels as third argument&quot;</span>
                <span class="s4">)</span>
            <span class="s3">else</span><span class="s4">:</span>
                <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
                    <span class="s5">&quot;The shape of pred_decision is not &quot;</span>
                    <span class="s5">&quot;consistent with the number of classes. &quot;</span>
                    <span class="s5">&quot;With a multiclass target, pred_decision &quot;</span>
                    <span class="s5">&quot;shape must be &quot;</span>
                    <span class="s5">&quot;(n_samples, n_classes), that is &quot;</span>
                    <span class="s5">f&quot;(</span><span class="s3">{</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]</span><span class="s3">}</span><span class="s5">, </span><span class="s3">{</span><span class="s1">y_true_unique</span><span class="s4">.</span><span class="s1">size</span><span class="s3">}</span><span class="s5">). &quot;</span>
                    <span class="s5">f&quot;Got: </span><span class="s3">{</span><span class="s1">pred_decision</span><span class="s4">.</span><span class="s1">shape</span><span class="s3">}</span><span class="s5">&quot;</span>
                <span class="s4">)</span>
        <span class="s3">if </span><span class="s1">labels </span><span class="s3">is None</span><span class="s4">:</span>
            <span class="s1">labels </span><span class="s4">= </span><span class="s1">y_true_unique</span>
        <span class="s1">le </span><span class="s4">= </span><span class="s1">LabelEncoder</span><span class="s4">()</span>
        <span class="s1">le</span><span class="s4">.</span><span class="s1">fit</span><span class="s4">(</span><span class="s1">labels</span><span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">le</span><span class="s4">.</span><span class="s1">transform</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s1">mask </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ones_like</span><span class="s4">(</span><span class="s1">pred_decision</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s1">bool</span><span class="s4">)</span>
        <span class="s1">mask</span><span class="s4">[</span><span class="s1">np</span><span class="s4">.</span><span class="s1">arange</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">]), </span><span class="s1">y_true</span><span class="s4">] = </span><span class="s3">False</span>
        <span class="s1">margin </span><span class="s4">= </span><span class="s1">pred_decision</span><span class="s4">[~</span><span class="s1">mask</span><span class="s4">]</span>
        <span class="s1">margin </span><span class="s4">-= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">max</span><span class="s4">(</span><span class="s1">pred_decision</span><span class="s4">[</span><span class="s1">mask</span><span class="s4">].</span><span class="s1">reshape</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">.</span><span class="s1">shape</span><span class="s4">[</span><span class="s6">0</span><span class="s4">], -</span><span class="s6">1</span><span class="s4">), </span><span class="s1">axis</span><span class="s4">=</span><span class="s6">1</span><span class="s4">)</span>

    <span class="s3">else</span><span class="s4">:</span>
        <span class="s2"># Handles binary class case</span>
        <span class="s2"># this code assumes that positive and negative labels</span>
        <span class="s2"># are encoded as +1 and -1 respectively</span>
        <span class="s1">pred_decision </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">pred_decision</span><span class="s4">)</span>
        <span class="s1">pred_decision </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">ravel</span><span class="s4">(</span><span class="s1">pred_decision</span><span class="s4">)</span>

        <span class="s1">lbin </span><span class="s4">= </span><span class="s1">LabelBinarizer</span><span class="s4">(</span><span class="s1">neg_label</span><span class="s4">=-</span><span class="s6">1</span><span class="s4">)</span>
        <span class="s1">y_true </span><span class="s4">= </span><span class="s1">lbin</span><span class="s4">.</span><span class="s1">fit_transform</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)[:, </span><span class="s6">0</span><span class="s4">]</span>

        <span class="s3">try</span><span class="s4">:</span>
            <span class="s1">margin </span><span class="s4">= </span><span class="s1">y_true </span><span class="s4">* </span><span class="s1">pred_decision</span>
        <span class="s3">except </span><span class="s1">TypeError</span><span class="s4">:</span>
            <span class="s3">raise </span><span class="s1">TypeError</span><span class="s4">(</span><span class="s5">&quot;pred_decision should be an array of floats.&quot;</span><span class="s4">)</span>

    <span class="s1">losses </span><span class="s4">= </span><span class="s6">1 </span><span class="s4">- </span><span class="s1">margin</span>
    <span class="s2"># The hinge_loss doesn't penalize good enough predictions.</span>
    <span class="s1">np</span><span class="s4">.</span><span class="s1">clip</span><span class="s4">(</span><span class="s1">losses</span><span class="s4">, </span><span class="s6">0</span><span class="s4">, </span><span class="s3">None</span><span class="s4">, </span><span class="s1">out</span><span class="s4">=</span><span class="s1">losses</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">(</span><span class="s1">losses</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_proba&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s1">Hidden</span><span class="s4">(</span><span class="s3">None</span><span class="s4">)],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;pos_label&quot;</span><span class="s4">: [</span><span class="s1">Real</span><span class="s4">, </span><span class="s1">str</span><span class="s4">, </span><span class="s5">&quot;boolean&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;y_prob&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s1">Hidden</span><span class="s4">(</span><span class="s1">StrOptions</span><span class="s4">({</span><span class="s5">&quot;deprecated&quot;</span><span class="s4">}))],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">brier_score_loss</span><span class="s4">(</span>
    <span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_proba</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">pos_label</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">y_prob</span><span class="s4">=</span><span class="s5">&quot;deprecated&quot;</span>
<span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot;Compute the Brier score loss. 
 
    The smaller the Brier score loss, the better, hence the naming with &quot;loss&quot;. 
    The Brier score measures the mean squared difference between the predicted 
    probability and the actual outcome. The Brier score always 
    takes on a value between zero and one, since this is the largest 
    possible difference between a predicted probability (which must be 
    between zero and one) and the actual outcome (which can take on values 
    of only 0 and 1). It can be decomposed as the sum of refinement loss and 
    calibration loss. 
 
    The Brier score is appropriate for binary and categorical outcomes that 
    can be structured as true or false, but is inappropriate for ordinal 
    variables which can take on three or more values (this is because the 
    Brier score assumes that all possible outcomes are equivalently 
    &quot;distant&quot; from one another). Which label is considered to be the positive 
    label is controlled via the parameter `pos_label`, which defaults to 
    the greater label unless `y_true` is all 0 or all -1, in which case 
    `pos_label` defaults to 1. 
 
    Read more in the :ref:`User Guide &lt;brier_score_loss&gt;`. 
 
    Parameters 
    ---------- 
    y_true : array-like of shape (n_samples,) 
        True targets. 
 
    y_proba : array-like of shape (n_samples,) 
        Probabilities of the positive class. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    pos_label : int, float, bool or str, default=None 
        Label of the positive class. `pos_label` will be inferred in the 
        following manner: 
 
        * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1; 
        * else if `y_true` contains string, an error will be raised and 
          `pos_label` should be explicitly specified; 
        * otherwise, `pos_label` defaults to the greater label, 
          i.e. `np.unique(y_true)[-1]`. 
 
    y_prob : array-like of shape (n_samples,) 
        Probabilities of the positive class. 
 
        .. deprecated:: 1.5 
            `y_prob` is deprecated and will be removed in 1.7. Use 
            `y_proba` instead. 
 
    Returns 
    ------- 
    score : float 
        Brier score loss. 
 
    References 
    ---------- 
    .. [1] `Wikipedia entry for the Brier score 
            &lt;https://en.wikipedia.org/wiki/Brier_score&gt;`_. 
 
    Examples 
    -------- 
    &gt;&gt;&gt; import numpy as np 
    &gt;&gt;&gt; from sklearn.metrics import brier_score_loss 
    &gt;&gt;&gt; y_true = np.array([0, 1, 1, 0]) 
    &gt;&gt;&gt; y_true_categorical = np.array([&quot;spam&quot;, &quot;ham&quot;, &quot;ham&quot;, &quot;spam&quot;]) 
    &gt;&gt;&gt; y_prob = np.array([0.1, 0.9, 0.8, 0.3]) 
    &gt;&gt;&gt; brier_score_loss(y_true, y_prob) 
    np.float64(0.037...) 
    &gt;&gt;&gt; brier_score_loss(y_true, 1-y_prob, pos_label=0) 
    np.float64(0.037...) 
    &gt;&gt;&gt; brier_score_loss(y_true_categorical, y_prob, pos_label=&quot;ham&quot;) 
    np.float64(0.037...) 
    &gt;&gt;&gt; brier_score_loss(y_true, np.array(y_prob) &gt; 0.5) 
    np.float64(0.0) 
    &quot;&quot;&quot;</span>
    <span class="s2"># TODO(1.7): remove in 1.7 and reset y_proba to be required</span>
    <span class="s2"># Note: validate params will raise an error if y_prob is not array-like,</span>
    <span class="s2"># or &quot;deprecated&quot;</span>
    <span class="s3">if </span><span class="s1">y_proba </span><span class="s3">is not None and not </span><span class="s1">isinstance</span><span class="s4">(</span><span class="s1">y_prob</span><span class="s4">, </span><span class="s1">str</span><span class="s4">):</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;`y_prob` and `y_proba` cannot be both specified. Please use `y_proba` only&quot;</span>
            <span class="s5">&quot; as `y_prob` is deprecated in v1.5 and will be removed in v1.7.&quot;</span>
        <span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_proba </span><span class="s3">is None</span><span class="s4">:</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span>
            <span class="s4">(</span>
                <span class="s5">&quot;y_prob was deprecated in version 1.5 and will be removed in 1.7.&quot;</span>
                <span class="s5">&quot;Please use ``y_proba`` instead.&quot;</span>
            <span class="s4">),</span>
            <span class="s1">FutureWarning</span><span class="s4">,</span>
        <span class="s4">)</span>
        <span class="s1">y_proba </span><span class="s4">= </span><span class="s1">y_prob</span>

    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">y_proba </span><span class="s4">= </span><span class="s1">column_or_1d</span><span class="s4">(</span><span class="s1">y_proba</span><span class="s4">)</span>
    <span class="s1">assert_all_finite</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s1">assert_all_finite</span><span class="s4">(</span><span class="s1">y_proba</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_proba</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>

    <span class="s1">y_type </span><span class="s4">= </span><span class="s1">type_of_target</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">input_name</span><span class="s4">=</span><span class="s5">&quot;y_true&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_type </span><span class="s4">!= </span><span class="s5">&quot;binary&quot;</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span>
            <span class="s5">&quot;Only binary classification is supported. The type of the target &quot;</span>
            <span class="s5">f&quot;is </span><span class="s3">{</span><span class="s1">y_type</span><span class="s3">}</span><span class="s5">.&quot;</span>
        <span class="s4">)</span>

    <span class="s3">if </span><span class="s1">y_proba</span><span class="s4">.</span><span class="s1">max</span><span class="s4">() &gt; </span><span class="s6">1</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_proba contains values greater than 1.&quot;</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">y_proba</span><span class="s4">.</span><span class="s1">min</span><span class="s4">() &lt; </span><span class="s6">0</span><span class="s4">:</span>
        <span class="s3">raise </span><span class="s1">ValueError</span><span class="s4">(</span><span class="s5">&quot;y_proba contains values less than 0.&quot;</span><span class="s4">)</span>

    <span class="s3">try</span><span class="s4">:</span>
        <span class="s1">pos_label </span><span class="s4">= </span><span class="s1">_check_pos_label_consistency</span><span class="s4">(</span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>
    <span class="s3">except </span><span class="s1">ValueError</span><span class="s4">:</span>
        <span class="s1">classes </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">)</span>
        <span class="s3">if </span><span class="s1">classes</span><span class="s4">.</span><span class="s1">dtype</span><span class="s4">.</span><span class="s1">kind </span><span class="s3">not in </span><span class="s4">(</span><span class="s5">&quot;O&quot;</span><span class="s4">, </span><span class="s5">&quot;U&quot;</span><span class="s4">, </span><span class="s5">&quot;S&quot;</span><span class="s4">):</span>
            <span class="s2"># for backward compatibility, if classes are not string then</span>
            <span class="s2"># `pos_label` will correspond to the greater label</span>
            <span class="s1">pos_label </span><span class="s4">= </span><span class="s1">classes</span><span class="s4">[-</span><span class="s6">1</span><span class="s4">]</span>
        <span class="s3">else</span><span class="s4">:</span>
            <span class="s3">raise</span>
    <span class="s1">y_true </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">array</span><span class="s4">(</span><span class="s1">y_true </span><span class="s4">== </span><span class="s1">pos_label</span><span class="s4">, </span><span class="s1">int</span><span class="s4">)</span>
    <span class="s3">return </span><span class="s1">np</span><span class="s4">.</span><span class="s1">average</span><span class="s4">((</span><span class="s1">y_true </span><span class="s4">- </span><span class="s1">y_proba</span><span class="s4">) ** </span><span class="s6">2</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">)</span>


<span class="s4">@</span><span class="s1">validate_params</span><span class="s4">(</span>
    <span class="s4">{</span>
        <span class="s5">&quot;y_true&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;y_pred&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">],</span>
        <span class="s5">&quot;sample_weight&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
        <span class="s5">&quot;labels&quot;</span><span class="s4">: [</span><span class="s5">&quot;array-like&quot;</span><span class="s4">, </span><span class="s3">None</span><span class="s4">],</span>
    <span class="s4">},</span>
    <span class="s1">prefer_skip_nested_validation</span><span class="s4">=</span><span class="s3">True</span><span class="s4">,</span>
<span class="s4">)</span>
<span class="s3">def </span><span class="s1">d2_log_loss_score</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">y_pred</span><span class="s4">, *, </span><span class="s1">sample_weight</span><span class="s4">=</span><span class="s3">None</span><span class="s4">, </span><span class="s1">labels</span><span class="s4">=</span><span class="s3">None</span><span class="s4">):</span>
    <span class="s0">&quot;&quot;&quot; 
    :math:`D^2` score function, fraction of log loss explained. 
 
    Best possible score is 1.0 and it can be negative (because the model can be 
    arbitrarily worse). A model that always predicts the per-class proportions 
    of `y_true`, disregarding the input features, gets a D^2 score of 0.0. 
 
    Read more in the :ref:`User Guide &lt;d2_score_classification&gt;`. 
 
    .. versionadded:: 1.5 
 
    Parameters 
    ---------- 
    y_true : array-like or label indicator matrix 
        The actuals labels for the n_samples samples. 
 
    y_pred : array-like of shape (n_samples, n_classes) or (n_samples,) 
        Predicted probabilities, as returned by a classifier's 
        predict_proba method. If ``y_pred.shape = (n_samples,)`` 
        the probabilities provided are assumed to be that of the 
        positive class. The labels in ``y_pred`` are assumed to be 
        ordered alphabetically, as done by 
        :class:`~sklearn.preprocessing.LabelBinarizer`. 
 
    sample_weight : array-like of shape (n_samples,), default=None 
        Sample weights. 
 
    labels : array-like, default=None 
        If not provided, labels will be inferred from y_true. If ``labels`` 
        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are 
        assumed to be binary and are inferred from ``y_true``. 
 
    Returns 
    ------- 
    d2 : float or ndarray of floats 
        The D^2 score. 
 
    Notes 
    ----- 
    This is not a symmetric function. 
 
    Like R^2, D^2 score may be negative (it need not actually be the square of 
    a quantity D). 
 
    This metric is not well-defined for a single sample and will return a NaN 
    value if n_samples is less than two. 
    &quot;&quot;&quot;</span>
    <span class="s1">y_pred </span><span class="s4">= </span><span class="s1">check_array</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">ensure_2d</span><span class="s4">=</span><span class="s3">False</span><span class="s4">, </span><span class="s1">dtype</span><span class="s4">=</span><span class="s5">&quot;numeric&quot;</span><span class="s4">)</span>
    <span class="s1">check_consistent_length</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">sample_weight</span><span class="s4">)</span>
    <span class="s3">if </span><span class="s1">_num_samples</span><span class="s4">(</span><span class="s1">y_pred</span><span class="s4">) &lt; </span><span class="s6">2</span><span class="s4">:</span>
        <span class="s1">msg </span><span class="s4">= </span><span class="s5">&quot;D^2 score is not well-defined with less than two samples.&quot;</span>
        <span class="s1">warnings</span><span class="s4">.</span><span class="s1">warn</span><span class="s4">(</span><span class="s1">msg</span><span class="s4">, </span><span class="s1">UndefinedMetricWarning</span><span class="s4">)</span>
        <span class="s3">return </span><span class="s1">float</span><span class="s4">(</span><span class="s5">&quot;nan&quot;</span><span class="s4">)</span>

    <span class="s2"># log loss of the fitted model</span>
    <span class="s1">numerator </span><span class="s4">= </span><span class="s1">log_loss</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">=</span><span class="s1">y_pred</span><span class="s4">,</span>
        <span class="s1">normalize</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
    <span class="s4">)</span>

    <span class="s2"># Proportion of labels in the dataset</span>
    <span class="s1">weights </span><span class="s4">= </span><span class="s1">_check_sample_weight</span><span class="s4">(</span><span class="s1">sample_weight</span><span class="s4">, </span><span class="s1">y_true</span><span class="s4">)</span>

    <span class="s1">_</span><span class="s4">, </span><span class="s1">y_value_indices </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">unique</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">, </span><span class="s1">return_inverse</span><span class="s4">=</span><span class="s3">True</span><span class="s4">)</span>
    <span class="s1">counts </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">bincount</span><span class="s4">(</span><span class="s1">y_value_indices</span><span class="s4">, </span><span class="s1">weights</span><span class="s4">=</span><span class="s1">weights</span><span class="s4">)</span>
    <span class="s1">y_prob </span><span class="s4">= </span><span class="s1">counts </span><span class="s4">/ </span><span class="s1">weights</span><span class="s4">.</span><span class="s1">sum</span><span class="s4">()</span>
    <span class="s1">y_pred_null </span><span class="s4">= </span><span class="s1">np</span><span class="s4">.</span><span class="s1">tile</span><span class="s4">(</span><span class="s1">y_prob</span><span class="s4">, (</span><span class="s1">len</span><span class="s4">(</span><span class="s1">y_true</span><span class="s4">), </span><span class="s6">1</span><span class="s4">))</span>

    <span class="s2"># log loss of the null model</span>
    <span class="s1">denominator </span><span class="s4">= </span><span class="s1">log_loss</span><span class="s4">(</span>
        <span class="s1">y_true</span><span class="s4">=</span><span class="s1">y_true</span><span class="s4">,</span>
        <span class="s1">y_pred</span><span class="s4">=</span><span class="s1">y_pred_null</span><span class="s4">,</span>
        <span class="s1">normalize</span><span class="s4">=</span><span class="s3">False</span><span class="s4">,</span>
        <span class="s1">sample_weight</span><span class="s4">=</span><span class="s1">sample_weight</span><span class="s4">,</span>
        <span class="s1">labels</span><span class="s4">=</span><span class="s1">labels</span><span class="s4">,</span>
    <span class="s4">)</span>

    <span class="s3">return </span><span class="s6">1 </span><span class="s4">- (</span><span class="s1">numerator </span><span class="s4">/ </span><span class="s1">denominator</span><span class="s4">)</span>
</pre>
</body>
</html>