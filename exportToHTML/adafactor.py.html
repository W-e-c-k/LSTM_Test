<html>
<head>
<title>adafactor.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
adafactor.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">optimizer</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">([</span><span class="s3">&quot;keras.optimizers.Adafactor&quot;</span><span class="s2">])</span>
<span class="s0">class </span><span class="s1">Adafactor</span><span class="s2">(</span><span class="s1">optimizer</span><span class="s2">.</span><span class="s1">Optimizer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Optimizer that implements the Adafactor algorithm. 
 
    Adafactor is commonly used in NLP tasks, and has the advantage 
    of taking less memory because it only saves partial information of previous 
    gradients. 
 
    The default argument setup is based on the original paper (see reference). 
    When gradients are of dimension &gt; 2, Adafactor optimizer will delete the 
    last 2 dimensions separately in its accumulator variables. 
 
    Args: 
        learning_rate: A float, a 
            `keras.optimizers.schedules.LearningRateSchedule` instance, or 
            a callable that takes no arguments and returns the actual value to 
            use. The learning rate. Defaults to `0.001`. 
        beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`. 
        epsilon_1: float, defaults to 1e-30. A small offset to keep denominator 
            away from 0. 
        epsilon_2: float, defaults to 1e-3. A small offset to avoid learning 
            rate becoming too small by time. 
        clip_threshold: float, defaults to 1.0. Clipping threshold. This is a 
            part of Adafactor algorithm, independent from `clipnorm`, 
            `clipvalue`, and `global_clipnorm`. 
        relative_step: bool, defaults to `True`. If `learning_rate` is a 
            constant and `relative_step=True`, learning rate will be adjusted 
            based on current iterations. This is a default learning rate decay 
            in Adafactor. 
        {{base_optimizer_keyword_args}} 
 
    Reference: 
 
    - [Shazeer, Noam et al., 2018](https://arxiv.org/abs/1804.04235). 
 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">learning_rate</span><span class="s2">=</span><span class="s5">0.001</span><span class="s2">,</span>
        <span class="s1">beta_2_decay</span><span class="s2">=-</span><span class="s5">0.8</span><span class="s2">,</span>
        <span class="s1">epsilon_1</span><span class="s2">=</span><span class="s5">1e-30</span><span class="s2">,</span>
        <span class="s1">epsilon_2</span><span class="s2">=</span><span class="s5">1e-3</span><span class="s2">,</span>
        <span class="s1">clip_threshold</span><span class="s2">=</span><span class="s5">1.0</span><span class="s2">,</span>
        <span class="s1">relative_step</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">weight_decay</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipvalue</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">global_clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_ema</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">ema_momentum</span><span class="s2">=</span><span class="s5">0.99</span><span class="s2">,</span>
        <span class="s1">ema_overwrite_frequency</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">loss_scale_factor</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">gradient_accumulation_steps</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;adafactor&quot;</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">learning_rate</span><span class="s2">=</span><span class="s1">learning_rate</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s1">weight_decay</span><span class="s2">=</span><span class="s1">weight_decay</span><span class="s2">,</span>
            <span class="s1">clipnorm</span><span class="s2">=</span><span class="s1">clipnorm</span><span class="s2">,</span>
            <span class="s1">clipvalue</span><span class="s2">=</span><span class="s1">clipvalue</span><span class="s2">,</span>
            <span class="s1">global_clipnorm</span><span class="s2">=</span><span class="s1">global_clipnorm</span><span class="s2">,</span>
            <span class="s1">use_ema</span><span class="s2">=</span><span class="s1">use_ema</span><span class="s2">,</span>
            <span class="s1">ema_momentum</span><span class="s2">=</span><span class="s1">ema_momentum</span><span class="s2">,</span>
            <span class="s1">ema_overwrite_frequency</span><span class="s2">=</span><span class="s1">ema_overwrite_frequency</span><span class="s2">,</span>
            <span class="s1">loss_scale_factor</span><span class="s2">=</span><span class="s1">loss_scale_factor</span><span class="s2">,</span>
            <span class="s1">gradient_accumulation_steps</span><span class="s2">=</span><span class="s1">gradient_accumulation_steps</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2_decay </span><span class="s2">= </span><span class="s1">beta_2_decay</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_1 </span><span class="s2">= </span><span class="s1">epsilon_1</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_2 </span><span class="s2">= </span><span class="s1">epsilon_2</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">clip_threshold </span><span class="s2">= </span><span class="s1">clip_threshold</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">relative_step </span><span class="s2">= </span><span class="s1">relative_step</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">var_list</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Initialize optimizer variables. 
 
        Adam optimizer has 3 types of variables: momentums, velocities and 
        velocity_hat (only set when amsgrad is applied), 
 
        Args: 
            var_list: list of model variables to build Adam variables on. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">build</span><span class="s2">(</span><span class="s1">var_list</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_r </span><span class="s2">= []</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_c </span><span class="s2">= []</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_v </span><span class="s2">= []</span>
        <span class="s0">for </span><span class="s1">var </span><span class="s0">in </span><span class="s1">var_list</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">var</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) &lt; </span><span class="s5">2</span><span class="s2">:</span>
                <span class="s6"># Don't factor if variable is of dimension &lt; 2, but we still</span>
                <span class="s6"># need to create dummy variables as placeholder.</span>
                <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">_r</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                        <span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">(</span><span class="s5">0</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>
                    <span class="s2">)</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">_c</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                        <span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">(</span><span class="s5">0</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">)</span>
                    <span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s6"># Always factor the last 2 dimensions.</span>
                <span class="s1">r_shape </span><span class="s2">= </span><span class="s1">var</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s5">1</span><span class="s2">]</span>
                <span class="s1">c_shape </span><span class="s2">= </span><span class="s1">var</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s5">2</span><span class="s2">] + (</span><span class="s1">var</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">],)</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_r</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
                        <span class="s1">shape</span><span class="s2">=</span><span class="s1">r_shape</span><span class="s2">,</span>
                        <span class="s1">dtype</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
                        <span class="s1">name</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_c</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
                        <span class="s1">shape</span><span class="s2">=</span><span class="s1">c_shape</span><span class="s2">,</span>
                        <span class="s1">dtype</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
                        <span class="s1">name</span><span class="s2">=</span><span class="s1">var</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_v</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
                    <span class="s1">reference_variable</span><span class="s2">=</span><span class="s1">var</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;velocity&quot;</span>
                <span class="s2">)</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_rms</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)))</span>

    <span class="s0">def </span><span class="s1">update_step</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Update step given gradient and the associated model variable.&quot;&quot;&quot;</span>

        <span class="s1">lr </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">gradient </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">epsilon_2 </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_2</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">one </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s5">1.0</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">local_step </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">iterations </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s0">if not </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">) </span><span class="s0">and </span><span class="s1">self</span><span class="s2">.</span><span class="s1">relative_step</span><span class="s2">:</span>
            <span class="s1">lr </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">minimum</span><span class="s2">(</span><span class="s1">lr</span><span class="s2">, </span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">local_step</span><span class="s2">))</span>

        <span class="s1">r </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_r</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>
        <span class="s1">c </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_c</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>
        <span class="s1">v </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_v</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>

        <span class="s1">rho_t </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">minimum</span><span class="s2">(</span><span class="s1">lr</span><span class="s2">, </span><span class="s5">1 </span><span class="s2">/ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">local_step</span><span class="s2">))</span>
        <span class="s1">alpha_t </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">epsilon_2</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_rms</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)) * </span><span class="s1">rho_t</span>
        <span class="s1">regulated_grad_square </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">), </span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_1</span><span class="s2">)</span>
        <span class="s1">beta_2_t </span><span class="s2">= </span><span class="s5">1 </span><span class="s2">- </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">power</span><span class="s2">(</span><span class="s1">local_step</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2_decay</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) &gt;= </span><span class="s5">2</span><span class="s2">:</span>
            <span class="s6"># `r` deletes the last dimension of gradient, so it is of shape</span>
            <span class="s6"># `gradient.shape[:-1]`.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">r</span><span class="s2">,</span>
                <span class="s1">beta_2_t </span><span class="s2">* </span><span class="s1">r</span>
                <span class="s2">+ (</span><span class="s5">1 </span><span class="s2">- </span><span class="s1">beta_2_t</span><span class="s2">) * </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">regulated_grad_square</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">),</span>
            <span class="s2">)</span>
            <span class="s6"># `c` deletes the second last dimension of gradient, so it is of</span>
            <span class="s6"># shape `gradient.shape[:-2] + gradient.shape[-1]`.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">c</span><span class="s2">,</span>
                <span class="s1">beta_2_t </span><span class="s2">* </span><span class="s1">c</span>
                <span class="s2">+ (</span><span class="s5">1 </span><span class="s2">- </span><span class="s1">beta_2_t</span><span class="s2">) * </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">regulated_grad_square</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">2</span><span class="s2">),</span>
            <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">v</span><span class="s2">,</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span>
                    <span class="s1">r </span><span class="s2">/ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">r</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span>
                <span class="s2">)</span>
                <span class="s2">* </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">c</span><span class="s2">, -</span><span class="s5">2</span><span class="s2">),</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
                <span class="s1">v</span><span class="s2">, </span><span class="s1">beta_2_t </span><span class="s2">* </span><span class="s1">v </span><span class="s2">+ (</span><span class="s5">1 </span><span class="s2">- </span><span class="s1">beta_2_t</span><span class="s2">) * </span><span class="s1">regulated_grad_square</span>
            <span class="s2">)</span>

        <span class="s1">u_t </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">v</span><span class="s2">))</span>
        <span class="s1">u_t_hat </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span>
            <span class="s1">u_t</span><span class="s2">,</span>
            <span class="s1">ops</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">one</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_rms</span><span class="s2">(</span><span class="s1">u_t</span><span class="s2">), </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clip_threshold</span><span class="s2">)),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">assign_sub</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span><span class="s1">alpha_t</span><span class="s2">, </span><span class="s1">u_t_hat</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>

        <span class="s1">config</span><span class="s2">.</span><span class="s1">update</span><span class="s2">(</span>
            <span class="s2">{</span>
                <span class="s3">&quot;beta_2_decay&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">beta_2_decay</span><span class="s2">,</span>
                <span class="s3">&quot;epsilon_1&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_1</span><span class="s2">,</span>
                <span class="s3">&quot;epsilon_2&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">epsilon_2</span><span class="s2">,</span>
                <span class="s3">&quot;clip_threshold&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clip_threshold</span><span class="s2">,</span>
                <span class="s3">&quot;relative_step&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">relative_step</span><span class="s2">,</span>
            <span class="s2">}</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">config</span>


<span class="s1">Adafactor</span><span class="s2">.</span><span class="s1">__doc__ </span><span class="s2">= </span><span class="s1">Adafactor</span><span class="s2">.</span><span class="s1">__doc__</span><span class="s2">.</span><span class="s1">replace</span><span class="s2">(</span>
    <span class="s3">&quot;{{base_optimizer_keyword_args}}&quot;</span><span class="s2">, </span><span class="s1">optimizer</span><span class="s2">.</span><span class="s1">base_optimizer_keyword_args</span>
<span class="s2">)</span>
</pre>
</body>
</html>