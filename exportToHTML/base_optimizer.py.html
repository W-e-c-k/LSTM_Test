<html>
<head>
<title>base_optimizer.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #2aacb8;}
.s4 { color: #6aab73;}
.s5 { color: #7a7e85;}
.s6 { color: #5f826b; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
base_optimizer.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">re</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">initializers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">optimizers</span><span class="s2">.</span><span class="s1">schedules </span><span class="s0">import </span><span class="s1">learning_rate_schedule</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">saving </span><span class="s0">import </span><span class="s1">serialization_lib</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">saving</span><span class="s2">.</span><span class="s1">keras_saveable </span><span class="s0">import </span><span class="s1">KerasSaveable</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">utils </span><span class="s0">import </span><span class="s1">tracking</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">naming </span><span class="s0">import </span><span class="s1">auto_name</span>


<span class="s0">class </span><span class="s1">BaseOptimizer</span><span class="s2">(</span><span class="s1">KerasSaveable</span><span class="s2">):</span>
    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">learning_rate</span><span class="s2">,</span>
        <span class="s1">weight_decay</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">clipvalue</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">global_clipnorm</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_ema</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">ema_momentum</span><span class="s2">=</span><span class="s3">0.99</span><span class="s2">,</span>
        <span class="s1">ema_overwrite_frequency</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">loss_scale_factor</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">gradient_accumulation_steps</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_lock </span><span class="s2">= </span><span class="s0">False</span>

        <span class="s0">if </span><span class="s1">kwargs</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s4">&quot;decay&quot;</span><span class="s2">, </span><span class="s0">None</span><span class="s2">) </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
                <span class="s4">&quot;Argument `decay` is no longer supported and will be ignored.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">kwargs</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">f&quot;Argument(s) not recognized: </span><span class="s0">{</span><span class="s1">kwargs</span><span class="s0">}</span><span class="s4">&quot;</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">name </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">name </span><span class="s2">= </span><span class="s1">auto_name</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">__name__</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">name </span><span class="s2">= </span><span class="s1">name</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay </span><span class="s2">= </span><span class="s1">weight_decay</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm </span><span class="s2">= </span><span class="s1">clipnorm</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">global_clipnorm </span><span class="s2">= </span><span class="s1">global_clipnorm</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">clipvalue </span><span class="s2">= </span><span class="s1">clipvalue</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema </span><span class="s2">= </span><span class="s1">use_ema</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">loss_scale_factor </span><span class="s2">= </span><span class="s1">loss_scale_factor</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps </span><span class="s2">= </span><span class="s1">gradient_accumulation_steps</span>

        <span class="s0">if </span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
            <span class="s0">if not </span><span class="s1">gradient_accumulation_steps </span><span class="s2">&gt;= </span><span class="s3">2</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;`gradient_accumulation_steps` must be an integer &gt;= 2. &quot;</span>
                    <span class="s4">&quot;Received: gradient_accumulation_steps=&quot;</span>
                    <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">gradient_accumulation_steps</span><span class="s0">}</span><span class="s4">&quot;</span>
                <span class="s2">)</span>

        <span class="s0">if </span><span class="s1">use_ema</span><span class="s2">:</span>
            <span class="s5"># Verify the arguments related to EMA.</span>
            <span class="s0">if </span><span class="s1">ema_momentum </span><span class="s2">&gt; </span><span class="s3">1 </span><span class="s0">or </span><span class="s1">ema_momentum </span><span class="s2">&lt; </span><span class="s3">0</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;`ema_momentum` must be in the range [0, 1]. &quot;</span>
                    <span class="s4">f&quot;Received: ema_momentum=</span><span class="s0">{</span><span class="s1">ema_momentum</span><span class="s0">}</span><span class="s4">&quot;</span>
                <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">ema_overwrite_frequency </span><span class="s0">and </span><span class="s2">(</span>
                <span class="s0">not </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">ema_overwrite_frequency</span><span class="s2">, </span><span class="s1">int</span><span class="s2">)</span>
                <span class="s0">or </span><span class="s1">ema_overwrite_frequency </span><span class="s2">&lt; </span><span class="s3">1</span>
            <span class="s2">):</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;`ema_overwrite_frequency` must be an integer &gt;= 1 or &quot;</span>
                    <span class="s4">&quot;None. Received: ema_overwrite_frequency=&quot;</span>
                    <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">ema_overwrite_frequency</span><span class="s0">}</span><span class="s4">&quot;</span>
                <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">ema_momentum </span><span class="s2">= </span><span class="s1">ema_momentum</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">ema_overwrite_frequency </span><span class="s2">= </span><span class="s1">ema_overwrite_frequency</span>

        <span class="s1">clip_args_sum </span><span class="s2">= </span><span class="s1">sum</span><span class="s2">(</span>
            <span class="s1">a </span><span class="s0">is not None for </span><span class="s1">a </span><span class="s0">in </span><span class="s2">[</span><span class="s1">clipnorm</span><span class="s2">, </span><span class="s1">clipvalue</span><span class="s2">, </span><span class="s1">global_clipnorm</span><span class="s2">]</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">clip_args_sum </span><span class="s2">&gt; </span><span class="s3">1</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">&quot;Only one of `clipnorm`, `clipvalue` and `global_clipnorm` can &quot;</span>
                <span class="s4">f&quot;be set. Received: clipnorm=</span><span class="s0">{</span><span class="s1">clipnorm</span><span class="s0">}</span><span class="s4">, &quot;</span>
                <span class="s4">f&quot;clipvalue=</span><span class="s0">{</span><span class="s1">clipvalue</span><span class="s0">}</span><span class="s4">, global_clipnorm=</span><span class="s0">{</span><span class="s1">global_clipnorm</span><span class="s0">}</span><span class="s4">&quot;</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">False</span>

        <span class="s5"># Set up variable tracking.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_variables </span><span class="s2">= []</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables </span><span class="s2">= []</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker </span><span class="s2">= </span><span class="s1">tracking</span><span class="s2">.</span><span class="s1">Tracker</span><span class="s2">(</span>
            <span class="s2">{</span>
                <span class="s4">&quot;variables&quot;</span><span class="s2">: (</span>
                    <span class="s0">lambda </span><span class="s1">x</span><span class="s2">: </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">),</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">_variables</span><span class="s2">,</span>
                <span class="s2">),</span>
            <span class="s2">}</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables_indices </span><span class="s2">= {}</span>

        <span class="s5"># Create iteration variable</span>
        <span class="s5"># Note: dtype=&quot;int&quot; will resolve to int32 in JAX</span>
        <span class="s5"># (since int64 is disallowed in JAX) and to int64 in TF.</span>
        <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
            <span class="s1">iterations </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">(</span>
                <span class="s3">0</span><span class="s2">,</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s4">&quot;iteration&quot;</span><span class="s2">,</span>
                <span class="s1">dtype</span><span class="s2">=</span><span class="s4">&quot;int&quot;</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
                <span class="s1">aggregation</span><span class="s2">=</span><span class="s4">&quot;only_first_replica&quot;</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_track_variable</span><span class="s2">(</span><span class="s1">iterations</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations </span><span class="s2">= </span><span class="s1">iterations</span>

        <span class="s5"># Create learning rate (schedule or variable)</span>
        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span>
            <span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">LearningRateSchedule</span>
        <span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate </span><span class="s2">= </span><span class="s1">learning_rate</span>
        <span class="s0">elif </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate </span><span class="s2">= </span><span class="s1">learning_rate</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">if not </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">float</span><span class="s2">):</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;Argument `learning_rate` should be float, or an instance &quot;</span>
                    <span class="s4">&quot;of LearningRateSchedule, or a callable &quot;</span>
                    <span class="s4">&quot;(that takes in the current iteration value &quot;</span>
                    <span class="s4">&quot;and returns the corresponding learning rate value). &quot;</span>
                    <span class="s4">f&quot;Received instead: learning_rate=</span><span class="s0">{</span><span class="s1">learning_rate</span><span class="s0">}</span><span class="s4">&quot;</span>
                <span class="s2">)</span>
            <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
                <span class="s1">learning_rate </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">(</span>
                    <span class="s1">learning_rate</span><span class="s2">,</span>
                    <span class="s1">name</span><span class="s2">=</span><span class="s4">&quot;learning_rate&quot;</span><span class="s2">,</span>
                    <span class="s1">dtype</span><span class="s2">=</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">floatx</span><span class="s2">(),</span>
                    <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
                    <span class="s1">aggregation</span><span class="s2">=</span><span class="s4">&quot;only_first_replica&quot;</span><span class="s2">,</span>
                <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_track_variable</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate </span><span class="s2">= </span><span class="s1">learning_rate</span>

    <span class="s2">@</span><span class="s1">property</span>
    <span class="s0">def </span><span class="s1">iterations</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">floor_divide</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span>
            <span class="s2">)</span>

        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations</span>

    <span class="s0">def </span><span class="s1">_track_variable</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">add_to_store</span><span class="s2">(</span><span class="s4">&quot;variables&quot;</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">tracking</span><span class="s2">.</span><span class="s1">no_automatic_dependency_tracking</span>
    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average </span><span class="s2">= []</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_accumulated_gradients </span><span class="s2">= []</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">variables</span><span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables_indices</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)] = </span><span class="s1">i</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">:</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
                        <span class="s1">variable</span><span class="s2">,</span>
                        <span class="s1">name</span><span class="s2">=</span><span class="s4">&quot;average&quot;</span><span class="s2">,</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_accumulated_gradients</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
                        <span class="s1">variable</span><span class="s2">,</span>
                        <span class="s1">name</span><span class="s2">=</span><span class="s4">&quot;gradient_accumulator&quot;</span><span class="s2">,</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables </span><span class="s2">= </span><span class="s1">variables</span><span class="s2">[:]</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">):</span>
        <span class="s5"># Helper function to get a stable ID and the variable instance mapping.</span>
        <span class="s0">return </span><span class="s1">id</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">property</span>
    <span class="s0">def </span><span class="s1">variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_variables</span><span class="s2">[:]</span>

    <span class="s0">def </span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables_indices</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)]</span>

    <span class="s0">def </span><span class="s1">add_variable</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">shape</span><span class="s2">,</span>
        <span class="s1">initializer</span><span class="s2">=</span><span class="s4">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">aggregation</span><span class="s2">=</span><span class="s4">&quot;mean&quot;</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_super_called</span><span class="s2">()</span>
        <span class="s1">initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">initializer</span><span class="s2">)</span>
        <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
            <span class="s1">variable </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">(</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializer</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=</span><span class="s1">shape</span><span class="s2">,</span>
                <span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
                <span class="s1">aggregation</span><span class="s2">=</span><span class="s1">aggregation</span><span class="s2">,</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_track_variable</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">variable</span>

    <span class="s0">def </span><span class="s1">add_variable_from_reference</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">reference_variable</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">initializer</span><span class="s2">=</span><span class="s4">&quot;zeros&quot;</span>
    <span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Add an all-zeros variable with the shape and dtype of a reference 
        variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">name </span><span class="s2">= </span><span class="s1">name </span><span class="s0">or </span><span class="s4">&quot;var&quot;</span>
        <span class="s0">if </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">reference_variable</span><span class="s2">, </span><span class="s4">&quot;path&quot;</span><span class="s2">):</span>
            <span class="s1">name </span><span class="s2">= </span><span class="s1">reference_variable</span><span class="s2">.</span><span class="s1">path</span><span class="s2">.</span><span class="s1">replace</span><span class="s2">(</span><span class="s4">&quot;/&quot;</span><span class="s2">, </span><span class="s4">&quot;_&quot;</span><span class="s2">) + </span><span class="s4">&quot;_&quot; </span><span class="s2">+ </span><span class="s1">name</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">name </span><span class="s2">= (</span>
                <span class="s1">str</span><span class="s2">(</span><span class="s1">reference_variable</span><span class="s2">.</span><span class="s1">name</span><span class="s2">).</span><span class="s1">replace</span><span class="s2">(</span><span class="s4">&quot;/&quot;</span><span class="s2">, </span><span class="s4">&quot;_&quot;</span><span class="s2">).</span><span class="s1">replace</span><span class="s2">(</span><span class="s4">&quot;:&quot;</span><span class="s2">, </span><span class="s4">&quot;_&quot;</span><span class="s2">)</span>
                <span class="s2">+ </span><span class="s4">&quot;_&quot;</span>
                <span class="s2">+ </span><span class="s1">name</span>
            <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">shape</span><span class="s2">=</span><span class="s1">reference_variable</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">,</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializer</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s1">reference_variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">,</span>
        <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_check_variables_are_known</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">):</span>
        <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">variables</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">v</span><span class="s2">) </span><span class="s0">not in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables_indices</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">f&quot;Unknown variable: </span><span class="s0">{</span><span class="s1">v</span><span class="s0">}</span><span class="s4">. This optimizer can only &quot;</span>
                    <span class="s4">&quot;be called for the variables it was originally built with. &quot;</span>
                    <span class="s4">&quot;When working with a new set of variables, you should &quot;</span>
                    <span class="s4">&quot;recreate a new optimizer instance.&quot;</span>
                <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">assign</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">value</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Assign a value to a variable. 
 
        This should be used in optimizers instead of `variable.assign(value)` to 
        support backend specific optimizations. 
        Note that the variable can be a model variable or an optimizer variable; 
        it can be a backend native variable or a Keras variable. 
 
        Args: 
            variable: The variable to update. 
            value: The value to add to the variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">assign_add</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">value</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Add a value to a variable. 
 
        This should be used in optimizers instead of 
        `variable.assign_add(value)` to support backend specific optimizations. 
        Note that the variable can be a model variable or an optimizer variable; 
        it can be a backend native variable or a Keras variable. 
 
        Args: 
            variable: The variable to update. 
            value: The value to add to the variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign_add</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">assign_sub</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">value</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Subtract a value from a variable. 
 
        This should be used in optimizers instead of 
        `variable.assign_sub(value)` to support backend specific optimizations. 
        Note that the variable can be a model variable or an optimizer variable; 
        it can be a backend native variable or a Keras variable. 
 
        Args: 
            variable: The variable to update. 
            value: The value to add to the variable. 
        &quot;&quot;&quot;</span>
        <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign_sub</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">update_step</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">gradient</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">NotImplementedError</span>

    <span class="s0">def </span><span class="s1">apply_gradients</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads_and_vars</span><span class="s2">):</span>
        <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables </span><span class="s2">= </span><span class="s1">zip</span><span class="s2">(*</span><span class="s1">grads_and_vars</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">apply</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">)</span>
        <span class="s5"># Return iterations for compat with tf.keras.</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations</span>

    <span class="s0">def </span><span class="s1">apply</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Update traininable variables according to provided gradient values. 
 
        `grads` should be a list of gradient tensors 
        with 1:1 mapping to the list of variables the optimizer was built with. 
 
        `trainable_variables` can be provided 
        on the first call to build the optimizer. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">) == </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s5"># It is possible that the grad is empty. In this case,</span>
            <span class="s5"># `apply_gradients` is a no-op.</span>
            <span class="s0">return</span>

        <span class="s0">if </span><span class="s1">trainable_variables </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;When passing `grads` without `variables`, the optimizer &quot;</span>
                    <span class="s4">&quot;must already be built on a list of variables. &quot;</span>
                    <span class="s4">&quot;Call `optimizer.build(trainable_variables)` first. &quot;</span>
                <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables_indices</span><span class="s2">):</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">&quot;When passing `grads` as a list of gradient tensors, the &quot;</span>
                    <span class="s4">f&quot;gradients must match `optimizer.variables` one-to-on. &quot;</span>
                    <span class="s4">f&quot;Received a list of </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span><span class="s0">} </span><span class="s4">gradients, but the &quot;</span>
                    <span class="s4">f&quot;optimizer is tracking </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span><span class="s2">)</span><span class="s0">} </span><span class="s4">&quot;</span>
                    <span class="s4">&quot;trainable variables.&quot;</span>
                <span class="s2">)</span>
            <span class="s1">trainable_variables </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">trainable_variables </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span>
            <span class="s5"># Optionally build optimizer.</span>
            <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
                <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_variables_are_known</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span>

        <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">name_scope</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s1">caller</span><span class="s2">=</span><span class="s1">self</span><span class="s2">):</span>
            <span class="s5"># Overwrite targeted variables directly with their gradients if</span>
            <span class="s5"># their `overwrite_with_gradient` is set.</span>
            <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables </span><span class="s2">= (</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_overwrite_variables_directly_with_gradients</span><span class="s2">(</span>
                    <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span>
                <span class="s2">)</span>
            <span class="s2">)</span>

            <span class="s5"># Filter empty gradients.</span>
            <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_filter_empty_gradients</span><span class="s2">(</span>
                <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span>
            <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">list</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)) == </span><span class="s3">0</span><span class="s2">:</span>
                <span class="s0">return</span>

            <span class="s5"># Unscale gradients.</span>
            <span class="s1">scale </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">loss_scale_factor</span>
            <span class="s0">if </span><span class="s1">scale </span><span class="s0">is not None</span><span class="s2">:</span>
                <span class="s1">grads </span><span class="s2">= [</span><span class="s1">g </span><span class="s0">if </span><span class="s1">g </span><span class="s0">is None else </span><span class="s1">g </span><span class="s2">/ </span><span class="s1">scale </span><span class="s0">for </span><span class="s1">g </span><span class="s0">in </span><span class="s1">grads</span><span class="s2">]</span>

            <span class="s5"># Apply gradient updates.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_backend_apply_gradients</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">)</span>
            <span class="s5"># Apply variable constraints after applying gradients.</span>
            <span class="s0">for </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">trainable_variables</span><span class="s2">:</span>
                <span class="s0">if </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">constraint </span><span class="s0">is not None</span><span class="s2">:</span>
                    <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">.</span><span class="s1">constraint</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">_backend_apply_gradients</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Apply method that can be overridden by different backends. 
 
        JAX overrides it in order to deal with statelessness in gradient 
        accumulation and EMA handling. 
 
        The below implementation is intended to be generally backend-agnostic, 
        but may not work with all backends. 
 
        This method does 4 things: 
        - Call the optimizer's update_step() to update trainable variables 
            and optimizer variables. 
        - Update EMA variables, if EMA is configured. 
        - Update gradient accumulators, if gradient accumulation is configured. 
        - Update the iteration counter. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
            <span class="s1">is_update_step </span><span class="s2">= (</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations </span><span class="s2">+ </span><span class="s3">1</span>
            <span class="s2">) % </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps </span><span class="s2">== </span><span class="s3">0</span>
            <span class="s5"># `trainable_variables` might have been filtered in previous</span>
            <span class="s5"># processing steps, so we need to ensure the correct mapping between</span>
            <span class="s5"># `self._accumulated_gradients` and `trainable_variables`</span>
            <span class="s1">acc_grads </span><span class="s2">= [</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_accumulated_gradients</span><span class="s2">[</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)]</span>
                <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">trainable_variables</span>
            <span class="s2">]</span>

            <span class="s0">def </span><span class="s1">_update_step_fn</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">):</span>
                <span class="s5"># Run update step with accumulated grads + reset accumulators</span>
                <span class="s1">steps </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span>
                <span class="s1">grads </span><span class="s2">= [</span>
                    <span class="s2">(</span><span class="s1">g </span><span class="s2">+ </span><span class="s1">acc_g</span><span class="s2">) / </span><span class="s1">steps </span><span class="s0">for </span><span class="s1">g</span><span class="s2">, </span><span class="s1">acc_g </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">acc_grads</span><span class="s2">)</span>
                <span class="s2">]</span>

                <span class="s5"># Apply clipping and weight decay.</span>
                <span class="s1">grads </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_clip_gradients</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_apply_weight_decay</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span>

                <span class="s1">self</span><span class="s2">.</span><span class="s1">_backend_update_step</span><span class="s2">(</span>
                    <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">learning_rate</span>
                <span class="s2">)</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_backend_reset_gradient_accumulators</span><span class="s2">()</span>

            <span class="s1">ops</span><span class="s2">.</span><span class="s1">cond</span><span class="s2">(</span>
                <span class="s1">is_update_step</span><span class="s2">,</span>
                <span class="s0">lambda</span><span class="s2">: </span><span class="s1">_update_step_fn</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">),</span>
                <span class="s0">lambda</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_backend_increment_gradient_accumulators</span><span class="s2">(</span>
                    <span class="s1">grads</span><span class="s2">, </span><span class="s1">acc_grads</span>
                <span class="s2">),</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s5"># Apply clipping and weight decay.</span>
            <span class="s1">grads </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_clip_gradients</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_apply_weight_decay</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span>

            <span class="s5"># Run udpate step.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_backend_update_step</span><span class="s2">(</span>
                <span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">learning_rate</span>
            <span class="s2">)</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_update_model_variables_moving_average</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span>
            <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">ema_overwrite_frequency</span><span class="s2">:</span>
                <span class="s5"># Only when self.ema_overwrite_frequency is not None, we</span>
                <span class="s5"># overwrite the model variables.</span>
                <span class="s1">should_overwrite_model_vars </span><span class="s2">= (</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">iterations </span><span class="s2">+ </span><span class="s3">1</span>
                <span class="s2">) % </span><span class="s1">self</span><span class="s2">.</span><span class="s1">ema_overwrite_frequency </span><span class="s2">== </span><span class="s3">0</span>
                <span class="s1">ops</span><span class="s2">.</span><span class="s1">cond</span><span class="s2">(</span>
                    <span class="s1">should_overwrite_model_vars</span><span class="s2">,</span>
                    <span class="s0">lambda</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_overwrite_model_variables_with_average_value</span><span class="s2">(</span>
                        <span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span>
                    <span class="s2">),</span>
                    <span class="s0">lambda</span><span class="s2">: </span><span class="s0">None</span><span class="s2">,</span>
                <span class="s2">)</span>
        <span class="s5"># Update iteration counter.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations</span><span class="s2">.</span><span class="s1">assign_add</span><span class="s2">(</span><span class="s3">1</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_backend_update_step</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Collective update_step that can be overridden by the backend. 
 
        It is overridden by torch for performance reasons, and 
        by TF to support tf.distribute. 
        &quot;&quot;&quot;</span>
        <span class="s0">for </span><span class="s1">grad</span><span class="s2">, </span><span class="s1">var </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">update_step</span><span class="s2">(</span><span class="s1">grad</span><span class="s2">, </span><span class="s1">var</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_backend_reset_gradient_accumulators</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">for </span><span class="s1">g_acc </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_accumulated_gradients</span><span class="s2">:</span>
            <span class="s1">g_acc</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">g_acc</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">g_acc</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">_backend_increment_gradient_accumulators</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">acc_grads</span><span class="s2">):</span>
        <span class="s1">new_g_accs </span><span class="s2">= [(</span><span class="s1">g </span><span class="s2">+ </span><span class="s1">acc_g</span><span class="s2">) </span><span class="s0">for </span><span class="s1">g</span><span class="s2">, </span><span class="s1">acc_g </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">acc_grads</span><span class="s2">)]</span>
        <span class="s0">for </span><span class="s1">n_g_acc</span><span class="s2">, </span><span class="s1">g_acc </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">new_g_accs</span><span class="s2">, </span><span class="s1">acc_grads</span><span class="s2">):</span>
            <span class="s1">g_acc</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">n_g_acc</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">stateless_apply</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">optimizer_variables</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">):</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_super_called</span><span class="s2">()</span>

        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">f&quot;To call `stateless_apply`, </span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">__name__</span><span class="s0">} </span><span class="s4">&quot;</span>
                <span class="s4">&quot;must be built (i.e. its variables must have been created). &quot;</span>
                <span class="s4">&quot;You can build it via `optimizer.build(trainable_variables)`.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">optimizer_variables</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">&quot;Argument `optimizer_variables` must be a list of tensors &quot;</span>
                <span class="s4">f&quot;corresponding 1:1 to </span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">__name__</span><span class="s0">}</span><span class="s4">().variables. &quot;</span>
                <span class="s4">f&quot;Received list with length </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">optimizer_variables</span><span class="s2">)</span><span class="s0">}</span><span class="s4">, but &quot;</span>
                <span class="s4">f&quot;expected </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">)</span><span class="s0">} </span><span class="s4">variables.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">&quot;Argument `optimizer_variables` must be a list of tensors &quot;</span>
                <span class="s4">&quot;corresponding 1:1 to the trainable variables list that &quot;</span>
                <span class="s4">&quot;the optimizer was built with. Received &quot;</span>
                <span class="s4">f&quot;len(trainable_variables) == </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span><span class="s0">} </span><span class="s4">&quot;</span>
                <span class="s4">&quot;whereas the optimizer was built with &quot;</span>
                <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span><span class="s2">)</span><span class="s0">} </span><span class="s4">variables.&quot;</span>
            <span class="s2">)</span>

        <span class="s5"># Gather variable mapping</span>
        <span class="s1">mapping </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span>
            <span class="s1">zip</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">)</span>
        <span class="s2">) + </span><span class="s1">list</span><span class="s2">(</span><span class="s1">zip</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">, </span><span class="s1">optimizer_variables</span><span class="s2">))</span>

        <span class="s5"># Call in stateless scope</span>
        <span class="s0">with </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">StatelessScope</span><span class="s2">(</span><span class="s1">state_mapping</span><span class="s2">=</span><span class="s1">mapping</span><span class="s2">) </span><span class="s0">as </span><span class="s1">scope</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">apply</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span>

        <span class="s5"># Gather updated variables</span>
        <span class="s1">trainable_variables </span><span class="s2">= []</span>
        <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_trainable_variables</span><span class="s2">:</span>
            <span class="s1">new_v </span><span class="s2">= </span><span class="s1">scope</span><span class="s2">.</span><span class="s1">get_current_value</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)</span>
            <span class="s0">if </span><span class="s1">new_v </span><span class="s0">is not None</span><span class="s2">:</span>
                <span class="s1">trainable_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">new_v</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">trainable_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)</span>
        <span class="s1">optimizer_variables </span><span class="s2">= []</span>
        <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">:</span>
            <span class="s1">new_v </span><span class="s2">= </span><span class="s1">scope</span><span class="s2">.</span><span class="s1">get_current_value</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)</span>
            <span class="s0">if </span><span class="s1">new_v </span><span class="s0">is not None</span><span class="s2">:</span>
                <span class="s1">optimizer_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">new_v</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">optimizer_variables</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">optimizer_variables</span>

    <span class="s0">def </span><span class="s1">scale_loss</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">loss</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Scale the loss before computing gradients. 
 
        Scales the loss before gradients are computed in a `train_step`. This 
        is primarily useful during mixed precision training to prevent numeric 
        underflow. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">loss_scale_factor </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">loss </span><span class="s2">* </span><span class="s1">self</span><span class="s2">.</span><span class="s1">loss_scale_factor</span>
        <span class="s0">return </span><span class="s1">loss</span>

    <span class="s2">@</span><span class="s1">property</span>
    <span class="s0">def </span><span class="s1">learning_rate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_current_learning_rate</span><span class="s2">()</span>

    <span class="s2">@</span><span class="s1">learning_rate</span><span class="s2">.</span><span class="s1">setter</span>
    <span class="s0">def </span><span class="s1">learning_rate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">learning_rate</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">):</span>
            <span class="s1">prev_lr_var </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">prev_lr_var </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span>
            <span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">LearningRateSchedule</span>
        <span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate </span><span class="s2">= </span><span class="s1">learning_rate</span>
        <span class="s0">elif </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate </span><span class="s2">= </span><span class="s1">learning_rate</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">LearningRateSchedule</span>
            <span class="s2">):</span>
                <span class="s0">raise </span><span class="s1">TypeError</span><span class="s2">(</span>
                    <span class="s4">&quot;This optimizer was created with a `LearningRateSchedule`&quot;</span>
                    <span class="s4">&quot; object as its `learning_rate` constructor argument, &quot;</span>
                    <span class="s4">&quot;hence its learning rate is not settable. If you need the&quot;</span>
                    <span class="s4">&quot; learning rate to be settable, you should instantiate &quot;</span>
                    <span class="s4">&quot;the optimizer with a float `learning_rate` argument.&quot;</span>
                <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">learning_rate</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">prev_lr_var </span><span class="s0">is not None and not </span><span class="s1">isinstance</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span>
        <span class="s2">):</span>
            <span class="s5"># Untrack learning rate variable</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_untrack_variable</span><span class="s2">(</span><span class="s1">prev_lr_var</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">set_weights</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Set the weights of the optimizer.&quot;&quot;&quot;</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">&quot;You are calling `set_weights()` on an optimizer that has not &quot;</span>
                <span class="s4">&quot;yet been built. Please call &quot;</span>
                <span class="s4">&quot;`optimizer.build(trainable_variables)` to create the &quot;</span>
                <span class="s4">&quot;optimizer weights before calling `set_weights()`.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">for </span><span class="s1">variable</span><span class="s2">, </span><span class="s1">weight </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_variables</span><span class="s2">, </span><span class="s1">weights</span><span class="s2">):</span>
            <span class="s0">if </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">shape </span><span class="s2">!= </span><span class="s1">weight</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s4">f&quot;Optimizer variable </span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)</span><span class="s0">} </span><span class="s4">has shape &quot;</span>
                    <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">str</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span><span class="s0">} </span><span class="s4">not compatible with provided &quot;</span>
                    <span class="s4">f&quot;weight shape </span><span class="s0">{</span><span class="s1">str</span><span class="s2">(</span><span class="s1">weight</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span><span class="s0">}</span><span class="s4">.&quot;</span>
                <span class="s2">)</span>
            <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">weight</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">save_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Get the state of this optimizer object.&quot;&quot;&quot;</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">):</span>
            <span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)] = </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">()</span>

    <span class="s0">def </span><span class="s1">load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Set the state of this optimizer object.&quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">()) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">):</span>
            <span class="s1">msg </span><span class="s2">= (</span>
                <span class="s4">f&quot;Skipping variable loading for optimizer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s0">}</span><span class="s4">', &quot;</span>
                <span class="s4">f&quot;because it has </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">)</span><span class="s0">} </span><span class="s4">variables whereas &quot;</span>
                <span class="s4">f&quot;the saved optimizer has </span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">store</span><span class="s2">.</span><span class="s1">keys</span><span class="s2">())</span><span class="s0">} </span><span class="s4">variables. &quot;</span>
            <span class="s2">)</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">) == </span><span class="s3">0</span><span class="s2">:</span>
                <span class="s1">msg </span><span class="s2">+= (</span>
                    <span class="s4">&quot;This is likely because the optimizer has not been &quot;</span>
                    <span class="s4">&quot;called/built yet.&quot;</span>
                <span class="s2">)</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span><span class="s1">msg</span><span class="s2">, </span><span class="s1">stacklevel</span><span class="s2">=</span><span class="s3">2</span><span class="s2">)</span>
            <span class="s0">return</span>
        <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">):</span>
            <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">store</span><span class="s2">[</span><span class="s1">str</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)])</span>

    <span class="s0">def </span><span class="s1">_get_current_learning_rate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">LearningRateSchedule</span>
        <span class="s2">):</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">()</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span>

    <span class="s0">def </span><span class="s1">_overwrite_variables_directly_with_gradients</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">vars</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Overwrite the variables directly by their gradients. 
 
        This method is designed for a special case where we want to overwrite 
        the variable directly with its computed gradient. For example, in float8 
        training, new `scale` and `amax_history` are computed as gradients, and 
        we want to overwrite them directly instead of following the typical 
        procedure such as gradient descent with a learning rate, gradient 
        clipping and weight decaying. 
 
        After the update, the processed pairs will be filtered out. 
        &quot;&quot;&quot;</span>
        <span class="s5"># Shortcut for `tf.Variable` because it doesn't have a</span>
        <span class="s5"># `overwrite_with_gradient` attr</span>
        <span class="s0">if not </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">vars</span><span class="s2">[</span><span class="s3">0</span><span class="s2">], </span><span class="s4">&quot;overwrite_with_gradient&quot;</span><span class="s2">):</span>
            <span class="s0">return </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">vars</span>

        <span class="s5"># Shallow copies</span>
        <span class="s1">filtered_grads </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span>
        <span class="s1">filtered_vars </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">vars</span><span class="s2">)</span>

        <span class="s5"># Iterate from right to left for safe popping</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">filtered_grads</span><span class="s2">) - </span><span class="s3">1</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">):</span>
            <span class="s1">g</span><span class="s2">, </span><span class="s1">v </span><span class="s2">= </span><span class="s1">filtered_grads</span><span class="s2">[</span><span class="s1">i</span><span class="s2">], </span><span class="s1">filtered_vars</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]</span>
            <span class="s0">if </span><span class="s1">v</span><span class="s2">.</span><span class="s1">overwrite_with_gradient</span><span class="s2">:</span>
                <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">:</span>
                    <span class="s5"># Utilize a stateless manner for JAX compatibility</span>
                    <span class="s1">steps </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span>
                    <span class="s1">is_update_step </span><span class="s2">= (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_iterations </span><span class="s2">+ </span><span class="s3">1</span><span class="s2">) % </span><span class="s1">steps </span><span class="s2">== </span><span class="s3">0</span>
                    <span class="s1">acc_g </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_accumulated_gradients</span><span class="s2">[</span>
                        <span class="s1">self</span><span class="s2">.</span><span class="s1">_get_variable_index</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)</span>
                    <span class="s2">]</span>
                    <span class="s5"># `ops.maximum` is utilized for gradient accumulation for</span>
                    <span class="s5"># `overwrite_with_gradient=True` variables</span>
                    <span class="s1">new_g_acc </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cond</span><span class="s2">(</span>
                        <span class="s1">is_update_step</span><span class="s2">,</span>
                        <span class="s0">lambda</span><span class="s2">: </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">g</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">g</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">),</span>
                        <span class="s0">lambda</span><span class="s2">: </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">g</span><span class="s2">, </span><span class="s1">acc_g</span><span class="s2">),</span>
                    <span class="s2">)</span>
                    <span class="s1">new_g </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cond</span><span class="s2">(</span>
                        <span class="s1">is_update_step</span><span class="s2">,</span>
                        <span class="s0">lambda</span><span class="s2">: </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">g</span><span class="s2">, </span><span class="s1">acc_g</span><span class="s2">),</span>
                        <span class="s0">lambda</span><span class="s2">: </span><span class="s1">g</span><span class="s2">,</span>
                    <span class="s2">)</span>
                    <span class="s1">new_v </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cond</span><span class="s2">(</span>
                        <span class="s1">is_update_step</span><span class="s2">, </span><span class="s0">lambda</span><span class="s2">: </span><span class="s1">new_g</span><span class="s2">, </span><span class="s0">lambda</span><span class="s2">: </span><span class="s1">v</span><span class="s2">.</span><span class="s1">value</span>
                    <span class="s2">)</span>
                    <span class="s1">v</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">new_v</span><span class="s2">)</span>
                    <span class="s1">acc_g</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">new_g_acc</span><span class="s2">)</span>
                <span class="s0">else</span><span class="s2">:</span>
                    <span class="s1">v</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">g</span><span class="s2">)</span>
                <span class="s1">filtered_grads</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)</span>
                <span class="s1">filtered_vars</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">filtered_grads</span><span class="s2">, </span><span class="s1">filtered_vars</span>

    <span class="s0">def </span><span class="s1">_filter_empty_gradients</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">, </span><span class="s1">vars</span><span class="s2">):</span>
        <span class="s1">filtered_grads </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">)</span>
        <span class="s1">filtered_vars </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">vars</span><span class="s2">)</span>
        <span class="s1">missing_grad_vars </span><span class="s2">= []</span>

        <span class="s5"># Iterate from right to left for safe popping</span>
        <span class="s0">for </span><span class="s1">i </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">filtered_grads</span><span class="s2">) - </span><span class="s3">1</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">, -</span><span class="s3">1</span><span class="s2">):</span>
            <span class="s0">if </span><span class="s1">filtered_grads</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] </span><span class="s0">is None</span><span class="s2">:</span>
                <span class="s1">filtered_grads</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)</span>
                <span class="s1">v </span><span class="s2">= </span><span class="s1">filtered_vars</span><span class="s2">.</span><span class="s1">pop</span><span class="s2">(</span><span class="s1">i</span><span class="s2">)</span>
                <span class="s1">missing_grad_vars</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">v</span><span class="s2">.</span><span class="s1">name</span><span class="s2">)</span>

        <span class="s0">if not </span><span class="s1">filtered_grads</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span><span class="s4">&quot;No gradients provided for any variable.&quot;</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">missing_grad_vars</span><span class="s2">:</span>
            <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
                <span class="s4">&quot;Gradients do not exist for variables &quot;</span>
                <span class="s4">f&quot;</span><span class="s0">{</span><span class="s1">list</span><span class="s2">(</span><span class="s1">reversed</span><span class="s2">(</span><span class="s1">missing_grad_vars</span><span class="s2">))</span><span class="s0">} </span><span class="s4">when minimizing the loss.&quot;</span>
                <span class="s4">&quot; If using `model.compile()`, did you forget to provide a &quot;</span>
                <span class="s4">&quot;`loss` argument?&quot;</span>
            <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">filtered_grads</span><span class="s2">, </span><span class="s1">filtered_vars</span>

    <span class="s0">def </span><span class="s1">_clip_gradients</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">grads</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm </span><span class="s0">and </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm </span><span class="s2">&gt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s2">[</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_clip_by_norm</span><span class="s2">(</span><span class="s1">g</span><span class="s2">) </span><span class="s0">if </span><span class="s1">g </span><span class="s0">is not None else </span><span class="s1">g </span><span class="s0">for </span><span class="s1">g </span><span class="s0">in </span><span class="s1">grads</span>
            <span class="s2">]</span>
        <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">global_clipnorm </span><span class="s0">and </span><span class="s1">self</span><span class="s2">.</span><span class="s1">global_clipnorm </span><span class="s2">&gt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">clip_by_global_norm</span><span class="s2">(</span><span class="s1">grads</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">global_clipnorm</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipvalue </span><span class="s0">and </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipvalue </span><span class="s2">&gt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s1">v </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipvalue</span>
            <span class="s0">return </span><span class="s2">[</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">clip</span><span class="s2">(</span><span class="s1">g</span><span class="s2">, -</span><span class="s1">v</span><span class="s2">, </span><span class="s1">v</span><span class="s2">) </span><span class="s0">if </span><span class="s1">g </span><span class="s0">is not None else </span><span class="s1">g </span><span class="s0">for </span><span class="s1">g </span><span class="s0">in </span><span class="s1">grads</span><span class="s2">]</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">grads</span>

    <span class="s0">def </span><span class="s1">exclude_from_weight_decay</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">var_list</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">var_names</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Exclude variables from weight decay. 
 
        This method must be called before the optimizer's `build` method is 
        called. You can set specific variables to exclude out, or set a list of 
        strings as the anchor words, if any of which appear in a variable's 
        name, then the variable is excluded. 
 
        Args: 
            var_list: A list of `Variable`s to exclude from weight decay. 
            var_names: A list of strings. If any string in `var_names` appear 
                in the model variable's name, then this model variable is 
                excluded from weight decay. For example, `var_names=['bias']` 
                excludes all bias variables from weight decay. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_built&quot;</span><span class="s2">) </span><span class="s0">and </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_built</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">&quot;`exclude_from_weight_decay()` can only be configued before &quot;</span>
                <span class="s4">&quot;the optimizer is built.&quot;</span>
            <span class="s2">)</span>

        <span class="s5"># Use a `set` for the ids of `var_list` to speed up the searching</span>
        <span class="s0">if </span><span class="s1">var_list</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay </span><span class="s2">= </span><span class="s1">set</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">) </span><span class="s0">for </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">var_list</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay </span><span class="s2">= </span><span class="s1">set</span><span class="s2">()</span>

        <span class="s5"># Precompile the pattern for `var_names` to speed up the searching</span>
        <span class="s0">if </span><span class="s1">var_names </span><span class="s0">and </span><span class="s1">len</span><span class="s2">(</span><span class="s1">var_names</span><span class="s2">) &gt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_pattern </span><span class="s2">= </span><span class="s1">re</span><span class="s2">.</span><span class="s1">compile</span><span class="s2">(</span>
                <span class="s4">&quot;|&quot;</span><span class="s2">.</span><span class="s1">join</span><span class="s2">(</span><span class="s1">set</span><span class="s2">(</span><span class="s1">var_names</span><span class="s2">))</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_pattern </span><span class="s2">= </span><span class="s0">None</span>

        <span class="s5"># Reset cache</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache </span><span class="s2">= </span><span class="s1">dict</span><span class="s2">()</span>

    <span class="s0">def </span><span class="s1">_use_weight_decay</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">):</span>
        <span class="s1">variable_id </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_var_key</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)</span>

        <span class="s5"># Immediately return the value if `variable_id` hits the cache</span>
        <span class="s0">if not </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_exclude_from_weight_decay_cache&quot;</span><span class="s2">):</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache </span><span class="s2">= </span><span class="s1">dict</span><span class="s2">()</span>
        <span class="s0">if </span><span class="s1">variable_id </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache</span><span class="s2">[</span><span class="s1">variable_id</span><span class="s2">]</span>

        <span class="s5"># Determine whether the variable should apply weight decay or not</span>
        <span class="s1">exclude_from_weight_decay </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_exclude_from_weight_decay&quot;</span><span class="s2">, </span><span class="s1">set</span><span class="s2">()</span>
        <span class="s2">)</span>
        <span class="s1">exclude_from_weight_decay_pattern </span><span class="s2">= </span><span class="s1">getattr</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_exclude_from_weight_decay_pattern&quot;</span><span class="s2">, </span><span class="s0">None</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">variable_id </span><span class="s0">in </span><span class="s1">exclude_from_weight_decay</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache</span><span class="s2">[</span><span class="s1">variable_id</span><span class="s2">] = </span><span class="s0">False</span>
            <span class="s0">return False</span>
        <span class="s0">if </span><span class="s1">exclude_from_weight_decay_pattern </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s2">(</span>
                <span class="s1">re</span><span class="s2">.</span><span class="s1">search</span><span class="s2">(</span><span class="s1">exclude_from_weight_decay_pattern</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">name</span><span class="s2">)</span>
                <span class="s0">is not None</span>
            <span class="s2">):</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache</span><span class="s2">[</span><span class="s1">variable_id</span><span class="s2">] = </span><span class="s0">False</span>
                <span class="s0">return False</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_exclude_from_weight_decay_cache</span><span class="s2">[</span><span class="s1">variable_id</span><span class="s2">] = </span><span class="s0">True</span>
        <span class="s0">return True</span>

    <span class="s0">def </span><span class="s1">_apply_weight_decay</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s0">return</span>
        <span class="s0">for </span><span class="s1">variable </span><span class="s0">in </span><span class="s1">variables</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_use_weight_decay</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">):</span>
                <span class="s1">lr </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">learning_rate</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
                <span class="s1">wd </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
                <span class="s1">variable</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">variable </span><span class="s2">- </span><span class="s1">variable </span><span class="s2">* </span><span class="s1">wd </span><span class="s2">* </span><span class="s1">lr</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_check_super_called</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if not </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_lock&quot;</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">RuntimeError</span><span class="s2">(</span>
                <span class="s4">f&quot;In optimizer '</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">__name__</span><span class="s0">}</span><span class="s4">', you forgot to call &quot;</span>
                <span class="s4">&quot;`super().__init__()` as the first statement &quot;</span>
                <span class="s4">&quot;in the `__init__()` method. &quot;</span>
                <span class="s4">&quot;Go add it!&quot;</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_update_model_variables_moving_average</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">trainable_variables</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Update the stored moving average using the latest value.&quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">:</span>
            <span class="s0">for </span><span class="s1">var</span><span class="s2">, </span><span class="s1">average </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span>
                <span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average</span>
            <span class="s2">):</span>
                <span class="s1">not_first_step </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">not_equal</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">iterations</span><span class="s2">, </span><span class="s3">0</span><span class="s2">)</span>
                <span class="s1">momentum </span><span class="s2">= (</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">not_first_step</span><span class="s2">, </span><span class="s1">var</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">) * </span><span class="s1">self</span><span class="s2">.</span><span class="s1">ema_momentum</span>
                <span class="s2">)</span>
                <span class="s1">average</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">momentum </span><span class="s2">* </span><span class="s1">average </span><span class="s2">+ (</span><span class="s3">1 </span><span class="s2">- </span><span class="s1">momentum</span><span class="s2">) * </span><span class="s1">var</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_overwrite_model_variables_with_average_value</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">trainable_variables</span>
    <span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Overwrite model variables with its moving average.&quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average</span>
        <span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s4">f&quot;The length of model variables (</span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">trainable_variables</span><span class="s2">)</span><span class="s0">}</span><span class="s4">) &quot;</span>
                <span class="s4">&quot;to override does not match the length of model variables &quot;</span>
                <span class="s4">&quot;stored in the optimizer &quot;</span>
                <span class="s4">f&quot;(</span><span class="s0">{</span><span class="s1">len</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average</span><span class="s2">)</span><span class="s0">}</span><span class="s4">). Please &quot;</span>
                <span class="s4">&quot;check if the optimizer was called on your model.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">for </span><span class="s1">var</span><span class="s2">, </span><span class="s1">average_var </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span>
            <span class="s1">trainable_variables</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_model_variables_moving_average</span>
        <span class="s2">):</span>
            <span class="s1">var</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">average_var</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">finalize_variable_values</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">var_list</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Set the final value of model's trainable variables. 
 
        Sometimes there are some extra steps before ending the variable updates, 
        such as overriding the model variables with its average value. 
 
        Args: 
          var_list: list of model variables. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">:</span>
            <span class="s5"># If the optimizer uses EMA, then when finalizing, we replace the</span>
            <span class="s5"># model variable value with its moving average stored inside</span>
            <span class="s5"># optimizer.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_overwrite_model_variables_with_average_value</span><span class="s2">(</span><span class="s1">var_list</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_obj_type</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s4">&quot;Optimizer&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Returns the config of the optimizer. 
 
        An optimizer config is a Python dictionary (serializable) 
        containing the configuration of an optimizer. 
        The same optimizer can be reinstantiated later 
        (without any saved state) from this configuration. 
 
        Subclass optimizer should override this method to include other 
        hyperparameters. 
 
        Returns: 
            Python dictionary. 
        &quot;&quot;&quot;</span>

        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">LearningRateSchedule</span>
        <span class="s2">):</span>
            <span class="s1">learning_rate </span><span class="s2">= </span><span class="s1">learning_rate_schedule</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span>
            <span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">):</span>
            <span class="s1">learning_rate </span><span class="s2">= </span><span class="s1">float</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">())</span>
        <span class="s0">elif </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">is_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">):</span>
            <span class="s1">learning_rate </span><span class="s2">= </span><span class="s1">float</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">callable</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span><span class="s2">):</span>
            <span class="s1">learning_rate </span><span class="s2">= </span><span class="s1">serialization_lib</span><span class="s2">.</span><span class="s1">serialize_keras_object</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">_learning_rate</span>
            <span class="s2">)</span>

        <span class="s1">config </span><span class="s2">= {</span>
            <span class="s4">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s4">&quot;learning_rate&quot;</span><span class="s2">: </span><span class="s1">learning_rate</span><span class="s2">,</span>
            <span class="s4">&quot;weight_decay&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay</span><span class="s2">,</span>
            <span class="s4">&quot;clipnorm&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm</span><span class="s2">,</span>
            <span class="s4">&quot;global_clipnorm&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">global_clipnorm</span><span class="s2">,</span>
            <span class="s4">&quot;clipvalue&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipvalue</span><span class="s2">,</span>
            <span class="s4">&quot;use_ema&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_ema</span><span class="s2">,</span>
            <span class="s4">&quot;ema_momentum&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">ema_momentum</span><span class="s2">,</span>
            <span class="s4">&quot;ema_overwrite_frequency&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">ema_overwrite_frequency</span><span class="s2">,</span>
            <span class="s4">&quot;loss_scale_factor&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">loss_scale_factor</span><span class="s2">,</span>
            <span class="s4">&quot;gradient_accumulation_steps&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">gradient_accumulation_steps</span><span class="s2">,</span>
        <span class="s2">}</span>
        <span class="s0">return </span><span class="s1">config</span>

    <span class="s2">@</span><span class="s1">classmethod</span>
    <span class="s0">def </span><span class="s1">from_config</span><span class="s2">(</span><span class="s1">cls</span><span class="s2">, </span><span class="s1">config</span><span class="s2">, </span><span class="s1">custom_objects</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s6">&quot;&quot;&quot;Creates an optimizer from its config. 
 
        This method is the reverse of `get_config`, capable of instantiating the 
        same optimizer from the config dictionary. 
 
        Args: 
            config: A Python dictionary, typically the output of get_config. 
            custom_objects: A Python dictionary mapping names to additional 
              user-defined Python objects needed to recreate this optimizer. 
 
        Returns: 
            An optimizer instance. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s4">&quot;learning_rate&quot; </span><span class="s0">in </span><span class="s1">config</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">config</span><span class="s2">[</span><span class="s4">&quot;learning_rate&quot;</span><span class="s2">], </span><span class="s1">dict</span><span class="s2">):</span>
                <span class="s1">config</span><span class="s2">[</span><span class="s4">&quot;learning_rate&quot;</span><span class="s2">] = (</span>
                    <span class="s1">serialization_lib</span><span class="s2">.</span><span class="s1">deserialize_keras_object</span><span class="s2">(</span>
                        <span class="s1">config</span><span class="s2">[</span><span class="s4">&quot;learning_rate&quot;</span><span class="s2">], </span><span class="s1">custom_objects</span><span class="s2">=</span><span class="s1">custom_objects</span>
                    <span class="s2">)</span>
                <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">cls</span><span class="s2">(**</span><span class="s1">config</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">__setattr__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">value</span><span class="s2">):</span>
        <span class="s5"># Prevent users from attaching state to the</span>
        <span class="s5"># layer before `super()` is called -- since that</span>
        <span class="s5"># state would silently not be tracked.</span>
        <span class="s0">if </span><span class="s1">name </span><span class="s2">!= </span><span class="s4">&quot;_lock&quot;</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_check_super_called</span><span class="s2">()</span>
        <span class="s5"># Track Variables.</span>
        <span class="s0">if </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s4">&quot;_tracker&quot;</span><span class="s2">):</span>
            <span class="s1">value </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">track</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">super</span><span class="s2">().</span><span class="s1">__setattr__</span><span class="s2">(</span><span class="s1">name</span><span class="s2">, </span><span class="s1">value</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_clip_by_norm</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">values</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s5"># Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm</span>
        <span class="s1">l2sum </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">values</span><span class="s2">), </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
        <span class="s1">pred </span><span class="s2">= </span><span class="s1">l2sum </span><span class="s2">&gt; </span><span class="s3">0</span>
        <span class="s5"># Two-tap tf.where trick to bypass NaN gradients</span>
        <span class="s1">l2sum_safe </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span><span class="s1">pred</span><span class="s2">, </span><span class="s1">l2sum</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">ones_like</span><span class="s2">(</span><span class="s1">l2sum</span><span class="s2">))</span>
        <span class="s1">l2norm </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span><span class="s1">pred</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">l2sum_safe</span><span class="s2">), </span><span class="s1">l2sum</span><span class="s2">)</span>
        <span class="s1">intermediate </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span><span class="s1">values</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm</span><span class="s2">)</span>
        <span class="s1">values_clip </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">intermediate</span><span class="s2">) / </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span>
            <span class="s1">l2norm</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">clipnorm</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">values_clip</span>

    <span class="s0">def </span><span class="s1">_untrack_variable</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variable</span><span class="s2">):</span>
        <span class="s1">previous_lock_state </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">locked</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">unlock</span><span class="s2">()</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">untrack</span><span class="s2">(</span><span class="s1">variable</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">previous_lock_state </span><span class="s0">is True</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_tracker</span><span class="s2">.</span><span class="s1">lock</span><span class="s2">()</span>


<span class="s1">base_optimizer_keyword_args </span><span class="s2">= </span><span class="s4">&quot;&quot;&quot;name: String. The name to use 
            for momentum accumulator weights created by 
            the optimizer. 
        weight_decay: Float. If set, weight decay is applied. 
        clipnorm: Float. If set, the gradient of each weight is individually 
            clipped so that its norm is no higher than this value. 
        clipvalue: Float. If set, the gradient of each weight is clipped to be 
            no higher than this value. 
        global_clipnorm: Float. If set, the gradient of all weights is clipped 
            so that their global norm is no higher than this value. 
        use_ema: Boolean, defaults to `False`. 
            If `True`, exponential moving average 
            (EMA) is applied. EMA consists of computing an exponential moving 
            average of the weights of the model (as the weight values change 
            after each training batch), and periodically overwriting the 
            weights with their moving average. 
        ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`. 
            This is the momentum to use when computing 
            the EMA of the model's weights: 
            `new_average = ema_momentum * old_average + (1 - ema_momentum) * 
            current_variable_value`. 
        ema_overwrite_frequency: Int or None, defaults to None. Only used if 
            `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, 
            we overwrite the model variable by its moving average. 
            If None, the optimizer 
            does not overwrite model variables in the middle of training, 
            and you need to explicitly overwrite the variables 
            at the end of training by calling 
            `optimizer.finalize_variable_values()` (which updates the model 
            variables in-place). When using the built-in `fit()` training loop, 
            this happens automatically after the last epoch, 
            and you don't need to do anything. 
        loss_scale_factor: Float or `None`. If a float, the scale factor will 
            be multiplied the loss before computing gradients, and the inverse 
            of the scale factor will be multiplied by the gradients before 
            updating variables. Useful for preventing underflow during 
            mixed precision training. Alternately, 
            `keras.optimizers.LossScaleOptimizer` will 
            automatically set a loss scale factor. 
        gradient_accumulation_steps: Int or `None`. If an int, model &amp; optimizer 
            variables will not be updated at every step; instead they will be 
            updated every `gradient_accumulation_steps` steps, using the average 
            value of the gradients since the last update. This is known as 
            &quot;gradient accumulation&quot;. This can be useful 
            when your batch size is very small, in order to reduce gradient 
            noise at each update step. EMA frequency will look at &quot;accumulated&quot; 
            iterations value (optimizer steps // gradient_accumulation_steps). 
            Learning rate schedules will look at &quot;real&quot; iterations value 
            (optimizer steps). 
&quot;&quot;&quot;</span>


<span class="s0">def </span><span class="s1">global_norm</span><span class="s2">(</span><span class="s1">value_list</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Computes the global norm of multiple tensors.&quot;&quot;&quot;</span>
    <span class="s1">squared_norms </span><span class="s2">= [</span>
        <span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)) </span><span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">value_list </span><span class="s0">if </span><span class="s1">v </span><span class="s0">is not None</span>
    <span class="s2">]</span>
    <span class="s1">squared_norm </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">stack</span><span class="s2">(</span><span class="s1">squared_norms</span><span class="s2">))</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">squared_norm</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">clip_by_global_norm</span><span class="s2">(</span><span class="s1">value_list</span><span class="s2">, </span><span class="s1">clip_norm</span><span class="s2">):</span>
    <span class="s1">use_norm </span><span class="s2">= </span><span class="s1">global_norm</span><span class="s2">(</span><span class="s1">value_list</span><span class="s2">)</span>
    <span class="s5"># Calculate L2-norm, clip elements by ratio of clip_norm to L2-norm</span>
    <span class="s1">scale_for_finite </span><span class="s2">= </span><span class="s1">clip_norm </span><span class="s2">* </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">minimum</span><span class="s2">(</span><span class="s3">1.0 </span><span class="s2">/ </span><span class="s1">use_norm</span><span class="s2">, </span><span class="s3">1.0 </span><span class="s2">/ </span><span class="s1">clip_norm</span><span class="s2">)</span>
    <span class="s5"># If use_norm is any finite number, this is a no-op. For inf/-inf/NaN,</span>
    <span class="s5"># this will make scale NaN.</span>
    <span class="s1">scale </span><span class="s2">= </span><span class="s1">scale_for_finite </span><span class="s2">+ (</span><span class="s1">use_norm </span><span class="s2">- </span><span class="s1">use_norm</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s2">[</span><span class="s1">v </span><span class="s2">* </span><span class="s1">scale </span><span class="s0">if </span><span class="s1">v </span><span class="s0">is not None else </span><span class="s1">v </span><span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">value_list</span><span class="s2">]</span>
</pre>
</body>
</html>