<html>
<head>
<title>normalization.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
normalization.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">math</span>

<span class="s0">import </span><span class="s1">numpy </span><span class="s0">as </span><span class="s1">np</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">preprocessing</span><span class="s2">.</span><span class="s1">tf_data_layer </span><span class="s0">import </span><span class="s1">TFDataLayer</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">module_utils </span><span class="s0">import </span><span class="s1">tensorflow </span><span class="s0">as </span><span class="s1">tf</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.layers.Normalization&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">Normalization</span><span class="s2">(</span><span class="s1">TFDataLayer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;A preprocessing layer that normalizes continuous features. 
 
    This layer will shift and scale inputs into a distribution centered around 
    0 with standard deviation 1. It accomplishes this by precomputing the mean 
    and variance of the data, and calling `(input - mean) / sqrt(var)` at 
    runtime. 
 
    The mean and variance values for the layer must be either supplied on 
    construction or learned via `adapt()`. `adapt()` will compute the mean and 
    variance of the data and store them as the layer's weights. `adapt()` should 
    be called before `fit()`, `evaluate()`, or `predict()`. 
 
    Args: 
        axis: Integer, tuple of integers, or None. The axis or axes that should 
            have a separate mean and variance for each index in the shape. 
            For example, if shape is `(None, 5)` and `axis=1`, the layer will 
            track 5 separate mean and variance values for the last axis. 
            If `axis` is set to `None`, the layer will normalize 
            all elements in the input by a scalar mean and variance. 
            When `-1`, the last axis of the input is assumed to be a 
            feature dimension and is normalized per index. 
            Note that in the specific case of batched scalar inputs where 
            the only axis is the batch axis, the default will normalize 
            each index in the batch separately. 
            In this case, consider passing `axis=None`. Defaults to `-1`. 
        mean: The mean value(s) to use during normalization. The passed value(s) 
            will be broadcast to the shape of the kept axes above; 
            if the value(s) cannot be broadcast, an error will be raised when 
            this layer's `build()` method is called. 
        variance: The variance value(s) to use during normalization. The passed 
            value(s) will be broadcast to the shape of the kept axes above; 
            if the value(s) cannot be broadcast, an error will be raised when 
            this layer's `build()` method is called. 
        invert: If `True`, this layer will apply the inverse transformation 
            to its inputs: it would turn a normalized input back into its 
            original form. 
 
    Examples: 
 
    Calculate a global mean and variance by analyzing the dataset in `adapt()`. 
 
    &gt;&gt;&gt; adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32') 
    &gt;&gt;&gt; input_data = np.array([1., 2., 3.], dtype='float32') 
    &gt;&gt;&gt; layer = keras.layers.Normalization(axis=None) 
    &gt;&gt;&gt; layer.adapt(adapt_data) 
    &gt;&gt;&gt; layer(input_data) 
    array([-1.4142135, -0.70710677, 0.], dtype=float32) 
 
    Calculate a mean and variance for each index on the last axis. 
 
    &gt;&gt;&gt; adapt_data = np.array([[0., 7., 4.], 
    ...                        [2., 9., 6.], 
    ...                        [0., 7., 4.], 
    ...                        [2., 9., 6.]], dtype='float32') 
    &gt;&gt;&gt; input_data = np.array([[0., 7., 4.]], dtype='float32') 
    &gt;&gt;&gt; layer = keras.layers.Normalization(axis=-1) 
    &gt;&gt;&gt; layer.adapt(adapt_data) 
    &gt;&gt;&gt; layer(input_data) 
    array([-1., -1., -1.], dtype=float32) 
 
    Pass the mean and variance directly. 
 
    &gt;&gt;&gt; input_data = np.array([[1.], [2.], [3.]], dtype='float32') 
    &gt;&gt;&gt; layer = keras.layers.Normalization(mean=3., variance=2.) 
    &gt;&gt;&gt; layer(input_data) 
    array([[-1.4142135 ], 
           [-0.70710677], 
           [ 0.        ]], dtype=float32) 
 
    Use the layer to de-normalize inputs (after adapting the layer). 
 
    &gt;&gt;&gt; adapt_data = np.array([[0., 7., 4.], 
    ...                        [2., 9., 6.], 
    ...                        [0., 7., 4.], 
    ...                        [2., 9., 6.]], dtype='float32') 
    &gt;&gt;&gt; input_data = np.array([[1., 2., 3.]], dtype='float32') 
    &gt;&gt;&gt; layer = keras.layers.Normalization(axis=-1, invert=True) 
    &gt;&gt;&gt; layer.adapt(adapt_data) 
    &gt;&gt;&gt; layer(input_data) 
    array([2., 10., 8.], dtype=float32) 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">variance</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">invert</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, **</span><span class="s1">kwargs</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(**</span><span class="s1">kwargs</span><span class="s2">)</span>
        <span class="s6"># Standardize `axis` to a tuple.</span>
        <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">axis </span><span class="s2">= ()</span>
        <span class="s0">elif </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">int</span><span class="s2">):</span>
            <span class="s1">axis </span><span class="s2">= (</span><span class="s1">axis</span><span class="s2">,)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">axis </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">axis </span><span class="s2">= </span><span class="s1">axis</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">input_mean </span><span class="s2">= </span><span class="s1">mean</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">input_variance </span><span class="s2">= </span><span class="s1">variance</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">invert </span><span class="s2">= </span><span class="s1">invert</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">supports_masking </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mean </span><span class="s2">= </span><span class="s0">None</span>

        <span class="s6"># Set `mean` and `variance` if passed.</span>
        <span class="s0">if </span><span class="s2">(</span><span class="s1">mean </span><span class="s0">is not None</span><span class="s2">) != (</span><span class="s1">variance </span><span class="s0">is not None</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;When setting values directly, both `mean` and `variance` &quot;</span>
                <span class="s3">f&quot;must be set. Received: mean=</span><span class="s0">{</span><span class="s1">mean</span><span class="s0">} </span><span class="s3">and variance=</span><span class="s0">{</span><span class="s1">variance</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">input_shape </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s0">return</span>

        <span class="s1">ndim </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape </span><span class="s2">= </span><span class="s1">input_shape</span>

        <span class="s0">if </span><span class="s1">any</span><span class="s2">(</span><span class="s1">a </span><span class="s2">&lt; -</span><span class="s1">ndim </span><span class="s0">or </span><span class="s1">a </span><span class="s2">&gt;= </span><span class="s1">ndim </span><span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s2">):</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;All `axis` values must be in the range [-ndim, ndim). &quot;</span>
                <span class="s3">f&quot;Received inputs with ndim=</span><span class="s0">{</span><span class="s1">ndim</span><span class="s0">}</span><span class="s3">, while axis=</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>

        <span class="s6"># Axes to be kept, replacing negative values with positive equivalents.</span>
        <span class="s6"># Sorted to avoid transposing axes.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span>
            <span class="s1">sorted</span><span class="s2">([</span><span class="s1">d </span><span class="s0">if </span><span class="s1">d </span><span class="s2">&gt;= </span><span class="s5">0 </span><span class="s0">else </span><span class="s1">d </span><span class="s2">+ </span><span class="s1">ndim </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s2">])</span>
        <span class="s2">)</span>
        <span class="s6"># All axes to be kept should have known shape.</span>
        <span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">input_shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">] </span><span class="s0">is None</span><span class="s2">:</span>
                <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                    <span class="s3">&quot;All `axis` values to be kept must have a known shape. &quot;</span>
                    <span class="s3">f&quot;Received axis=</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s0">}</span><span class="s3">, &quot;</span>
                    <span class="s3">f&quot;inputs.shape=</span><span class="s0">{</span><span class="s1">input_shape</span><span class="s0">}</span><span class="s3">, &quot;</span>
                    <span class="s3">f&quot;with unknown axis at index </span><span class="s0">{</span><span class="s1">d</span><span class="s0">}</span><span class="s3">&quot;</span>
                <span class="s2">)</span>
        <span class="s6"># Axes to be reduced.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span>
            <span class="s1">d </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">ndim</span><span class="s2">) </span><span class="s0">if </span><span class="s1">d </span><span class="s0">not in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis</span>
        <span class="s2">)</span>
        <span class="s6"># 1 if an axis should be reduced, 0 otherwise.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis_mask </span><span class="s2">= [</span>
            <span class="s5">0 </span><span class="s0">if </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis </span><span class="s0">else </span><span class="s5">1 </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">ndim</span><span class="s2">)</span>
        <span class="s2">]</span>
        <span class="s6"># Broadcast any reduced axes.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_broadcast_shape </span><span class="s2">= [</span>
            <span class="s1">input_shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">] </span><span class="s0">if </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis </span><span class="s0">else </span><span class="s5">1 </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">ndim</span><span class="s2">)</span>
        <span class="s2">]</span>
        <span class="s1">mean_and_var_shape </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">] </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_mean_and_var_shape </span><span class="s2">= </span><span class="s1">mean_and_var_shape</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_mean </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_mean </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;mean&quot;</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=</span><span class="s1">mean_and_var_shape</span><span class="s2">,</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
            <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_variance </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;variance&quot;</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=</span><span class="s1">mean_and_var_shape</span><span class="s2">,</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s3">&quot;ones&quot;</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
            <span class="s2">)</span>
            <span class="s6"># For backwards compatibility with older saved models.</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">count </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_weight</span><span class="s2">(</span>
                <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;count&quot;</span><span class="s2">,</span>
                <span class="s1">shape</span><span class="s2">=(),</span>
                <span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;int&quot;</span><span class="s2">,</span>
                <span class="s1">initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
                <span class="s1">trainable</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
            <span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">finalize_state</span><span class="s2">()</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s6"># In the no adapt case, make constant tensors for mean and variance</span>
            <span class="s6"># with proper broadcast shape for use during call.</span>
            <span class="s1">mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_mean</span><span class="s2">)</span>
            <span class="s1">variance </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_variance</span><span class="s2">)</span>
            <span class="s1">mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_broadcast_shape</span><span class="s2">)</span>
            <span class="s1">variance </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_broadcast_shape</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">variance </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">adapt</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">data</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Computes the mean and variance of values in a dataset. 
 
        Calling `adapt()` on a `Normalization` layer is an alternative to 
        passing in `mean` and `variance` arguments during layer construction. A 
        `Normalization` layer should always either be adapted over a dataset or 
        passed `mean` and `variance`. 
 
        During `adapt()`, the layer will compute a `mean` and `variance` 
        separately for each position in each axis specified by the `axis` 
        argument. To calculate a single `mean` and `variance` over the input 
        data, simply pass `axis=None` to the layer. 
 
        Arg: 
            data: The data to train on. It can be passed either as a 
                `tf.data.Dataset`, as a NumPy array, or as a backend-native 
                eager tensor. 
                If a dataset, *it must be batched*. Keras will assume that the 
                data is batched, and if that assumption doesn't hold, the mean 
                and variance may be incorrectly computed. 
        &quot;&quot;&quot;</span>
        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ndarray</span><span class="s2">) </span><span class="s0">or </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">is_tensor</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
            <span class="s1">input_shape </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">shape</span>
        <span class="s0">elif </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">data</span><span class="s2">.</span><span class="s1">Dataset</span><span class="s2">):</span>
            <span class="s1">input_shape </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">data</span><span class="s2">.</span><span class="s1">element_spec</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
            <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">) == </span><span class="s5">1</span><span class="s2">:</span>
                <span class="s6"># Batch dataset if it isn't batched</span>
                <span class="s1">data </span><span class="s2">= </span><span class="s1">data</span><span class="s2">.</span><span class="s1">batch</span><span class="s2">(</span><span class="s5">128</span><span class="s2">)</span>
            <span class="s1">input_shape </span><span class="s2">= </span><span class="s1">tuple</span><span class="s2">(</span><span class="s1">data</span><span class="s2">.</span><span class="s1">element_spec</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>

        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_keep_axis</span><span class="s2">:</span>
                <span class="s0">if </span><span class="s1">input_shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">] != </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">]:</span>
                    <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                        <span class="s3">&quot;The layer was built with &quot;</span>
                        <span class="s3">f&quot;input_shape=</span><span class="s0">{</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape</span><span class="s0">}</span><span class="s3">, &quot;</span>
                        <span class="s3">&quot;but adapt() is being called with data with &quot;</span>
                        <span class="s3">f&quot;an incompatible shape, data.shape=</span><span class="s0">{</span><span class="s1">input_shape</span><span class="s0">}</span><span class="s3">&quot;</span>
                    <span class="s2">)</span>

        <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">np</span><span class="s2">.</span><span class="s1">ndarray</span><span class="s2">):</span>
            <span class="s1">total_mean </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
            <span class="s1">total_var </span><span class="s2">= </span><span class="s1">np</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">is_tensor</span><span class="s2">(</span><span class="s1">data</span><span class="s2">):</span>
            <span class="s1">total_mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
            <span class="s1">total_var </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">data</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">data</span><span class="s2">.</span><span class="s1">Dataset</span><span class="s2">):</span>
            <span class="s1">total_mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_mean_and_var_shape</span><span class="s2">)</span>
            <span class="s1">total_var </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_mean_and_var_shape</span><span class="s2">)</span>
            <span class="s1">total_count </span><span class="s2">= </span><span class="s5">0</span>
            <span class="s0">for </span><span class="s1">batch </span><span class="s0">in </span><span class="s1">data</span><span class="s2">:</span>
                <span class="s1">batch </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span>
                    <span class="s1">batch</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span>
                <span class="s2">)</span>
                <span class="s1">batch_mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">batch</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
                <span class="s1">batch_var </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">var</span><span class="s2">(</span><span class="s1">batch</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">)</span>
                <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span><span class="s2">:</span>
                    <span class="s1">batch_reduce_shape </span><span class="s2">= (</span>
                        <span class="s1">batch</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s1">d</span><span class="s2">] </span><span class="s0">for </span><span class="s1">d </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_reduce_axis</span>
                    <span class="s2">)</span>
                    <span class="s1">batch_count </span><span class="s2">= </span><span class="s1">math</span><span class="s2">.</span><span class="s1">prod</span><span class="s2">(</span><span class="s1">batch_reduce_shape</span><span class="s2">)</span>
                <span class="s0">else</span><span class="s2">:</span>
                    <span class="s1">batch_count </span><span class="s2">= </span><span class="s5">1</span>

                <span class="s1">total_count </span><span class="s2">+= </span><span class="s1">batch_count</span>
                <span class="s1">batch_weight </span><span class="s2">= </span><span class="s1">float</span><span class="s2">(</span><span class="s1">batch_count</span><span class="s2">) / </span><span class="s1">total_count</span>
                <span class="s1">existing_weight </span><span class="s2">= </span><span class="s5">1.0 </span><span class="s2">- </span><span class="s1">batch_weight</span>
                <span class="s1">new_total_mean </span><span class="s2">= (</span>
                    <span class="s1">total_mean </span><span class="s2">* </span><span class="s1">existing_weight </span><span class="s2">+ </span><span class="s1">batch_mean </span><span class="s2">* </span><span class="s1">batch_weight</span>
                <span class="s2">)</span>
                <span class="s6"># The variance is computed using the lack-of-fit sum of squares</span>
                <span class="s6"># formula (see</span>
                <span class="s6"># https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).</span>
                <span class="s1">total_var </span><span class="s2">= (</span>
                    <span class="s1">total_var </span><span class="s2">+ (</span><span class="s1">total_mean </span><span class="s2">- </span><span class="s1">new_total_mean</span><span class="s2">) ** </span><span class="s5">2</span>
                <span class="s2">) * </span><span class="s1">existing_weight </span><span class="s2">+ (</span>
                    <span class="s1">batch_var </span><span class="s2">+ (</span><span class="s1">batch_mean </span><span class="s2">- </span><span class="s1">new_total_mean</span><span class="s2">) ** </span><span class="s5">2</span>
                <span class="s2">) * </span><span class="s1">batch_weight</span>
                <span class="s1">total_mean </span><span class="s2">= </span><span class="s1">new_total_mean</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_mean</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">total_mean</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_variance</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">total_var</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">finalize_state</span><span class="s2">()</span>

    <span class="s0">def </span><span class="s1">finalize_state</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_mean </span><span class="s0">is not None or not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">built</span><span class="s2">:</span>
            <span class="s0">return</span>

        <span class="s6"># In the adapt case, we make constant tensors for mean and variance with</span>
        <span class="s6"># proper broadcast shape and dtype each time `finalize_state` is called.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_mean</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_broadcast_shape</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">mean </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">variance </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">adapt_variance</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_broadcast_shape</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">variance </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">):</span>
        <span class="s6"># This layer can be called in tf.data</span>
        <span class="s6"># even with another backend after it has been adapted.</span>
        <span class="s6"># However it must use backend-native logic for adapt().</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">mean </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s6"># May happen when in tf.data when mean/var was passed explicitly</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;You must call `.build(input_shape)` &quot;</span>
                <span class="s3">&quot;on the layer before using it.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">core</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">compute_dtype</span>
        <span class="s2">)</span>
        <span class="s6"># Enusre the weights are in the correct backend. Without this, it is</span>
        <span class="s6"># possible to cause breakage when using this layer in tf.data.</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">convert_weight</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">)</span>
        <span class="s1">variance </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">convert_weight</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">variance</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">invert</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">add</span><span class="s2">(</span>
                <span class="s1">mean</span><span class="s2">,</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span>
                    <span class="s1">inputs</span><span class="s2">,</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span>
                        <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">), </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">()</span>
                    <span class="s2">),</span>
                <span class="s2">),</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">),</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span>
                    <span class="s1">self</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">), </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">()</span>
                <span class="s2">),</span>
            <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_output_shape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">input_shape</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s1">config</span><span class="s2">.</span><span class="s1">update</span><span class="s2">(</span>
            <span class="s2">{</span>
                <span class="s3">&quot;axis&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">axis</span><span class="s2">,</span>
                <span class="s3">&quot;invert&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">invert</span><span class="s2">,</span>
                <span class="s3">&quot;mean&quot;</span><span class="s2">: </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_mean</span><span class="s2">).</span><span class="s1">tolist</span><span class="s2">(),</span>
                <span class="s3">&quot;variance&quot;</span><span class="s2">: </span><span class="s1">np</span><span class="s2">.</span><span class="s1">array</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">input_variance</span><span class="s2">).</span><span class="s1">tolist</span><span class="s2">(),</span>
            <span class="s2">}</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">config</span>

    <span class="s0">def </span><span class="s1">load_own_variables</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">store</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">load_own_variables</span><span class="s2">(</span><span class="s1">store</span><span class="s2">)</span>
        <span class="s6"># Ensure that we call finalize_state after variable loading.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">finalize_state</span><span class="s2">()</span>

    <span class="s0">def </span><span class="s1">get_build_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;input_shape&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_build_input_shape</span><span class="s2">}</span>

    <span class="s0">def </span><span class="s1">build_from_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">config</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">config</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">config</span><span class="s2">[</span><span class="s3">&quot;input_shape&quot;</span><span class="s2">])</span>
</pre>
</body>
</html>