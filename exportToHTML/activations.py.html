<html>
<head>
<title>activations.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #2aacb8;}
.s5 { color: #5f826b; font-style: italic;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
activations.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.relu&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">max_value</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">threshold</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Applies the rectified linear unit activation function. 
 
    With default values, this returns the standard ReLU activation: 
    `max(x, 0)`, the element-wise maximum of 0 and the input tensor. 
 
    Modifying default parameters allows you to use non-zero thresholds, 
    change the max value of the activation, 
    and to use a non-zero multiple of the input for values below the threshold. 
 
    Examples: 
 
    &gt;&gt;&gt; x = [-10, -5, 0.0, 5, 10] 
    &gt;&gt;&gt; keras.activations.relu(x) 
    [ 0.,  0.,  0.,  5., 10.] 
    &gt;&gt;&gt; keras.activations.relu(x, negative_slope=0.5) 
    [-5. , -2.5,  0. ,  5. , 10. ] 
    &gt;&gt;&gt; keras.activations.relu(x, max_value=5.) 
    [0., 0., 0., 5., 5.] 
    &gt;&gt;&gt; keras.activations.relu(x, threshold=5.) 
    [-0., -0.,  0.,  0., 10.] 
 
    Args: 
        x: Input tensor. 
        negative_slope: A `float` that controls the slope 
            for values lower than the threshold. 
        max_value: A `float` that sets the saturation threshold (the largest 
            value the function will return). 
        threshold: A `float` giving the threshold value of the activation 
            function below which values will be damped or set to zero. 
 
    Returns: 
        A tensor with the same shape and dtype as input `x`. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">any_symbolic_tensors</span><span class="s2">((</span><span class="s1">x</span><span class="s2">,)):</span>
        <span class="s0">return </span><span class="s1">ReLU</span><span class="s2">(</span>
            <span class="s1">negative_slope</span><span class="s2">=</span><span class="s1">negative_slope</span><span class="s2">,</span>
            <span class="s1">max_value</span><span class="s2">=</span><span class="s1">max_value</span><span class="s2">,</span>
            <span class="s1">threshold</span><span class="s2">=</span><span class="s1">threshold</span><span class="s2">,</span>
        <span class="s2">)(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">ReLU</span><span class="s2">.</span><span class="s1">static_call</span><span class="s2">(</span>
        <span class="s1">x</span><span class="s2">,</span>
        <span class="s1">negative_slope</span><span class="s2">=</span><span class="s1">negative_slope</span><span class="s2">,</span>
        <span class="s1">max_value</span><span class="s2">=</span><span class="s1">max_value</span><span class="s2">,</span>
        <span class="s1">threshold</span><span class="s2">=</span><span class="s1">threshold</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">class </span><span class="s1">ReLU</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">Operation</span><span class="s2">):</span>
    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">max_value</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">threshold</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s0">None</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">negative_slope </span><span class="s2">= </span><span class="s1">negative_slope</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">max_value </span><span class="s2">= </span><span class="s1">max_value</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">threshold </span><span class="s2">= </span><span class="s1">threshold</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">static_call</span><span class="s2">(</span>
            <span class="s1">x</span><span class="s2">,</span>
            <span class="s1">negative_slope</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">negative_slope</span><span class="s2">,</span>
            <span class="s1">max_value</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">max_value</span><span class="s2">,</span>
            <span class="s1">threshold</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">threshold</span><span class="s2">,</span>
        <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_output_spec</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">KerasTensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">staticmethod</span>
    <span class="s0">def </span><span class="s1">static_call</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">max_value</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">threshold</span><span class="s2">=</span><span class="s4">0.0</span><span class="s2">):</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">negative_slope </span><span class="s2">!= </span><span class="s4">0.0</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">max_value </span><span class="s0">is None and </span><span class="s1">threshold </span><span class="s2">== </span><span class="s4">0</span><span class="s2">:</span>
                <span class="s0">return </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">leaky_relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s1">negative_slope</span><span class="s2">)</span>

            <span class="s0">if </span><span class="s1">threshold </span><span class="s2">!= </span><span class="s4">0</span><span class="s2">:</span>
                <span class="s1">negative_part </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu</span><span class="s2">(-</span><span class="s1">x </span><span class="s2">+ </span><span class="s1">threshold</span><span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">negative_part </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu</span><span class="s2">(-</span><span class="s1">x</span><span class="s2">)</span>

        <span class="s1">clip_max </span><span class="s2">= </span><span class="s1">max_value </span><span class="s0">is not None</span>
        <span class="s0">if </span><span class="s1">threshold </span><span class="s2">!= </span><span class="s4">0</span><span class="s2">:</span>
            <span class="s6"># computes x for x &gt; threshold else 0</span>
            <span class="s1">threshold </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">threshold</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">x </span><span class="s2">* </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span>
                <span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">greater</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">threshold</span><span class="s2">), </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span>
            <span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">max_value </span><span class="s2">== </span><span class="s4">6</span><span class="s2">:</span>
            <span class="s6"># if no threshold, then can use nn.relu6 native op for performance</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
            <span class="s1">clip_max </span><span class="s2">= </span><span class="s0">False</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">clip_max</span><span class="s2">:</span>
            <span class="s1">min_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s4">0.0</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s1">max_value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">max_value</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">numpy</span><span class="s2">.</span><span class="s1">clip</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">min_value</span><span class="s2">, </span><span class="s1">max_value</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">negative_slope </span><span class="s2">!= </span><span class="s4">0.0</span><span class="s2">:</span>
            <span class="s1">x </span><span class="s2">-= </span><span class="s1">negative_slope </span><span class="s2">* </span><span class="s1">negative_part</span>
        <span class="s0">return </span><span class="s1">x</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.leaky_relu&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">leaky_relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s4">0.2</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Leaky relu activation function. 
 
    Args: 
        x: Input tensor. 
        negative_slope: A `float` that controls the slope 
            for values lower than the threshold. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">leaky_relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s1">negative_slope</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.relu6&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Relu6 activation function. 
 
    It's the ReLU function, but truncated to a maximum value of 6. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.softmax&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s4">1</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Softmax converts a vector of values to a probability distribution. 
 
    The elements of the output vector are in range `[0, 1]` and sum to 1. 
 
    Each input vector is handled independently. 
    The `axis` argument sets which axis of the input the function 
    is applied along. 
 
    Softmax is often used as the activation for the last 
    layer of a classification network because the result could be interpreted as 
    a probability distribution. 
 
    The softmax of each vector x is computed as 
    `exp(x) / sum(exp(x))`. 
 
    The input values in are the log-odds of the resulting probability. 
 
    Args: 
        x: Input tensor. 
        axis: Integer, axis along which the softmax is applied. 
    &quot;&quot;&quot;</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s6"># Cache the logits to use for crossentropy loss.</span>
    <span class="s0">try</span><span class="s2">:</span>
        <span class="s1">output</span><span class="s2">.</span><span class="s1">_keras_logits </span><span class="s2">= </span><span class="s1">x</span>
    <span class="s0">except </span><span class="s1">AttributeError</span><span class="s2">:</span>
        <span class="s6"># We're dealing with a C-type.</span>
        <span class="s0">pass</span>
    <span class="s0">return </span><span class="s1">output</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.elu&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">elu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s4">1.0</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Exponential Linear Unit. 
 
    The exponential linear unit (ELU) with `alpha &gt; 0` is define as: 
 
    - `x` if `x &gt; 0` 
    - alpha * `exp(x) - 1` if `x &lt; 0` 
 
    ELUs have negative values which pushes the mean of the activations 
    closer to zero. 
 
    Mean activations that are closer to zero enable faster learning as they 
    bring the gradient closer to the natural gradient. 
    ELUs saturate to a negative value when the argument gets smaller. 
    Saturation means a small derivative which decreases the variation 
    and the information that is propagated to the next layer. 
 
    Args: 
        x: Input tensor. 
 
    Reference: 
 
    - [Clevert et al., 2016](https://arxiv.org/abs/1511.07289) 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">elu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s1">alpha</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.selu&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">selu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Scaled Exponential Linear Unit (SELU). 
 
    The Scaled Exponential Linear Unit (SELU) activation function is defined as: 
 
    - `scale * x` if `x &gt; 0` 
    - `scale * alpha * (exp(x) - 1)` if `x &lt; 0` 
 
    where `alpha` and `scale` are pre-defined constants 
    (`alpha=1.67326324` and `scale=1.05070098`). 
 
    Basically, the SELU activation function multiplies `scale` (&gt; 1) with the 
    output of the `keras.activations.elu` function to ensure a slope larger 
    than one for positive inputs. 
 
    The values of `alpha` and `scale` are 
    chosen so that the mean and variance of the inputs are preserved 
    between two consecutive layers as long as the weights are initialized 
    correctly (see `keras.initializers.LecunNormal` initializer) 
    and the number of input units is &quot;large enough&quot; 
    (see reference paper for more information). 
 
    Args: 
        x: Input tensor. 
 
    Notes: 
 
    - To be used together with the 
        `keras.initializers.LecunNormal` initializer. 
    - To be used together with the dropout variant 
        `keras.layers.AlphaDropout` (rather than regular dropout). 
 
    Reference: 
 
    - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515) 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">selu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.softplus&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">softplus</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Softplus activation function. 
 
    It is defined as: `softplus(x) = log(exp(x) + 1)`. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">softplus</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.softsign&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">softsign</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Softsign activation function. 
 
    Softsign is defined as: `softsign(x) = x / (abs(x) + 1)`. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">softsign</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">([</span><span class="s3">&quot;keras.activations.silu&quot;</span><span class="s2">, </span><span class="s3">&quot;keras.activations.swish&quot;</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Swish (or Silu) activation function. 
 
    It is defined as: `swish(x) = x * sigmoid(x)`. 
 
    The Swish (or Silu) activation function is a smooth, 
    non-monotonic function that is unbounded above and 
    bounded below. 
 
    Args: 
        x: Input tensor. 
 
    Reference: 
 
    - [Ramachandran et al., 2017](https://arxiv.org/abs/1710.05941) 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.gelu&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">gelu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">approximate</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Gaussian error linear unit (GELU) activation function. 
 
    The Gaussian error linear unit (GELU) is defined as: 
 
    `gelu(x) = x * P(X &lt;= x)` where `P(X) ~ N(0, 1)`, 
    i.e. `gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))`. 
 
    GELU weights inputs by their value, rather than gating 
    inputs by their sign as in ReLU. 
 
    Args: 
        x: Input tensor. 
        approximate: A `bool`, whether to enable approximation. 
 
    Reference: 
 
    - [Hendrycks et al., 2016](https://arxiv.org/abs/1606.08415) 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">gelu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">approximate</span><span class="s2">=</span><span class="s1">approximate</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.tanh&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">tanh</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Hyperbolic tangent activation function. 
 
    It is defined as: 
    `tanh(x) = sinh(x) / cosh(x)`, i.e. 
    `tanh(x) = ((exp(x) - exp(-x)) / (exp(x) + exp(-x)))`. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">tanh</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.sigmoid&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Sigmoid activation function. 
 
    It is defined as: `sigmoid(x) = 1 / (1 + exp(-x))`. 
 
    For small values (&lt;-5), 
    `sigmoid` returns a value close to zero, and for large values (&gt;5) 
    the result of the function gets close to 1. 
 
    Sigmoid is equivalent to a 2-element softmax, where the second element is 
    assumed to be zero. The sigmoid function always returns a value between 
    0 and 1. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s6"># Cache the logits to use for crossentropy loss.</span>
    <span class="s0">try</span><span class="s2">:</span>
        <span class="s1">output</span><span class="s2">.</span><span class="s1">_keras_logits </span><span class="s2">= </span><span class="s1">x</span>
    <span class="s0">except </span><span class="s1">AttributeError</span><span class="s2">:</span>
        <span class="s6"># We're dealing with a C-type.</span>
        <span class="s0">pass</span>
    <span class="s0">return </span><span class="s1">output</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.exponential&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">exponential</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Exponential activation function. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">exp</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.hard_sigmoid&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">hard_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Hard sigmoid activation function. 
 
    The hard sigmoid activation is defined as: 
 
    - `0` if `if x &lt;= -3` 
    - `1` if `x &gt;= 3` 
    - `(x/6) + 0.5` if `-3 &lt; x &lt; 3` 
 
    It's a faster, piecewise linear approximation 
    of the sigmoid activation. 
 
    Args: 
        x: Input tensor. 
 
    Reference: 
 
    - [Wikipedia &quot;Hard sigmoid&quot;](https://en.wikipedia.org/wiki/Hard_sigmoid) 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">hard_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">([</span><span class="s3">&quot;keras.activations.hard_silu&quot;</span><span class="s2">, </span><span class="s3">&quot;keras.activations.hard_swish&quot;</span><span class="s2">])</span>
<span class="s0">def </span><span class="s1">hard_silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Hard SiLU activation function, also known as Hard Swish. 
 
    It is defined as: 
 
    - `0` if `if x &lt; -3` 
    - `x` if `x &gt; 3` 
    - `x * (x + 3) / 6` if `-3 &lt;= x &lt;= 3` 
 
    It's a faster, piecewise linear approximation of the silu activation. 
 
    Args: 
        x: Input tensor. 
 
    Reference: 
 
    - [A Howard, 2019](https://arxiv.org/abs/1905.02244) 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">hard_silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.linear&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">linear</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Linear activation function (pass-through). 
 
    A &quot;linear&quot; activation is an identity function: 
    it returns the input, unmodified. 
 
    Args: 
        x: Input tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">x</span>


<span class="s0">class </span><span class="s1">Mish</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">Operation</span><span class="s2">):</span>
    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">static_call</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_output_spec</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">KerasTensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">staticmethod</span>
    <span class="s0">def </span><span class="s1">static_call</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">x </span><span class="s2">* </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">tanh</span><span class="s2">(</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">softplus</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.mish&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">mish</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Mish activation function. 
 
    It is defined as: 
 
    `mish(x) = x * tanh(softplus(x))` 
 
    where `softplus` is defined as: 
 
    `softplus(x) = log(exp(x) + 1)` 
 
    Args: 
        x: Input tensor. 
 
    Reference: 
 
    - [Misra, 2019](https://arxiv.org/abs/1908.08681) 
    &quot;&quot;&quot;</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">Mish</span><span class="s2">.</span><span class="s1">static_call</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.activations.log_softmax&quot;</span><span class="s2">)</span>
<span class="s0">def </span><span class="s1">log_softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s4">1</span><span class="s2">):</span>
    <span class="s5">&quot;&quot;&quot;Log-Softmax activation function. 
 
    Each input vector is handled independently. 
    The `axis` argument sets which axis of the input the function 
    is applied along. 
 
    Args: 
        x: Input tensor. 
        axis: Integer, axis along which the softmax is applied. 
    &quot;&quot;&quot;</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">log_softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
</pre>
</body>
</html>