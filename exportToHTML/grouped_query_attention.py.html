<html>
<head>
<title>grouped_query_attention.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
grouped_query_attention.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">constraints</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">initializers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">regularizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">activations</span><span class="s2">.</span><span class="s1">softmax </span><span class="s0">import </span><span class="s1">Softmax</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">core</span><span class="s2">.</span><span class="s1">einsum_dense </span><span class="s0">import </span><span class="s1">EinsumDense</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">layer </span><span class="s0">import </span><span class="s1">Layer</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">regularization</span><span class="s2">.</span><span class="s1">dropout </span><span class="s0">import </span><span class="s1">Dropout</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.layers.GroupQueryAttention&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">GroupedQueryAttention</span><span class="s2">(</span><span class="s1">Layer</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Grouped Query Attention layer. 
 
    This is an implementation of grouped-query attention introduced by 
    [Ainslie et al., 2023](https://arxiv.org/abs/2305.13245). Here 
    `num_key_value_heads` denotes number of groups, setting 
    `num_key_value_heads` to 1 is equivalent to multi-query attention, and 
    when `num_key_value_heads` is equal to `num_query_heads` it is equivalent 
    to multi-head attention. 
 
    This layer first projects `query`, `key`, and `value` tensors. Then, `key` 
    and `value` are repeated to match the number of heads of `query`. 
 
    Then, the `query` is scaled and dot-producted with `key` tensors. These are 
    softmaxed to obtain attention probabilities. The value tensors are then 
    interpolated by these probabilities and concatenated back to a single 
    tensor. 
 
    Args: 
        head_dim: Size of each attention head. 
        num_query_heads: Number of query attention heads. 
        num_key_value_heads: Number of key and value attention heads. 
        dropout: Dropout probability. 
        use_bias: Boolean, whether the dense layers use bias vectors/matrices. 
        kernel_initializer: Initializer for dense layer kernels. 
        bias_initializer: Initializer for dense layer biases. 
        kernel_regularizer: Regularizer for dense layer kernels. 
        bias_regularizer: Regularizer for dense layer biases. 
        activity_regularizer: Regularizer for dense layer activity. 
        kernel_constraint: Constraint for dense layer kernels. 
        bias_constraint: Constraint for dense layer kernels. 
 
    Call arguments: 
        query: Query tensor of shape `(batch_dim, target_seq_len, feature_dim)`, 
            where `batch_dim` is batch size, `target_seq_len` is the length of 
            target sequence, and `feature_dim` is dimension of feature. 
        value: Value tensor of shape `(batch_dim, source_seq_len, feature_dim)`, 
            where `batch_dim` is batch size, `source_seq_len` is the length of 
            source sequence, and `feature_dim` is dimension of feature. 
        key: Optional key tensor of shape 
            `(batch_dim, source_seq_len, feature_dim)`. If not given, will use 
            `value` for both `key` and `value`, which is most common case. 
        attention_mask: A boolean mask of shape 
            `(batch_dim, target_seq_len, source_seq_len)`, that prevents 
            attention to certain positions. The boolean mask specifies which 
            query elements can attend to which key elements, where 1 indicates 
            attention and 0 indicates no attention. Broadcasting can happen for 
            the missing batch dimensions and the head dimension. 
        return_attention_scores: A boolean to indicate whether the output 
            should be `(attention_output, attention_scores)` if `True`, or 
            `attention_output` if `False`. Defaults to `False`. 
        training: Python boolean indicating whether the layer should behave in 
            training mode (adding dropout) or in inference mode (no dropout). 
            Will go with either using the training mode of the parent 
            layer/model or `False` (inference) if there is no parent layer. 
        use_causal_mask: A boolean to indicate whether to apply a causal mask to 
            prevent tokens from attending to future tokens (e.g., used in a 
            decoder Transformer). 
 
    Returns: 
        attention_output: Result of the computation, of shape 
            `(batch_dim, target_seq_len, feature_dim)`, where `target_seq_len` 
            is for target sequence length and `feature_dim` is the query input 
            last dim. 
        attention_scores: (Optional) attention coefficients of shape 
            `(batch_dim, num_query_heads, target_seq_len, source_seq_len)`. 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">head_dim</span><span class="s2">,</span>
        <span class="s1">num_query_heads</span><span class="s2">,</span>
        <span class="s1">num_key_value_heads</span><span class="s2">,</span>
        <span class="s1">dropout</span><span class="s2">=</span><span class="s5">0.0</span><span class="s2">,</span>
        <span class="s1">use_bias</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">kernel_initializer</span><span class="s2">=</span><span class="s3">&quot;glorot_uniform&quot;</span><span class="s2">,</span>
        <span class="s1">bias_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">kernel_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">kernel_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(**</span><span class="s1">kwargs</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">supports_masking </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim </span><span class="s2">= </span><span class="s1">head_dim</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_query_heads </span><span class="s2">= </span><span class="s1">num_query_heads</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_key_value_heads </span><span class="s2">= </span><span class="s1">num_key_value_heads</span>
        <span class="s0">if </span><span class="s1">num_query_heads </span><span class="s2">% </span><span class="s1">num_key_value_heads </span><span class="s2">!= </span><span class="s5">0</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;`num_query_heads` must be divisible&quot;</span>
                <span class="s3">&quot; by `num_key_value_heads`.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_repeats </span><span class="s2">= </span><span class="s1">num_query_heads </span><span class="s2">// </span><span class="s1">num_key_value_heads</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">dropout </span><span class="s2">= </span><span class="s1">dropout</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s2">= </span><span class="s1">use_bias</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_initializer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer </span><span class="s2">= </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_initializer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">activity_regularizer </span><span class="s2">= </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">activity_regularizer</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint </span><span class="s2">= </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">kernel_constraint</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint </span><span class="s2">= </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">get</span><span class="s2">(</span><span class="s1">bias_constraint</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">build</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">query_shape</span><span class="s2">,</span>
        <span class="s1">value_shape</span><span class="s2">,</span>
        <span class="s1">key_shape</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s6"># Einsum variables:</span>
        <span class="s6"># b = batch size</span>
        <span class="s6"># q = query length</span>
        <span class="s6"># k = key/value length</span>
        <span class="s6"># m = model dim</span>
        <span class="s6"># u = num query heads</span>
        <span class="s6"># v = num key/value heads</span>
        <span class="s6"># h = head dim</span>
        <span class="s1">key_shape </span><span class="s2">= </span><span class="s1">value_shape </span><span class="s0">if </span><span class="s1">key_shape </span><span class="s0">is None else </span><span class="s1">key_shape</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">feature_dim </span><span class="s2">= </span><span class="s1">query_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_query_dense </span><span class="s2">= </span><span class="s1">EinsumDense</span><span class="s2">(</span>
            <span class="s3">&quot;bqm,muh-&gt;bquh&quot;</span><span class="s2">,</span>
            <span class="s1">output_shape</span><span class="s2">=(</span><span class="s0">None</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_query_heads</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">),</span>
            <span class="s1">bias_axes</span><span class="s2">=</span><span class="s3">&quot;uh&quot; </span><span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s0">else None</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;query&quot;</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_common_kwargs_for_sublayer</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_query_dense</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">query_shape</span><span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">_key_dense </span><span class="s2">= </span><span class="s1">EinsumDense</span><span class="s2">(</span>
            <span class="s3">&quot;bkm,mvh-&gt;bkvh&quot;</span><span class="s2">,</span>
            <span class="s1">output_shape</span><span class="s2">=(</span><span class="s0">None</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_key_value_heads</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">),</span>
            <span class="s1">bias_axes</span><span class="s2">=</span><span class="s3">&quot;vh&quot; </span><span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s0">else None</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;key&quot;</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_common_kwargs_for_sublayer</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_key_dense</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">key_shape</span><span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">_value_dense </span><span class="s2">= </span><span class="s1">EinsumDense</span><span class="s2">(</span>
            <span class="s3">&quot;bkm,mvh-&gt;bkvh&quot;</span><span class="s2">,</span>
            <span class="s1">output_shape</span><span class="s2">=(</span><span class="s0">None</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_key_value_heads</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">),</span>
            <span class="s1">bias_axes</span><span class="s2">=</span><span class="s3">&quot;vh&quot; </span><span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s0">else None</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;value&quot;</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_common_kwargs_for_sublayer</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_value_dense</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span><span class="s1">value_shape</span><span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">_softmax </span><span class="s2">= </span><span class="s1">Softmax</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_dropout_layer </span><span class="s2">= </span><span class="s1">Dropout</span><span class="s2">(</span>
            <span class="s1">rate</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dropout</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span>
        <span class="s2">)</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">_dot_product_equation </span><span class="s2">= </span><span class="s3">&quot;bquh,bkuh-&gt;buqk&quot;</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_combine_equation </span><span class="s2">= </span><span class="s3">&quot;buqk,bkuh-&gt;bquh&quot;</span>

        <span class="s1">self</span><span class="s2">.</span><span class="s1">_output_dense </span><span class="s2">= </span><span class="s1">EinsumDense</span><span class="s2">(</span>
            <span class="s3">&quot;bquh,uhm-&gt;bqm&quot;</span><span class="s2">,</span>
            <span class="s1">output_shape</span><span class="s2">=(</span><span class="s0">None</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">feature_dim</span><span class="s2">),</span>
            <span class="s1">bias_axes</span><span class="s2">=</span><span class="s3">&quot;m&quot; </span><span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias </span><span class="s0">else None</span><span class="s2">,</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;attention_output&quot;</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_common_kwargs_for_sublayer</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_output_dense</span><span class="s2">.</span><span class="s1">build</span><span class="s2">(</span>
            <span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_query_heads</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">)</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">built </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">_get_common_kwargs_for_sublayer</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">common_kwargs </span><span class="s2">= </span><span class="s1">dict</span><span class="s2">(</span>
            <span class="s1">kernel_regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span><span class="s2">,</span>
            <span class="s1">bias_regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer</span><span class="s2">,</span>
            <span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">activity_regularizer</span><span class="s2">,</span>
            <span class="s1">kernel_constraint</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint</span><span class="s2">,</span>
            <span class="s1">bias_constraint</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint</span><span class="s2">,</span>
            <span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype_policy</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s6"># Create new clone of kernel/bias initializer, so that we don't reuse</span>
        <span class="s6"># the initializer instance, which could lead to same init value since</span>
        <span class="s6"># initializer is stateless.</span>
        <span class="s1">kernel_initializer </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">from_config</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer</span><span class="s2">.</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s2">)</span>
        <span class="s1">bias_initializer </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer</span><span class="s2">.</span><span class="s1">__class__</span><span class="s2">.</span><span class="s1">from_config</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer</span><span class="s2">.</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s2">)</span>
        <span class="s1">common_kwargs</span><span class="s2">[</span><span class="s3">&quot;kernel_initializer&quot;</span><span class="s2">] = </span><span class="s1">kernel_initializer</span>
        <span class="s1">common_kwargs</span><span class="s2">[</span><span class="s3">&quot;bias_initializer&quot;</span><span class="s2">] = </span><span class="s1">bias_initializer</span>
        <span class="s0">return </span><span class="s1">common_kwargs</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">query</span><span class="s2">,</span>
        <span class="s1">value</span><span class="s2">,</span>
        <span class="s1">key</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">query_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">value_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">key_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">attention_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">return_attention_scores</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
        <span class="s1">training</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_causal_mask</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s0">if </span><span class="s1">key </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">key </span><span class="s2">= </span><span class="s1">value</span>

        <span class="s1">attention_mask </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_compute_attention_mask</span><span class="s2">(</span>
            <span class="s1">query</span><span class="s2">,</span>
            <span class="s1">value</span><span class="s2">,</span>
            <span class="s1">query_mask</span><span class="s2">=</span><span class="s1">query_mask</span><span class="s2">,</span>
            <span class="s1">value_mask</span><span class="s2">=</span><span class="s1">value_mask</span><span class="s2">,</span>
            <span class="s1">key_mask</span><span class="s2">=</span><span class="s1">key_mask</span><span class="s2">,</span>
            <span class="s1">attention_mask</span><span class="s2">=</span><span class="s1">attention_mask</span><span class="s2">,</span>
            <span class="s1">use_causal_mask</span><span class="s2">=</span><span class="s1">use_causal_mask</span><span class="s2">,</span>
        <span class="s2">)</span>

        <span class="s1">query </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_query_dense</span><span class="s2">(</span><span class="s1">query</span><span class="s2">)</span>
        <span class="s1">key </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_key_dense</span><span class="s2">(</span><span class="s1">key</span><span class="s2">)</span>
        <span class="s1">value </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_value_dense</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)</span>

        <span class="s1">key </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">repeat</span><span class="s2">(</span>
            <span class="s1">key</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_repeats</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">2</span>
        <span class="s2">)  </span><span class="s6"># (batch_dim, source_seq_len, query_heads, head_dim)</span>
        <span class="s1">value </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">repeat</span><span class="s2">(</span>
            <span class="s1">value</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_repeats</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s5">2</span>
        <span class="s2">)  </span><span class="s6"># (batch_dim, source_seq_len, query_heads, head_dim)</span>

        <span class="s1">output</span><span class="s2">, </span><span class="s1">scores </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_compute_attention</span><span class="s2">(</span>
            <span class="s1">query</span><span class="s2">,</span>
            <span class="s1">key</span><span class="s2">,</span>
            <span class="s1">value</span><span class="s2">,</span>
            <span class="s1">attention_mask</span><span class="s2">=</span><span class="s1">attention_mask</span><span class="s2">,</span>
            <span class="s1">training</span><span class="s2">=</span><span class="s1">training</span><span class="s2">,</span>
        <span class="s2">)</span>

        <span class="s1">output </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_output_dense</span><span class="s2">(</span>
            <span class="s1">output</span>
        <span class="s2">)  </span><span class="s6"># (batch_dim, target_seq_len, feature_dim)</span>

        <span class="s0">if </span><span class="s1">return_attention_scores</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">output</span><span class="s2">, </span><span class="s1">scores</span>
        <span class="s0">return </span><span class="s1">output</span>

    <span class="s0">def </span><span class="s1">_compute_attention_mask</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">query</span><span class="s2">,</span>
        <span class="s1">value</span><span class="s2">,</span>
        <span class="s1">query_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">value_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">key_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">attention_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_causal_mask</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Computes the attention mask, using the Keras masks of the inputs. 
 
        * The `query`'s mask is reshaped from [B, T] to [B, T, 1]. 
        * The `value`'s mask is reshaped from [B, S] to [B, 1, S]. 
        * The `key`'s mask is reshaped from [B, S] to [B, 1, S]. The `key`'s 
          mask is ignored if `key` is `None` or if `key is value`. 
        * If `use_causal_mask=True`, then the causal mask is computed. Its shape 
          is [1, T, S]. 
 
        All defined masks are merged using a logical AND operation (`&amp;`). 
 
        In general, if the `query` and `value` are masked, then there is no need 
        to define the `attention_mask`. 
 
        Args: 
            query: Projected query tensor of shape `(B, T, N, key_dim)`. 
            key: Projected key tensor of shape `(B, T, N, key_dim)`. 
            value: Projected value tensor of shape `(B, T, N, value_dim)`. 
            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents 
                attention to certain positions. 
            use_causal_mask: A boolean to indicate whether to apply a causal 
                mask to prevent tokens from attending to future tokens (e.g., 
                used in a decoder Transformer). 
 
        Returns: 
            attention_mask: a boolean mask of shape `(B, T, S)`, that prevents 
                attention to certain positions, based on the Keras masks of the 
                `query`, `key`, `value`, and `attention_mask` tensors, and the 
                causal mask if `use_causal_mask=True`. 
        &quot;&quot;&quot;</span>
        <span class="s1">auto_mask </span><span class="s2">= </span><span class="s0">None</span>
        <span class="s0">if </span><span class="s1">query_mask </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">query_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">query_mask</span><span class="s2">, </span><span class="s3">&quot;bool&quot;</span><span class="s2">)  </span><span class="s6"># defensive casting</span>
            <span class="s6"># B = batch size, T = max query length</span>
            <span class="s1">auto_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">query_mask</span><span class="s2">, -</span><span class="s5">1</span><span class="s2">)  </span><span class="s6"># shape is [B, T, 1]</span>
        <span class="s0">if </span><span class="s1">value_mask </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">value_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">value_mask</span><span class="s2">, </span><span class="s3">&quot;bool&quot;</span><span class="s2">)  </span><span class="s6"># defensive casting</span>
            <span class="s6"># B = batch size, S == max value length</span>
            <span class="s1">mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">value_mask</span><span class="s2">, -</span><span class="s5">2</span><span class="s2">)  </span><span class="s6"># shape is [B, 1, S]</span>
            <span class="s1">auto_mask </span><span class="s2">= </span><span class="s1">mask </span><span class="s0">if </span><span class="s1">auto_mask </span><span class="s0">is None else </span><span class="s1">auto_mask </span><span class="s2">&amp; </span><span class="s1">mask</span>
        <span class="s0">if </span><span class="s1">key_mask </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">key_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">key_mask</span><span class="s2">, </span><span class="s3">&quot;bool&quot;</span><span class="s2">)  </span><span class="s6"># defensive casting</span>
            <span class="s6"># B == batch size, S == max key length == max value length</span>
            <span class="s1">mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">key_mask</span><span class="s2">, -</span><span class="s5">2</span><span class="s2">)  </span><span class="s6"># shape is [B, 1, S]</span>
            <span class="s1">auto_mask </span><span class="s2">= </span><span class="s1">mask </span><span class="s0">if </span><span class="s1">auto_mask </span><span class="s0">is None else </span><span class="s1">auto_mask </span><span class="s2">&amp; </span><span class="s1">mask</span>
        <span class="s0">if </span><span class="s1">use_causal_mask</span><span class="s2">:</span>
            <span class="s6"># the shape of the causal mask is [1, T, S]</span>
            <span class="s1">mask </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_compute_causal_mask</span><span class="s2">(</span><span class="s1">query</span><span class="s2">, </span><span class="s1">value</span><span class="s2">)</span>
            <span class="s1">auto_mask </span><span class="s2">= </span><span class="s1">mask </span><span class="s0">if </span><span class="s1">auto_mask </span><span class="s0">is None else </span><span class="s1">auto_mask </span><span class="s2">&amp; </span><span class="s1">mask</span>
        <span class="s0">if </span><span class="s1">auto_mask </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s6"># merge attention_mask &amp; automatic mask, to shape [B, T, S]</span>
            <span class="s1">attention_mask </span><span class="s2">= (</span>
                <span class="s1">auto_mask</span>
                <span class="s0">if </span><span class="s1">attention_mask </span><span class="s0">is None</span>
                <span class="s0">else </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">attention_mask</span><span class="s2">, </span><span class="s1">bool</span><span class="s2">) &amp; </span><span class="s1">auto_mask</span>
            <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">attention_mask</span>

    <span class="s0">def </span><span class="s1">_compute_causal_mask</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">query</span><span class="s2">, </span><span class="s1">value</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Computes a causal mask (e.g., for masked self-attention layers). 
 
        For example, if query and value both contain sequences of length 4, 
        this function returns a boolean tensor equal to: 
 
        ``` 
        [[[True,  False, False, False], 
          [True,  True,  False, False], 
          [True,  True,  True,  False], 
          [True,  True,  True,  True]]] 
        ``` 
 
        Args: 
            query: query tensor of shape `(B, T, ...)`. 
            value: value tensor of shape `(B, S, ...)` (optional, defaults to 
                query). 
 
        Returns: 
            mask: a boolean tensor of shape `(1, T, S)` containing a lower 
                triangular matrix of shape `(T, S)`. 
        &quot;&quot;&quot;</span>
        <span class="s1">q_seq_length </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">query</span><span class="s2">)[</span><span class="s5">1</span><span class="s2">]</span>
        <span class="s1">v_seq_length </span><span class="s2">= </span><span class="s1">q_seq_length </span><span class="s0">if </span><span class="s1">value </span><span class="s0">is None else </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">value</span><span class="s2">)[</span><span class="s5">1</span><span class="s2">]</span>
        <span class="s1">ones_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">ones</span><span class="s2">((</span><span class="s5">1</span><span class="s2">, </span><span class="s1">q_seq_length</span><span class="s2">, </span><span class="s1">v_seq_length</span><span class="s2">), </span><span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;int32&quot;</span><span class="s2">)</span>
        <span class="s1">row_index </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">(</span><span class="s1">ones_mask</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">2</span><span class="s2">)</span>
        <span class="s1">col_index </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cumsum</span><span class="s2">(</span><span class="s1">ones_mask</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s5">1</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">greater_equal</span><span class="s2">(</span><span class="s1">row_index</span><span class="s2">, </span><span class="s1">col_index</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_compute_attention</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">, </span><span class="s1">query</span><span class="s2">, </span><span class="s1">key</span><span class="s2">, </span><span class="s1">value</span><span class="s2">, </span><span class="s1">attention_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s0">None</span>
    <span class="s2">):</span>
        <span class="s1">query </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span>
            <span class="s1">query</span><span class="s2">,</span>
            <span class="s5">1.0 </span><span class="s2">/ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">, </span><span class="s1">query</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)),</span>
        <span class="s2">)</span>
        <span class="s6"># Take the dot product between &quot;query&quot; and &quot;key&quot; to get the raw</span>
        <span class="s6"># attention scores.</span>
        <span class="s1">scores </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">einsum</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_dot_product_equation</span><span class="s2">, </span><span class="s1">query</span><span class="s2">, </span><span class="s1">key</span>
        <span class="s2">)  </span><span class="s6"># (batch_dim, query_heads, target_seq_len, source_seq_len)</span>
        <span class="s1">scores </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_masked_softmax</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">, </span><span class="s1">attention_mask</span><span class="s2">=</span><span class="s1">attention_mask</span><span class="s2">)</span>
        <span class="s6"># This is actually dropping out entire tokens to attend to, which might</span>
        <span class="s6"># seem a bit unusual, but is taken from the original Transformer paper.</span>
        <span class="s1">scores_dropout </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_dropout_layer</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">, </span><span class="s1">training</span><span class="s2">=</span><span class="s1">training</span><span class="s2">)</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">einsum</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_combine_equation</span><span class="s2">, </span><span class="s1">scores_dropout</span><span class="s2">, </span><span class="s1">value</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">output</span><span class="s2">, </span><span class="s1">scores</span>

    <span class="s0">def </span><span class="s1">_masked_softmax</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">, </span><span class="s1">attention_mask</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s6"># Normalize the attention scores to probabilities.</span>
        <span class="s6"># scores = [B, N, T, S]</span>
        <span class="s0">if </span><span class="s1">attention_mask </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s6"># The expand dim happens starting from the `num_heads` dimension,</span>
            <span class="s6"># (&lt;batch_dims&gt;, num_heads, &lt;query_attention_dims,</span>
            <span class="s6"># key_attention_dims&gt;)</span>
            <span class="s1">mask_expansion_axis </span><span class="s2">= -</span><span class="s5">1 </span><span class="s2">* </span><span class="s5">2 </span><span class="s2">- </span><span class="s5">1</span>
            <span class="s0">for </span><span class="s1">_ </span><span class="s0">in </span><span class="s1">range</span><span class="s2">(</span><span class="s1">len</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s1">len</span><span class="s2">(</span><span class="s1">attention_mask</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)):</span>
                <span class="s1">attention_mask </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span>
                    <span class="s1">attention_mask</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">mask_expansion_axis</span>
                <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_softmax</span><span class="s2">(</span><span class="s1">scores</span><span class="s2">, </span><span class="s1">mask</span><span class="s2">=</span><span class="s1">attention_mask</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">compute_output_shape</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">query_shape</span><span class="s2">,</span>
        <span class="s1">value_shape</span><span class="s2">,</span>
        <span class="s1">key_shape</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s0">if </span><span class="s1">key_shape </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">key_shape </span><span class="s2">= </span><span class="s1">value_shape</span>

        <span class="s0">if </span><span class="s1">query_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">] != </span><span class="s1">value_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;The last dimension of `query_shape` and `value_shape` &quot;</span>
                <span class="s3">f&quot;must be equal, but are </span><span class="s0">{</span><span class="s1">query_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span><span class="s0">}</span><span class="s3">, </span><span class="s0">{</span><span class="s1">value_shape</span><span class="s2">[-</span><span class="s5">1</span><span class="s2">]</span><span class="s0">}</span><span class="s3">. &quot;</span>
                <span class="s3">&quot;Received: query_shape={query_shape}, value_shape={value_shape}&quot;</span>
            <span class="s2">)</span>

        <span class="s0">if </span><span class="s1">value_shape</span><span class="s2">[</span><span class="s5">1</span><span class="s2">:-</span><span class="s5">1</span><span class="s2">] != </span><span class="s1">key_shape</span><span class="s2">[</span><span class="s5">1</span><span class="s2">:-</span><span class="s5">1</span><span class="s2">]:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;All dimensions of `value` and `key`, except the last one, &quot;</span>
                <span class="s3">f&quot;must be equal. Received: value_shape=</span><span class="s0">{</span><span class="s1">value_shape</span><span class="s0">} </span><span class="s3">and &quot;</span>
                <span class="s3">f&quot;key_shape=</span><span class="s0">{</span><span class="s1">key_shape</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>

        <span class="s0">return </span><span class="s1">query_shape</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">config </span><span class="s2">= {</span>
            <span class="s3">&quot;head_dim&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">head_dim</span><span class="s2">,</span>
            <span class="s3">&quot;num_query_heads&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_query_heads</span><span class="s2">,</span>
            <span class="s3">&quot;num_key_value_heads&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_key_value_heads</span><span class="s2">,</span>
            <span class="s3">&quot;use_bias&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">,</span>
            <span class="s3">&quot;dropout&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dropout</span><span class="s2">,</span>
            <span class="s3">&quot;kernel_initializer&quot;</span><span class="s2">: </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_initializer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;bias_initializer&quot;</span><span class="s2">: </span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_initializer</span><span class="s2">),</span>
            <span class="s3">&quot;kernel_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_regularizer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;bias_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_regularizer</span><span class="s2">),</span>
            <span class="s3">&quot;activity_regularizer&quot;</span><span class="s2">: </span><span class="s1">regularizers</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span>
                <span class="s1">self</span><span class="s2">.</span><span class="s1">activity_regularizer</span>
            <span class="s2">),</span>
            <span class="s3">&quot;kernel_constraint&quot;</span><span class="s2">: </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_constraint</span><span class="s2">),</span>
            <span class="s3">&quot;bias_constraint&quot;</span><span class="s2">: </span><span class="s1">constraints</span><span class="s2">.</span><span class="s1">serialize</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias_constraint</span><span class="s2">),</span>
        <span class="s2">}</span>
        <span class="s1">base_config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s0">return </span><span class="s2">{**</span><span class="s1">base_config</span><span class="s2">, **</span><span class="s1">config</span><span class="s2">}</span>
</pre>
</body>
</html>