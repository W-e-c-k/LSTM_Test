<html>
<head>
<title>nn.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #2aacb8;}
.s4 { color: #7a7e85;}
.s5 { color: #6aab73;}
.s6 { color: #5f826b; font-style: italic;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
nn.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">math</span>
<span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">import </span><span class="s1">tensorflow </span><span class="s0">as </span><span class="s1">tf</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">backend</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">common</span><span class="s2">.</span><span class="s1">backend_utils </span><span class="s0">import </span><span class="s2">(</span>
    <span class="s1">compute_conv_transpose_output_shape</span><span class="s2">,</span>
<span class="s2">)</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">tensorflow</span><span class="s2">.</span><span class="s1">core </span><span class="s0">import </span><span class="s1">cast</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">tensorflow</span><span class="s2">.</span><span class="s1">core </span><span class="s0">import </span><span class="s1">convert_to_tensor</span>


<span class="s0">def </span><span class="s1">relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s1">logits </span><span class="s2">= </span><span class="s1">x</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s1">output</span><span class="s2">.</span><span class="s1">_keras_logits </span><span class="s2">= </span><span class="s1">logits</span>
    <span class="s0">return </span><span class="s1">output</span>


<span class="s0">def </span><span class="s1">tanh</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">tanh</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">softplus</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">softplus</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">softsign</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">softsign</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">log_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">log_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">leaky_relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">negative_slope</span><span class="s2">=</span><span class="s3">0.2</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">leaky_relu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s1">negative_slope</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">hard_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">relu6</span><span class="s2">(</span><span class="s1">x </span><span class="s2">+ </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">constant</span><span class="s2">(</span><span class="s3">3.0</span><span class="s2">, </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)) / </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">constant</span><span class="s2">(</span><span class="s3">6.0</span><span class="s2">, </span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">hard_silu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">x </span><span class="s2">* </span><span class="s1">hard_sigmoid</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">elu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">alpha</span><span class="s2">=</span><span class="s3">1.0</span><span class="s2">):</span>
    <span class="s1">res </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">elu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">alpha </span><span class="s2">== </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">res</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span><span class="s1">x </span><span class="s2">&gt; </span><span class="s3">0</span><span class="s2">, </span><span class="s1">res</span><span class="s2">, </span><span class="s1">alpha </span><span class="s2">* </span><span class="s1">res</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">selu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">selu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">gelu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">approximate</span><span class="s2">=</span><span class="s0">True</span><span class="s2">):</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">gelu</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">approximate</span><span class="s2">=</span><span class="s1">approximate</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">):</span>
    <span class="s1">logits </span><span class="s2">= </span><span class="s1">x</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s4"># Unlike numpy, tf will handle axis=None as axis=-1.</span>
        <span class="s4"># We need this workaround for the reduction on every dim.</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, [-</span><span class="s3">1</span><span class="s2">])</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">softmax</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">)</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">output</span><span class="s2">.</span><span class="s1">_keras_logits </span><span class="s2">= </span><span class="s1">logits</span>
    <span class="s0">return </span><span class="s1">output</span>


<span class="s0">def </span><span class="s1">log_softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">):</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s4"># Unlike numpy, tf will handle axis=None as axis=-1.</span>
        <span class="s4"># We need this workaround for the reduction on every dim.</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, [-</span><span class="s3">1</span><span class="s2">])</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">log_softmax</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">x</span><span class="s2">))</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">log_softmax</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_transpose_spatial_inputs</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">):</span>
    <span class="s1">num_spatial_dims </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s3">2</span>
    <span class="s4"># Tensorflow pooling does not support `channels_first` format, so</span>
    <span class="s4"># we need to transpose to `channels_last` format.</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">2</span><span class="s2">:</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">3</span><span class="s2">:</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Pooling inputs's shape must be 3, 4 or 5, corresponding to 1D, 2D &quot;</span>
            <span class="s5">f&quot;and 3D inputs. But received shape: </span><span class="s0">{</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">.&quot;</span>
        <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">inputs</span>


<span class="s0">def </span><span class="s1">_transpose_spatial_outputs</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">):</span>
    <span class="s4"># Undo the transpose in `_transpose_spatial_inputs`.</span>
    <span class="s1">num_spatial_dims </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s3">2</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">1</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">2</span><span class="s2">:</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">3</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">))</span>
    <span class="s0">elif </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">3</span><span class="s2">:</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, (</span><span class="s3">0</span><span class="s2">, </span><span class="s3">4</span><span class="s2">, </span><span class="s3">1</span><span class="s2">, </span><span class="s3">2</span><span class="s2">, </span><span class="s3">3</span><span class="s2">))</span>
    <span class="s0">return </span><span class="s1">outputs</span>


<span class="s0">def </span><span class="s1">max_pool</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">pool_size</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s1">strides </span><span class="s2">= </span><span class="s1">pool_size </span><span class="s0">if </span><span class="s1">strides </span><span class="s0">is None else </span><span class="s1">strides</span>
    <span class="s1">padding </span><span class="s2">= </span><span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">()</span>
    <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s5">&quot;channels_last&quot;</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot;</span><span class="s2">:</span>
        <span class="s4"># Tensorflow pooling does not support `channels_first` format, so</span>
        <span class="s4"># we need to transpose to `channels_last` format.</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">_transpose_spatial_inputs</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>

    <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">max_pool</span><span class="s2">(</span>
        <span class="s1">inputs</span><span class="s2">,</span>
        <span class="s1">pool_size</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">,</span>
        <span class="s1">tf_data_format</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot;</span><span class="s2">:</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">_transpose_spatial_outputs</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">outputs</span>


<span class="s0">def </span><span class="s1">average_pool</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">pool_size</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s1">strides </span><span class="s2">= </span><span class="s1">pool_size </span><span class="s0">if </span><span class="s1">strides </span><span class="s0">is None else </span><span class="s1">strides</span>
    <span class="s1">padding </span><span class="s2">= </span><span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">()</span>
    <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s5">&quot;channels_last&quot;</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot;</span><span class="s2">:</span>
        <span class="s4"># Tensorflow pooling does not support `channels_first` format, so</span>
        <span class="s4"># we need to transpose to `channels_last` format.</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">_transpose_spatial_inputs</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>

    <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">avg_pool</span><span class="s2">(</span>
        <span class="s1">inputs</span><span class="s2">,</span>
        <span class="s1">pool_size</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">,</span>
        <span class="s1">tf_data_format</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot;</span><span class="s2">:</span>
        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">_transpose_spatial_outputs</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">outputs</span>


<span class="s0">def </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">, </span><span class="s1">ndim</span><span class="s2">):</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">3</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NWC&quot;</span>
        <span class="s0">elif </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">4</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NHWC&quot;</span>
        <span class="s0">elif </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">5</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NDHWC&quot;</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s5">f&quot;Input rank not supported: </span><span class="s0">{</span><span class="s1">ndim</span><span class="s0">}</span><span class="s5">. &quot;</span>
                <span class="s5">&quot;Expected values are [3, 4, 5]&quot;</span>
            <span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot;</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">3</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NCW&quot;</span>
        <span class="s0">elif </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">4</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NCHW&quot;</span>
        <span class="s0">elif </span><span class="s1">ndim </span><span class="s2">== </span><span class="s3">5</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s5">&quot;NCDHW&quot;</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s5">f&quot;Input rank not supported: </span><span class="s0">{</span><span class="s1">ndim</span><span class="s0">}</span><span class="s5">. &quot;</span>
                <span class="s5">&quot;Expected values are [3, 4, 5]&quot;</span>
            <span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">f&quot;Invalid data_format: </span><span class="s0">{</span><span class="s1">data_format</span><span class="s0">}</span><span class="s5">. &quot;</span>
            <span class="s5">'Expected values are [&quot;channels_first&quot;, &quot;channels_last&quot;]'</span>
        <span class="s2">)</span>


<span class="s0">def </span><span class="s1">conv</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">kernel</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s0">def </span><span class="s1">_conv</span><span class="s2">():</span>
        <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">convolution</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">kernel</span><span class="s2">,</span>
            <span class="s1">strides</span><span class="s2">,</span>
            <span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">(),</span>
            <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
            <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
        <span class="s2">)</span>

    <span class="s4"># Certain ops are are broken in Tensorflow on CPU only.</span>
    <span class="s4"># We can work around by compiling the op with XLA.</span>
    <span class="s2">@</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">function</span><span class="s2">(</span><span class="s1">jit_compile</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
    <span class="s0">def </span><span class="s1">_conv_xla</span><span class="s2">():</span>
        <span class="s0">return </span><span class="s1">_conv</span><span class="s2">()</span>

    <span class="s4"># Channels first &quot;NCDHW&quot; (3d convolutions) are broken on CPU without XLA.</span>
    <span class="s1">needs_xla </span><span class="s2">= </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_first&quot; </span><span class="s0">and </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) == </span><span class="s3">5</span>
    <span class="s4"># grouped convolutions are broken on CPU without XLA.</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
        <span class="s1">channels </span><span class="s2">= </span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s3">1</span><span class="s2">]</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">channels </span><span class="s2">= </span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]</span>
    <span class="s1">needs_xla </span><span class="s2">= </span><span class="s1">needs_xla </span><span class="s0">or </span><span class="s1">channels </span><span class="s2">!= </span><span class="s1">kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s3">2</span><span class="s2">]</span>
    <span class="s0">if </span><span class="s1">needs_xla</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">_conv_xla</span><span class="s2">()</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">_conv</span><span class="s2">()</span>


<span class="s0">def </span><span class="s1">depthwise_conv</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">kernel</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s1">num_spatial_dims </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s3">2</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">&gt; </span><span class="s3">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;`inputs` rank must be 3 (1D conv) or 4 (2D conv). Received: &quot;</span>
            <span class="s5">&quot;{inputs.ndim}.&quot;</span>
        <span class="s2">)</span>
    <span class="s4"># Because we use `tf.nn.depthwise_conv2d` for both 1D and 2D convs, we set</span>
    <span class="s4"># `tf_data_format` using 2D conv format.</span>
    <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">, </span><span class="s3">4</span><span class="s2">)</span>
    <span class="s1">padding </span><span class="s2">= </span><span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">()</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">strides</span><span class="s2">, </span><span class="s1">int</span><span class="s2">):</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s1">strides</span><span class="s2">,) * </span><span class="s1">num_spatial_dims</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">dilation_rate</span><span class="s2">, </span><span class="s1">int</span><span class="s2">):</span>
        <span class="s1">dilation_rate </span><span class="s2">= (</span><span class="s1">dilation_rate</span><span class="s2">,) * </span><span class="s1">num_spatial_dims</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s4"># 1D depthwise conv.</span>
        <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
            <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">strides </span><span class="s2">* </span><span class="s3">2 </span><span class="s2">+ (</span><span class="s3">1</span><span class="s2">,)</span>
            <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">1</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">) + </span><span class="s1">strides </span><span class="s2">* </span><span class="s3">2</span>
            <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">2</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">spatial_start_dim</span><span class="s2">)</span>
        <span class="s1">kernel </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">kernel</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>

        <span class="s1">dilation_rate </span><span class="s2">= </span><span class="s0">None if </span><span class="s1">dilation_rate </span><span class="s0">is None else </span><span class="s2">(</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">dilation_rate</span>

        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">depthwise_conv2d</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">kernel</span><span class="s2">,</span>
            <span class="s1">strides</span><span class="s2">,</span>
            <span class="s1">padding</span><span class="s2">,</span>
            <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
            <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, [</span><span class="s1">spatial_start_dim</span><span class="s2">])</span>

    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">strides </span><span class="s2">+ (</span><span class="s3">1</span><span class="s2">,)</span>
        <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">1</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">) + </span><span class="s1">strides</span>
        <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">2</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">depthwise_conv2d</span><span class="s2">(</span>
        <span class="s1">inputs</span><span class="s2">,</span>
        <span class="s1">kernel</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">,</span>
        <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
        <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">separable_conv</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">depthwise_kernel</span><span class="s2">,</span>
    <span class="s1">pointwise_kernel</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s1">num_spatial_dims </span><span class="s2">= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s3">2</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">&gt; </span><span class="s3">2</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;`num_spatial_dims` must be 1 or 2. Received: &quot;</span>
            <span class="s5">f&quot;num_spatial_dims=</span><span class="s0">{</span><span class="s1">num_spatial_dims</span><span class="s0">}</span><span class="s5">.&quot;</span>
        <span class="s2">)</span>
    <span class="s4"># Because we use `tf.nn.separable_conv2d` for both 1D and 2D convs, we set</span>
    <span class="s4"># `tf_data_format` using 2D conv format.</span>
    <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">, </span><span class="s3">4</span><span class="s2">)</span>
    <span class="s1">padding </span><span class="s2">= </span><span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">()</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">strides</span><span class="s2">, </span><span class="s1">int</span><span class="s2">):</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s1">strides</span><span class="s2">,) * </span><span class="s1">num_spatial_dims</span>
    <span class="s0">if </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">dilation_rate</span><span class="s2">, </span><span class="s1">int</span><span class="s2">):</span>
        <span class="s1">dilation_rate </span><span class="s2">= (</span><span class="s1">dilation_rate</span><span class="s2">,) * </span><span class="s1">num_spatial_dims</span>
    <span class="s0">if </span><span class="s1">num_spatial_dims </span><span class="s2">== </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s4"># 1D depthwise conv.</span>
        <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
            <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">strides </span><span class="s2">* </span><span class="s3">2 </span><span class="s2">+ (</span><span class="s3">1</span><span class="s2">,)</span>
            <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">1</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">) + </span><span class="s1">strides </span><span class="s2">* </span><span class="s3">2</span>
            <span class="s1">spatial_start_dim </span><span class="s2">= </span><span class="s3">2</span>
        <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">spatial_start_dim</span><span class="s2">)</span>
        <span class="s1">depthwise_kernel </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">depthwise_kernel</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
        <span class="s1">pointwise_kernel </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">pointwise_kernel</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
        <span class="s1">dilation_rate </span><span class="s2">= </span><span class="s0">None if </span><span class="s1">dilation_rate </span><span class="s0">is None else </span><span class="s2">(</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">dilation_rate</span>

        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">separable_conv2d</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">depthwise_kernel</span><span class="s2">,</span>
            <span class="s1">pointwise_kernel</span><span class="s2">,</span>
            <span class="s1">strides</span><span class="s2">,</span>
            <span class="s1">padding</span><span class="s2">,</span>
            <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
            <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, [</span><span class="s1">spatial_start_dim</span><span class="s2">])</span>

    <span class="s0">if </span><span class="s1">data_format </span><span class="s2">== </span><span class="s5">&quot;channels_last&quot;</span><span class="s2">:</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">,) + </span><span class="s1">strides </span><span class="s2">+ (</span><span class="s3">1</span><span class="s2">,)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">strides </span><span class="s2">= (</span><span class="s3">1</span><span class="s2">, </span><span class="s3">1</span><span class="s2">) + </span><span class="s1">strides</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">separable_conv2d</span><span class="s2">(</span>
        <span class="s1">inputs</span><span class="s2">,</span>
        <span class="s1">depthwise_kernel</span><span class="s2">,</span>
        <span class="s1">pointwise_kernel</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">,</span>
        <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
        <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">conv_transpose</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">kernel</span><span class="s2">,</span>
    <span class="s1">strides</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">padding</span><span class="s2">=</span><span class="s5">&quot;valid&quot;</span><span class="s2">,</span>
    <span class="s1">output_padding</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">data_format </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">)</span>
    <span class="s1">tf_data_format </span><span class="s2">= </span><span class="s1">_convert_data_format</span><span class="s2">(</span><span class="s1">data_format</span><span class="s2">, </span><span class="s1">len</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">))</span>
    <span class="s1">kernel_size </span><span class="s2">= </span><span class="s1">kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s3">2</span><span class="s2">]</span>
    <span class="s1">filters </span><span class="s2">= </span><span class="s1">kernel</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s3">2</span><span class="s2">]</span>
    <span class="s1">input_shape </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
    <span class="s1">symbolic_shape </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>
    <span class="s0">for </span><span class="s1">i</span><span class="s2">, </span><span class="s1">e </span><span class="s0">in </span><span class="s1">enumerate</span><span class="s2">(</span><span class="s1">input_shape</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">e </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">input_shape</span><span class="s2">[</span><span class="s1">i</span><span class="s2">] = </span><span class="s1">symbolic_shape</span><span class="s2">[</span><span class="s1">i</span><span class="s2">]</span>
    <span class="s1">output_shape </span><span class="s2">= </span><span class="s1">compute_conv_transpose_output_shape</span><span class="s2">(</span>
        <span class="s1">input_shape</span><span class="s2">,</span>
        <span class="s1">kernel_size</span><span class="s2">,</span>
        <span class="s1">filters</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">,</span>
        <span class="s1">output_padding</span><span class="s2">,</span>
        <span class="s1">data_format</span><span class="s2">,</span>
        <span class="s1">dilation_rate</span><span class="s2">,</span>
    <span class="s2">)</span>

    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">conv_transpose</span><span class="s2">(</span>
        <span class="s1">inputs</span><span class="s2">,</span>
        <span class="s1">kernel</span><span class="s2">,</span>
        <span class="s1">output_shape</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">=</span><span class="s1">padding</span><span class="s2">.</span><span class="s1">upper</span><span class="s2">(),</span>
        <span class="s1">data_format</span><span class="s2">=</span><span class="s1">tf_data_format</span><span class="s2">,</span>
        <span class="s1">dilations</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">one_hot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;float32&quot;</span><span class="s2">, </span><span class="s1">sparse</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s1">x </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;int64&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">dtype </span><span class="s0">is None</span><span class="s2">:</span>
        <span class="s1">dtype </span><span class="s2">= </span><span class="s5">&quot;float32&quot;</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">dtype </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_dtype</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">sparse</span><span class="s2">:</span>
        <span class="s4"># We don't use `tf.sparse.bincount`, it doesn't handle negative indices</span>
        <span class="s4"># and only support rank 1 and 2 tensors (`one_hot` adds a dimension).</span>
        <span class="s0">if </span><span class="s1">axis </span><span class="s2">&lt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s1">axis </span><span class="s2">= </span><span class="s1">axis </span><span class="s2">+ </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) + </span><span class="s3">1</span>
        <span class="s1">values_count </span><span class="s2">= </span><span class="s1">math</span><span class="s2">.</span><span class="s1">prod</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s1">values </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, (</span><span class="s1">values_count</span><span class="s2">,))</span>
        <span class="s4"># We deal with negative inputs by having zeros in the output although</span>
        <span class="s4"># it's useless. It makes shapes static.</span>
        <span class="s1">values </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">greater_equal</span><span class="s2">(</span><span class="s1">values</span><span class="s2">, </span><span class="s3">0</span><span class="s2">), </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">indices </span><span class="s2">= [</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">range</span><span class="s2">(</span><span class="s1">dim</span><span class="s2">) </span><span class="s0">for </span><span class="s1">dim </span><span class="s0">in </span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">]</span>
        <span class="s1">indices </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">meshgrid</span><span class="s2">(*</span><span class="s1">indices</span><span class="s2">, </span><span class="s1">indexing</span><span class="s2">=</span><span class="s5">&quot;ij&quot;</span><span class="s2">)</span>
        <span class="s1">indices</span><span class="s2">.</span><span class="s1">insert</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s3">0</span><span class="s2">))  </span><span class="s4"># Deal with negative indices</span>
        <span class="s1">indices </span><span class="s2">= [</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">a</span><span class="s2">, (</span><span class="s1">values_count</span><span class="s2">, </span><span class="s3">1</span><span class="s2">)) </span><span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">indices</span><span class="s2">]</span>
        <span class="s1">indices </span><span class="s2">= [</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">a</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">int64</span><span class="s2">) </span><span class="s0">for </span><span class="s1">a </span><span class="s0">in </span><span class="s1">indices</span><span class="s2">]</span>
        <span class="s1">indices </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">concat</span><span class="s2">(</span><span class="s1">indices</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">1</span><span class="s2">)</span>
        <span class="s1">shape </span><span class="s2">= </span><span class="s1">list</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s1">shape</span><span class="s2">.</span><span class="s1">insert</span><span class="s2">(</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">SparseTensor</span><span class="s2">(</span><span class="s1">indices</span><span class="s2">, </span><span class="s1">values</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>
    <span class="s1">on_value</span><span class="s2">, </span><span class="s1">off_value </span><span class="s2">= (</span><span class="s0">True</span><span class="s2">, </span><span class="s0">False</span><span class="s2">) </span><span class="s0">if </span><span class="s1">dtype </span><span class="s2">== </span><span class="s5">&quot;bool&quot; </span><span class="s0">else </span><span class="s2">(</span><span class="s0">None</span><span class="s2">, </span><span class="s0">None</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">one_hot</span><span class="s2">(</span>
        <span class="s1">x</span><span class="s2">,</span>
        <span class="s1">num_classes</span><span class="s2">,</span>
        <span class="s1">on_value</span><span class="s2">=</span><span class="s1">on_value</span><span class="s2">,</span>
        <span class="s1">off_value</span><span class="s2">=</span><span class="s1">off_value</span><span class="s2">,</span>
        <span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">multi_hot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;float32&quot;</span><span class="s2">, </span><span class="s1">sparse</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s1">reduction_axis </span><span class="s2">= </span><span class="s3">1 </span><span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) &gt; </span><span class="s3">1 </span><span class="s0">else </span><span class="s3">0</span>
    <span class="s0">if </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_dtype</span><span class="s2">(</span><span class="s1">dtype</span><span class="s2">) == </span><span class="s5">&quot;bool&quot;</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">sparse</span><span class="s2">:</span>
            <span class="s4"># `tf.sparse.reduce_max` doesn't work on bool and there is no</span>
            <span class="s4"># `tf.sparse.reduce_any`.</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">one_hot</span><span class="s2">(</span>
                <span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;int8&quot;</span><span class="s2">, </span><span class="s1">sparse</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">sparse</span><span class="s2">.</span><span class="s1">reduce_max</span><span class="s2">(</span>
                <span class="s1">outputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">reduction_axis</span><span class="s2">, </span><span class="s1">output_is_sparse</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
            <span class="s1">outputs_shape </span><span class="s2">= </span><span class="s1">outputs</span><span class="s2">.</span><span class="s1">shape</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s1">outputs</span><span class="s2">.</span><span class="s1">set_shape</span><span class="s2">(</span><span class="s1">outputs_shape</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">outputs</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">one_hot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_any</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">reduction_axis</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">sparse</span><span class="s2">:</span>
            <span class="s4"># We don't use `tf.sparse.bincount`, it doesn't handle negative</span>
            <span class="s4"># indices and has a rank limitation.</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">one_hot</span><span class="s2">(</span>
                <span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">sparse</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
            <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">sparse</span><span class="s2">.</span><span class="s1">reduce_max</span><span class="s2">(</span>
                <span class="s1">outputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">reduction_axis</span><span class="s2">, </span><span class="s1">output_is_sparse</span><span class="s2">=</span><span class="s0">True</span>
            <span class="s2">)</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">outputs </span><span class="s2">= </span><span class="s1">one_hot</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">num_classes</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
            <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_max</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">reduction_axis</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">_get_logits</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">, </span><span class="s1">op_type</span><span class="s2">, </span><span class="s1">fn_name</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Retrieves logits tensor from maybe-softmax or maybe-sigmoid tensor.&quot;&quot;&quot;</span>
    <span class="s1">output_ </span><span class="s2">= </span><span class="s1">output</span>
    <span class="s1">from_logits_ </span><span class="s2">= </span><span class="s1">from_logits</span>

    <span class="s1">has_keras_logits </span><span class="s2">= </span><span class="s1">hasattr</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s5">&quot;_keras_logits&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">has_keras_logits</span><span class="s2">:</span>
        <span class="s1">output_ </span><span class="s2">= </span><span class="s1">output</span><span class="s2">.</span><span class="s1">_keras_logits</span>
        <span class="s1">from_logits_ </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s1">from_expected_op_type </span><span class="s2">= (</span>
        <span class="s1">hasattr</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s5">&quot;op&quot;</span><span class="s2">)</span>
        <span class="s0">and not </span><span class="s1">isinstance</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, (</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">__internal__</span><span class="s2">.</span><span class="s1">EagerTensor</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">Variable</span><span class="s2">))</span>
        <span class="s0">and </span><span class="s1">output</span><span class="s2">.</span><span class="s1">op</span><span class="s2">.</span><span class="s1">type </span><span class="s2">== </span><span class="s1">op_type</span>
    <span class="s2">) </span><span class="s0">and not </span><span class="s1">has_keras_logits</span>

    <span class="s0">if </span><span class="s1">from_expected_op_type</span><span class="s2">:</span>
        <span class="s4"># When softmax activation function is used for output operation, we</span>
        <span class="s4"># use logits from the softmax function directly to compute loss in order</span>
        <span class="s4"># to prevent collapsing zero when training.</span>
        <span class="s0">assert </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">op</span><span class="s2">.</span><span class="s1">inputs</span><span class="s2">) == </span><span class="s3">1</span>
        <span class="s1">output_ </span><span class="s2">= </span><span class="s1">output</span><span class="s2">.</span><span class="s1">op</span><span class="s2">.</span><span class="s1">inputs</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
        <span class="s1">from_logits_ </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">if </span><span class="s1">from_logits </span><span class="s0">and </span><span class="s2">(</span><span class="s1">has_keras_logits </span><span class="s0">or </span><span class="s1">from_expected_op_type</span><span class="s2">):</span>
        <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
            <span class="s5">f'&quot;`</span><span class="s0">{</span><span class="s1">fn_name</span><span class="s0">}</span><span class="s5">` received `from_logits=True`, but '</span>
            <span class="s5">f&quot;the `output` argument was produced by a </span><span class="s0">{</span><span class="s1">op_type</span><span class="s0">} </span><span class="s5">&quot;</span>
            <span class="s5">&quot;activation and thus does not represent logits. &quot;</span>
            <span class="s5">&quot;Was this intended?&quot;</span><span class="s2">,</span>
            <span class="s1">stacklevel</span><span class="s2">=</span><span class="s3">2</span><span class="s2">,</span>
        <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">output_</span><span class="s2">, </span><span class="s1">from_logits_</span>


<span class="s0">def </span><span class="s1">categorical_crossentropy</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Categorical crossentropy between an output tensor and a target tensor. 
 
    Args: 
        target: A tensor of the same shape as `output`. 
        output: A tensor resulting from a softmax 
            (unless `from_logits` is `True`, in which 
            case `output` is expected to be the logits). 
        from_logits: Boolean, whether `output` is the 
            result of a softmax, or is a tensor of logits. 
        axis: Int specifying the channels axis. `axis=-1` corresponds to data 
            format `channels_last`, and `axis=1` corresponds to data format 
            `channels_first`. 
 
    Returns: 
        Output tensor. 
 
    Example: 
 
    &gt;&gt;&gt; a = tf.constant([1., 0., 0., 0., 1., 0., 0., 0., 1.], shape=[3,3]) 
    &gt;&gt;&gt; print(a) 
    tf.Tensor( 
      [[1. 0. 0.] 
       [0. 1. 0.] 
       [0. 0. 1.]], shape=(3, 3), dtype=float32) 
    &gt;&gt;&gt; b = tf.constant([.9, .05, .05, .05, .89, .06, .05, .01, .94], 
    ...                 shape=[3, 3]) 
    &gt;&gt;&gt; print(b) 
    tf.Tensor( 
      [[0.9  0.05 0.05] 
       [0.05 0.89 0.06] 
       [0.05 0.01 0.94]], shape=(3, 3), dtype=float32) 
    &gt;&gt;&gt; loss = categorical_crossentropy(a, b) 
    &gt;&gt;&gt; print(np.around(loss, 5)) 
    [0.10536 0.11653 0.06188] 
    &gt;&gt;&gt; loss = categorical_crossentropy(a, a) 
    &gt;&gt;&gt; print(np.around(loss, 5)) 
    [0. 0. 0.] 
    &quot;&quot;&quot;</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">target</span><span class="s2">)</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) &lt; </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Arguments `target` and `output` must be at least rank 1. &quot;</span>
            <span class="s5">&quot;Received: &quot;</span>
            <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Arguments `target` and `output` must have the same rank &quot;</span>
            <span class="s5">&quot;(ndim). Received: &quot;</span>
            <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s0">for </span><span class="s1">e1</span><span class="s2">, </span><span class="s1">e2 </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">e1 </span><span class="s0">is not None and </span><span class="s1">e2 </span><span class="s0">is not None and </span><span class="s1">e1 </span><span class="s2">!= </span><span class="s1">e2</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s5">&quot;Arguments `target` and `output` must have the same shape. &quot;</span>
                <span class="s5">&quot;Received: &quot;</span>
                <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
            <span class="s2">)</span>

    <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">_get_logits</span><span class="s2">(</span>
        <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">, </span><span class="s5">&quot;Softmax&quot;</span><span class="s2">, </span><span class="s5">&quot;categorical_crossentropy&quot;</span>
    <span class="s2">)</span>
    <span class="s0">if </span><span class="s1">from_logits</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">softmax_cross_entropy_with_logits</span><span class="s2">(</span>
            <span class="s1">labels</span><span class="s2">=</span><span class="s1">target</span><span class="s2">, </span><span class="s1">logits</span><span class="s2">=</span><span class="s1">output</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span>
        <span class="s2">)</span>

    <span class="s4"># Adjust the predictions so that the probability of</span>
    <span class="s4"># each class for every sample adds up to 1</span>
    <span class="s4"># This is needed to ensure that the cross entropy is</span>
    <span class="s4"># computed correctly.</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">output </span><span class="s2">/ </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_sum</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>

    <span class="s4"># Compute cross entropy from probabilities.</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">clip_by_value</span><span class="s2">(</span>
        <span class="s1">output</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">(), </span><span class="s3">1.0 </span><span class="s2">- </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">()</span>
    <span class="s2">)</span>
    <span class="s0">return </span><span class="s2">-</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_sum</span><span class="s2">(</span><span class="s1">target </span><span class="s2">* </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">output</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">sparse_categorical_crossentropy</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Categorical crossentropy with integer targets. 
 
    Args: 
        target: An integer tensor. 
        output: A tensor resulting from a softmax 
            (unless `from_logits` is True, in which 
            case `output` is expected to be the logits). 
        from_logits: Boolean, whether `output` is the 
            result of a softmax, or is a tensor of logits. 
        axis: Int specifying the channels axis. `axis=-1` corresponds to data 
            format `channels_last`, and `axis=1` corresponds to data format 
            `channels_first`. 
 
    Returns: 
        Output tensor. 
    &quot;&quot;&quot;</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s2">!= -</span><span class="s3">1 </span><span class="s0">and </span><span class="s1">axis </span><span class="s2">!= </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) - </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">f&quot;Only axis=-1 is currently supported. Received: axis=</span><span class="s0">{</span><span class="s1">axis</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">_get_logits</span><span class="s2">(</span>
        <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">, </span><span class="s5">&quot;Softmax&quot;</span><span class="s2">, </span><span class="s5">&quot;sparse_categorical_crossentropy&quot;</span>
    <span class="s2">)</span>

    <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">target</span><span class="s2">)</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;int64&quot;</span><span class="s2">)</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) == </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) </span><span class="s0">and </span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[-</span><span class="s3">1</span><span class="s2">] == </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) &lt; </span><span class="s3">1</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Argument `output` must be at least rank 1. &quot;</span>
            <span class="s5">&quot;Received: &quot;</span>
            <span class="s5">f&quot;output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s3">1</span><span class="s2">]):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Argument `output` must have rank (ndim) `target.ndim - 1`. &quot;</span>
            <span class="s5">&quot;Received: &quot;</span>
            <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s0">for </span><span class="s1">e1</span><span class="s2">, </span><span class="s1">e2 </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[:-</span><span class="s3">1</span><span class="s2">]):</span>
        <span class="s0">if </span><span class="s1">e1 </span><span class="s0">is not None and </span><span class="s1">e2 </span><span class="s0">is not None and </span><span class="s1">e1 </span><span class="s2">!= </span><span class="s1">e2</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s5">&quot;Arguments `target` and `output` must have the same shape &quot;</span>
                <span class="s5">&quot;up until the last dimension: &quot;</span>
                <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
            <span class="s2">)</span>

    <span class="s0">if not </span><span class="s1">from_logits</span><span class="s2">:</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">clip_by_value</span><span class="s2">(</span>
            <span class="s1">output</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">(), </span><span class="s3">1 </span><span class="s2">- </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">()</span>
        <span class="s2">)</span>
        <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>

    <span class="s1">result </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">sparse_softmax_cross_entropy_with_logits</span><span class="s2">(</span>
        <span class="s1">labels</span><span class="s2">=</span><span class="s1">target</span><span class="s2">, </span><span class="s1">logits</span><span class="s2">=</span><span class="s1">output</span>
    <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">result</span>


<span class="s0">def </span><span class="s1">binary_crossentropy</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s6">&quot;&quot;&quot;Binary crossentropy between an output tensor and a target tensor. 
 
    Args: 
        target: A tensor with the same shape as `output`. 
        output: A tensor. 
        from_logits: Whether `output` is expected to be a logits tensor. 
            By default, we consider that `output` 
            encodes a probability distribution. 
 
    Returns: 
        A tensor. 
    &quot;&quot;&quot;</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">target</span><span class="s2">)</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) != </span><span class="s1">len</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">):</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">&quot;Arguments `target` and `output` must have the same rank &quot;</span>
            <span class="s5">&quot;(ndim). Received: &quot;</span>
            <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
        <span class="s2">)</span>
    <span class="s0">for </span><span class="s1">e1</span><span class="s2">, </span><span class="s1">e2 </span><span class="s0">in </span><span class="s1">zip</span><span class="s2">(</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">e1 </span><span class="s0">is not None and </span><span class="s1">e2 </span><span class="s0">is not None and </span><span class="s1">e1 </span><span class="s2">!= </span><span class="s1">e2</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s5">&quot;Arguments `target` and `output` must have the same shape. &quot;</span>
                <span class="s5">&quot;Received: &quot;</span>
                <span class="s5">f&quot;target.shape=</span><span class="s0">{</span><span class="s1">target</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">, output.shape=</span><span class="s0">{</span><span class="s1">output</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">}</span><span class="s5">&quot;</span>
            <span class="s2">)</span>

    <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits </span><span class="s2">= </span><span class="s1">_get_logits</span><span class="s2">(</span>
        <span class="s1">output</span><span class="s2">, </span><span class="s1">from_logits</span><span class="s2">, </span><span class="s5">&quot;Sigmoid&quot;</span><span class="s2">, </span><span class="s5">&quot;binary_crossentropy&quot;</span>
    <span class="s2">)</span>

    <span class="s0">if </span><span class="s1">from_logits</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">sigmoid_cross_entropy_with_logits</span><span class="s2">(</span>
            <span class="s1">labels</span><span class="s2">=</span><span class="s1">target</span><span class="s2">, </span><span class="s1">logits</span><span class="s2">=</span><span class="s1">output</span>
        <span class="s2">)</span>

    <span class="s4"># Compute cross entropy from probabilities.</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">clip_by_value</span><span class="s2">(</span>
        <span class="s1">output</span><span class="s2">, </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">(), </span><span class="s3">1.0 </span><span class="s2">- </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">epsilon</span><span class="s2">()</span>
    <span class="s2">)</span>
    <span class="s1">bce </span><span class="s2">= </span><span class="s1">target </span><span class="s2">* </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>
    <span class="s1">bce </span><span class="s2">+= (</span><span class="s3">1 </span><span class="s2">- </span><span class="s1">target</span><span class="s2">) * </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">log</span><span class="s2">(</span><span class="s3">1 </span><span class="s2">- </span><span class="s1">output</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s2">-</span><span class="s1">bce</span>


<span class="s0">def </span><span class="s1">moments</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">False</span><span class="s2">, </span><span class="s1">synchronized</span><span class="s2">=</span><span class="s0">False</span><span class="s2">):</span>
    <span class="s4"># The dynamic range of float16 is too limited for statistics. As a</span>
    <span class="s4"># workaround, we simply perform the operations on float32 and convert back</span>
    <span class="s4"># to float16</span>
    <span class="s1">need_cast </span><span class="s2">= </span><span class="s0">False</span>
    <span class="s1">ori_dtype </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">standardize_dtype</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">ori_dtype </span><span class="s0">in </span><span class="s2">(</span><span class="s5">&quot;float16&quot;</span><span class="s2">, </span><span class="s5">&quot;bfloat16&quot;</span><span class="s2">):</span>
        <span class="s1">need_cast </span><span class="s2">= </span><span class="s0">True</span>
        <span class="s1">x </span><span class="s2">= </span><span class="s1">cast</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s5">&quot;float32&quot;</span><span class="s2">)</span>

    <span class="s0">if </span><span class="s1">synchronized</span><span class="s2">:</span>
        <span class="s1">mean</span><span class="s2">, </span><span class="s1">variance </span><span class="s2">= </span><span class="s1">_compute_moments_sync</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s1">mean</span><span class="s2">, </span><span class="s1">variance </span><span class="s2">= </span><span class="s1">_compute_moments</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">need_cast</span><span class="s2">:</span>
        <span class="s4"># avoid overflow and underflow when casting from float16 to float32</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">clip_by_value</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">float16</span><span class="s2">.</span><span class="s1">min</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">float16</span><span class="s2">.</span><span class="s1">max</span><span class="s2">)</span>
        <span class="s1">variance </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">clip_by_value</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">float16</span><span class="s2">.</span><span class="s1">min</span><span class="s2">, </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">float16</span><span class="s2">.</span><span class="s1">max</span><span class="s2">)</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">cast</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">ori_dtype</span><span class="s2">)</span>
        <span class="s1">variance </span><span class="s2">= </span><span class="s1">cast</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">ori_dtype</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">variance</span>


<span class="s0">def </span><span class="s1">_compute_moments_sync</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">):</span>
    <span class="s1">replica_ctx </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">distribute</span><span class="s2">.</span><span class="s1">get_replica_context</span><span class="s2">()</span>
    <span class="s0">if not </span><span class="s1">replica_ctx</span><span class="s2">:</span>
        <span class="s0">return </span><span class="s1">_compute_moments</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">)</span>

    <span class="s1">local_count </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">ones_like</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s5">&quot;count&quot;</span><span class="s2">)</span>

    <span class="s1">local_sum </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_sum</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
    <span class="s1">local_squared_sum </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_sum</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">x</span><span class="s2">), </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>
    <span class="s1">local_count </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_sum</span><span class="s2">(</span><span class="s1">local_count</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s0">True</span><span class="s2">)</span>

    <span class="s4"># TODO(b/163099951): batch the all-reduces once we sort out the</span>
    <span class="s4"># ordering issue for NCCL. We don't have a mechanism to launch</span>
    <span class="s4"># NCCL in the same order in each replica nowadays, so we limit</span>
    <span class="s4"># NCCL to batch all-reduces.</span>
    <span class="s1">y_sum </span><span class="s2">= </span><span class="s1">replica_ctx</span><span class="s2">.</span><span class="s1">all_reduce</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">distribute</span><span class="s2">.</span><span class="s1">ReduceOp</span><span class="s2">.</span><span class="s1">SUM</span><span class="s2">, </span><span class="s1">local_sum</span><span class="s2">)</span>
    <span class="s1">y_squared_sum </span><span class="s2">= </span><span class="s1">replica_ctx</span><span class="s2">.</span><span class="s1">all_reduce</span><span class="s2">(</span>
        <span class="s1">tf</span><span class="s2">.</span><span class="s1">distribute</span><span class="s2">.</span><span class="s1">ReduceOp</span><span class="s2">.</span><span class="s1">SUM</span><span class="s2">, </span><span class="s1">local_squared_sum</span>
    <span class="s2">)</span>
    <span class="s1">count_sum </span><span class="s2">= </span><span class="s1">replica_ctx</span><span class="s2">.</span><span class="s1">all_reduce</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">distribute</span><span class="s2">.</span><span class="s1">ReduceOp</span><span class="s2">.</span><span class="s1">SUM</span><span class="s2">, </span><span class="s1">local_count</span><span class="s2">)</span>

    <span class="s1">mean </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">divide_no_nan</span><span class="s2">(</span><span class="s1">y_sum</span><span class="s2">, </span><span class="s1">count_sum</span><span class="s2">)</span>
    <span class="s1">y_squared_mean </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">math</span><span class="s2">.</span><span class="s1">divide_no_nan</span><span class="s2">(</span><span class="s1">y_squared_sum</span><span class="s2">, </span><span class="s1">count_sum</span><span class="s2">)</span>
    <span class="s4"># var = E(x^2) - E(x)^2</span>
    <span class="s1">variance </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">maximum</span><span class="s2">(</span><span class="s1">y_squared_mean </span><span class="s2">- </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">), </span><span class="s3">0.0</span><span class="s2">)</span>
    <span class="s0">if not </span><span class="s1">keepdims</span><span class="s2">:</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">)</span>
        <span class="s1">variance </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">squeeze</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">variance</span>


<span class="s0">def </span><span class="s1">_compute_moments</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">):</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">moments</span><span class="s2">(</span><span class="s1">x</span><span class="s2">, </span><span class="s1">axes</span><span class="s2">, </span><span class="s1">keepdims</span><span class="s2">=</span><span class="s1">keepdims</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">batch_normalization</span><span class="s2">(</span>
    <span class="s1">x</span><span class="s2">, </span><span class="s1">mean</span><span class="s2">, </span><span class="s1">variance</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">, </span><span class="s1">offset</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">scale</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">epsilon</span><span class="s2">=</span><span class="s3">1e-3</span>
<span class="s2">):</span>
    <span class="s0">if </span><span class="s1">axis </span><span class="s2">!= -</span><span class="s3">1</span><span class="s2">:</span>
        <span class="s1">shape </span><span class="s2">= [</span><span class="s3">1</span><span class="s2">] * </span><span class="s1">len</span><span class="s2">(</span><span class="s1">x</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s1">shape</span><span class="s2">[</span><span class="s1">axis</span><span class="s2">] = </span><span class="s1">mean</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">]</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">mean</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s1">variance </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">variance</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">offset </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">offset </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">offset</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">scale </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">scale </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">scale</span><span class="s2">, </span><span class="s1">shape</span><span class="s2">)</span>

    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">batch_normalization</span><span class="s2">(</span>
        <span class="s1">x</span><span class="s2">=</span><span class="s1">x</span><span class="s2">,</span>
        <span class="s1">mean</span><span class="s2">=</span><span class="s1">mean</span><span class="s2">,</span>
        <span class="s1">variance</span><span class="s2">=</span><span class="s1">variance</span><span class="s2">,</span>
        <span class="s1">offset</span><span class="s2">=</span><span class="s1">offset</span><span class="s2">,</span>
        <span class="s1">scale</span><span class="s2">=</span><span class="s1">scale</span><span class="s2">,</span>
        <span class="s1">variance_epsilon</span><span class="s2">=</span><span class="s1">epsilon</span><span class="s2">,</span>
    <span class="s2">)</span>


<span class="s0">def </span><span class="s1">ctc_loss</span><span class="s2">(</span>
    <span class="s1">target</span><span class="s2">,</span>
    <span class="s1">output</span><span class="s2">,</span>
    <span class="s1">target_length</span><span class="s2">,</span>
    <span class="s1">output_length</span><span class="s2">,</span>
    <span class="s1">mask_index</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">target</span><span class="s2">)</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">output</span><span class="s2">)</span>
    <span class="s1">target </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">target</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;int32&quot;</span><span class="s2">)</span>

    <span class="s4"># `tf.nn.ctc_loss` will internally cast to float32 when the input is float16</span>
    <span class="s4"># or bfloat16. Additionally, it will raise an error when the input is</span>
    <span class="s4"># float64. As a result, we perform the casting externally and add support</span>
    <span class="s4"># for float64.</span>
    <span class="s1">result_dtype </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">result_type</span><span class="s2">(</span><span class="s1">output</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s5">&quot;float32&quot;</span><span class="s2">)</span>
    <span class="s1">compute_dtype </span><span class="s2">= </span><span class="s5">&quot;float32&quot; </span><span class="s0">if </span><span class="s1">result_dtype </span><span class="s2">== </span><span class="s5">&quot;float64&quot; </span><span class="s0">else </span><span class="s1">result_dtype</span>
    <span class="s1">output </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">output</span><span class="s2">, </span><span class="s1">compute_dtype</span><span class="s2">)</span>
    <span class="s1">loss </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">ctc_loss</span><span class="s2">(</span>
        <span class="s1">labels</span><span class="s2">=</span><span class="s1">target</span><span class="s2">,</span>
        <span class="s1">logits</span><span class="s2">=</span><span class="s1">output</span><span class="s2">,</span>
        <span class="s1">label_length</span><span class="s2">=</span><span class="s1">target_length</span><span class="s2">,</span>
        <span class="s1">logit_length</span><span class="s2">=</span><span class="s1">output_length</span><span class="s2">,</span>
        <span class="s1">blank_index</span><span class="s2">=</span><span class="s1">mask_index</span><span class="s2">,</span>
        <span class="s1">logits_time_major</span><span class="s2">=</span><span class="s0">False</span><span class="s2">,</span>
    <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">loss</span><span class="s2">, </span><span class="s1">result_dtype</span><span class="s2">)</span>


<span class="s0">def </span><span class="s1">ctc_decode</span><span class="s2">(</span>
    <span class="s1">inputs</span><span class="s2">,</span>
    <span class="s1">sequence_lengths</span><span class="s2">,</span>
    <span class="s1">strategy</span><span class="s2">=</span><span class="s5">&quot;greedy&quot;</span><span class="s2">,</span>
    <span class="s1">beam_width</span><span class="s2">=</span><span class="s3">100</span><span class="s2">,</span>
    <span class="s1">top_paths</span><span class="s2">=</span><span class="s3">1</span><span class="s2">,</span>
    <span class="s1">merge_repeated</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
    <span class="s1">mask_index</span><span class="s2">=</span><span class="s3">0</span><span class="s2">,</span>
<span class="s2">):</span>
    <span class="s1">inputs </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>
    <span class="s1">input_shape </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">)</span>
    <span class="s1">num_samples</span><span class="s2">, </span><span class="s1">num_steps </span><span class="s2">= </span><span class="s1">input_shape</span><span class="s2">[</span><span class="s3">0</span><span class="s2">], </span><span class="s1">input_shape</span><span class="s2">[</span><span class="s3">1</span><span class="s2">]</span>
    <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">transpose</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, (</span><span class="s3">1</span><span class="s2">, </span><span class="s3">0</span><span class="s2">, </span><span class="s3">2</span><span class="s2">))</span>

    <span class="s1">dtype </span><span class="s2">= </span><span class="s1">backend</span><span class="s2">.</span><span class="s1">result_type</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">, </span><span class="s5">&quot;float32&quot;</span><span class="s2">)</span>
    <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">)</span>

    <span class="s1">sequence_lengths </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">sequence_lengths</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s5">&quot;int32&quot;</span><span class="s2">)</span>
    <span class="s0">if </span><span class="s1">strategy </span><span class="s2">== </span><span class="s5">&quot;greedy&quot;</span><span class="s2">:</span>
        <span class="s2">(</span><span class="s1">decoded</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">) = </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">ctc_greedy_decoder</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">=</span><span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">sequence_length</span><span class="s2">=</span><span class="s1">sequence_lengths</span><span class="s2">,</span>
            <span class="s1">merge_repeated</span><span class="s2">=</span><span class="s1">merge_repeated</span><span class="s2">,</span>
            <span class="s1">blank_index</span><span class="s2">=</span><span class="s1">mask_index</span><span class="s2">,</span>
        <span class="s2">)</span>
    <span class="s0">elif </span><span class="s1">strategy </span><span class="s2">== </span><span class="s5">&quot;beam_search&quot;</span><span class="s2">:</span>
        <span class="s4"># Move `mask_index` column to the last position since this is the</span>
        <span class="s4"># default for `tf.nn.ctc_beam_search_decoder`</span>
        <span class="s0">if </span><span class="s1">mask_index </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s1">inputs_before </span><span class="s2">= </span><span class="s1">inputs</span><span class="s2">[..., :</span><span class="s1">mask_index</span><span class="s2">]</span>
            <span class="s1">inputs_mask </span><span class="s2">= </span><span class="s1">inputs</span><span class="s2">[..., </span><span class="s1">mask_index </span><span class="s2">: </span><span class="s1">mask_index </span><span class="s2">+ </span><span class="s3">1</span><span class="s2">]</span>
            <span class="s1">inputs_after </span><span class="s2">= </span><span class="s1">inputs</span><span class="s2">[..., </span><span class="s1">mask_index </span><span class="s2">+ </span><span class="s3">1 </span><span class="s2">:]</span>
            <span class="s1">inputs </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">concat</span><span class="s2">(</span>
                <span class="s2">[</span><span class="s1">inputs_before</span><span class="s2">, </span><span class="s1">inputs_after</span><span class="s2">, </span><span class="s1">inputs_mask</span><span class="s2">], </span><span class="s1">axis</span><span class="s2">=-</span><span class="s3">1</span>
            <span class="s2">)</span>
        <span class="s2">(</span><span class="s1">decoded</span><span class="s2">, </span><span class="s1">scores</span><span class="s2">) = </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">nn</span><span class="s2">.</span><span class="s1">ctc_beam_search_decoder</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">=</span><span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">sequence_length</span><span class="s2">=</span><span class="s1">sequence_lengths</span><span class="s2">,</span>
            <span class="s1">beam_width</span><span class="s2">=</span><span class="s1">beam_width</span><span class="s2">,</span>
            <span class="s1">top_paths</span><span class="s2">=</span><span class="s1">top_paths</span><span class="s2">,</span>
        <span class="s2">)</span>
    <span class="s0">else</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">f&quot;Invalid strategy </span><span class="s0">{</span><span class="s1">strategy</span><span class="s0">}</span><span class="s5">. Supported values are &quot;</span>
            <span class="s5">&quot;'greedy' and 'beam_search'.&quot;</span>
        <span class="s2">)</span>

    <span class="s4"># Postprocess sparse tensor</span>
    <span class="s1">decoded_dense </span><span class="s2">= []</span>
    <span class="s0">for </span><span class="s1">st </span><span class="s0">in </span><span class="s1">decoded</span><span class="s2">:</span>
        <span class="s1">st </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">SparseTensor</span><span class="s2">(</span><span class="s1">st</span><span class="s2">.</span><span class="s1">indices</span><span class="s2">, </span><span class="s1">st</span><span class="s2">.</span><span class="s1">values</span><span class="s2">, (</span><span class="s1">num_samples</span><span class="s2">, </span><span class="s1">num_steps</span><span class="s2">))</span>
        <span class="s1">decoded_dense</span><span class="s2">.</span><span class="s1">append</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">sparse</span><span class="s2">.</span><span class="s1">to_dense</span><span class="s2">(</span><span class="s1">sp_input</span><span class="s2">=</span><span class="s1">st</span><span class="s2">, </span><span class="s1">default_value</span><span class="s2">=-</span><span class="s3">1</span><span class="s2">))</span>
    <span class="s1">decoded_dense </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">stack</span><span class="s2">(</span><span class="s1">decoded_dense</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s3">0</span><span class="s2">)</span>
    <span class="s1">decoded_dense </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">decoded_dense</span><span class="s2">, </span><span class="s5">&quot;int32&quot;</span><span class="s2">)</span>

    <span class="s4"># We need to recover the labels because we swapped the indices earlier</span>
    <span class="s0">if </span><span class="s1">strategy </span><span class="s2">== </span><span class="s5">&quot;beam_search&quot; </span><span class="s0">and </span><span class="s1">mask_index </span><span class="s0">is not None</span><span class="s2">:</span>
        <span class="s0">if </span><span class="s1">mask_index </span><span class="s2">&lt; </span><span class="s3">0</span><span class="s2">:</span>
            <span class="s1">mask_index </span><span class="s2">= </span><span class="s1">mask_index </span><span class="s2">+ </span><span class="s1">input_shape</span><span class="s2">[-</span><span class="s3">1</span><span class="s2">]</span>
        <span class="s1">decoded_dense </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span>
            <span class="s1">decoded_dense </span><span class="s2">&gt;= </span><span class="s1">mask_index</span><span class="s2">, </span><span class="s1">decoded_dense </span><span class="s2">+ </span><span class="s3">1</span><span class="s2">, </span><span class="s1">decoded_dense</span>
        <span class="s2">)</span>
    <span class="s0">return </span><span class="s1">decoded_dense</span><span class="s2">, </span><span class="s1">scores</span>


<span class="s0">def </span><span class="s1">psnr</span><span class="s2">(</span><span class="s1">x1</span><span class="s2">, </span><span class="s1">x2</span><span class="s2">, </span><span class="s1">max_val</span><span class="s2">):</span>
    <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">tensorflow</span><span class="s2">.</span><span class="s1">numpy </span><span class="s0">import </span><span class="s1">log10</span>

    <span class="s0">if </span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape </span><span class="s2">!= </span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">:</span>
        <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
            <span class="s5">f&quot;Input shapes </span><span class="s0">{</span><span class="s1">x1</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">} </span><span class="s5">and </span><span class="s0">{</span><span class="s1">x2</span><span class="s2">.</span><span class="s1">shape</span><span class="s0">} </span><span class="s5">must &quot;</span>
            <span class="s5">&quot;match for PSNR calculation. &quot;</span>
        <span class="s2">)</span>

    <span class="s1">max_val </span><span class="s2">= </span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">max_val</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">x2</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
    <span class="s1">mse </span><span class="s2">= </span><span class="s1">tf</span><span class="s2">.</span><span class="s1">reduce_mean</span><span class="s2">(</span><span class="s1">tf</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">x1 </span><span class="s2">- </span><span class="s1">x2</span><span class="s2">))</span>
    <span class="s1">psnr </span><span class="s2">= </span><span class="s3">20 </span><span class="s2">* </span><span class="s1">log10</span><span class="s2">(</span><span class="s1">max_val</span><span class="s2">) - </span><span class="s3">10 </span><span class="s2">* </span><span class="s1">log10</span><span class="s2">(</span><span class="s1">mse</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">psnr</span>
</pre>
</body>
</html>