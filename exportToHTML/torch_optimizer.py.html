<html>
<head>
<title>torch_optimizer.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #7a7e85;}
.s4 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
torch_optimizer.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">torch</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">optimizers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">optimizers</span><span class="s2">.</span><span class="s1">base_optimizer </span><span class="s0">import </span><span class="s1">BaseOptimizer</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">utils </span><span class="s0">import </span><span class="s1">torch_utils</span>


<span class="s0">class </span><span class="s1">TorchOptimizer</span><span class="s2">(</span><span class="s1">BaseOptimizer</span><span class="s2">):</span>
    <span class="s0">def </span><span class="s1">__new__</span><span class="s2">(</span><span class="s1">cls</span><span class="s2">, *</span><span class="s1">args</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">):</span>
        <span class="s3"># Import locally to avoid circular imports.</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_adadelta</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_adagrad</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_adam</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_adamax</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_adamw</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_lion</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_nadam</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_rmsprop</span>
        <span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">backend</span><span class="s2">.</span><span class="s1">torch</span><span class="s2">.</span><span class="s1">optimizers </span><span class="s0">import </span><span class="s1">torch_sgd</span>

        <span class="s1">OPTIMIZERS </span><span class="s2">= {</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Adadelta</span><span class="s2">: </span><span class="s1">torch_adadelta</span><span class="s2">.</span><span class="s1">Adadelta</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Adagrad</span><span class="s2">: </span><span class="s1">torch_adagrad</span><span class="s2">.</span><span class="s1">Adagrad</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Adam</span><span class="s2">: </span><span class="s1">torch_adam</span><span class="s2">.</span><span class="s1">Adam</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Adamax</span><span class="s2">: </span><span class="s1">torch_adamax</span><span class="s2">.</span><span class="s1">Adamax</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">AdamW</span><span class="s2">: </span><span class="s1">torch_adamw</span><span class="s2">.</span><span class="s1">AdamW</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Lion</span><span class="s2">: </span><span class="s1">torch_lion</span><span class="s2">.</span><span class="s1">Lion</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">Nadam</span><span class="s2">: </span><span class="s1">torch_nadam</span><span class="s2">.</span><span class="s1">Nadam</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">RMSprop</span><span class="s2">: </span><span class="s1">torch_rmsprop</span><span class="s2">.</span><span class="s1">RMSprop</span><span class="s2">,</span>
            <span class="s1">optimizers</span><span class="s2">.</span><span class="s1">SGD</span><span class="s2">: </span><span class="s1">torch_sgd</span><span class="s2">.</span><span class="s1">SGD</span><span class="s2">,</span>
        <span class="s2">}</span>

        <span class="s0">if </span><span class="s1">cls </span><span class="s0">in </span><span class="s1">OPTIMIZERS</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">OPTIMIZERS</span><span class="s2">[</span><span class="s1">cls</span><span class="s2">](*</span><span class="s1">args</span><span class="s2">, **</span><span class="s1">kwargs</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">super</span><span class="s2">().</span><span class="s1">__new__</span><span class="s2">(</span><span class="s1">cls</span><span class="s2">)</span>

    <span class="s2">@</span><span class="s1">torch_utils</span><span class="s2">.</span><span class="s1">no_grad</span>
    <span class="s0">def </span><span class="s1">_apply_weight_decay</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">variables</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s0">return</span>

        <span class="s1">torch</span><span class="s2">.</span><span class="s1">_foreach_mul_</span><span class="s2">(</span>
            <span class="s2">[</span><span class="s1">v</span><span class="s2">.</span><span class="s1">value </span><span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">variables </span><span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_use_weight_decay</span><span class="s2">(</span><span class="s1">v</span><span class="s2">)],</span>
            <span class="s4">1 </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">weight_decay </span><span class="s2">* </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_get_current_learning_rate</span><span class="s2">(),</span>
        <span class="s2">)</span>
</pre>
</body>
</html>