<html>
<head>
<title>learning_rate_schedule.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #5f826b; font-style: italic;}
.s1 { color: #bcbec4;}
.s2 { color: #cf8e6d;}
.s3 { color: #bcbec4;}
.s4 { color: #6aab73;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
learning_rate_schedule.py</font>
</center></td></tr></table>
<pre><span class="s0">&quot;&quot;&quot;Various learning rate schedule functions.&quot;&quot;&quot;</span>

<span class="s2">import </span><span class="s1">math</span>

<span class="s2">from </span><span class="s1">keras</span><span class="s3">.</span><span class="s1">src </span><span class="s2">import </span><span class="s1">ops</span>
<span class="s2">from </span><span class="s1">keras</span><span class="s3">.</span><span class="s1">src</span><span class="s3">.</span><span class="s1">api_export </span><span class="s2">import </span><span class="s1">keras_export</span>
<span class="s2">from </span><span class="s1">keras</span><span class="s3">.</span><span class="s1">src</span><span class="s3">.</span><span class="s1">saving </span><span class="s2">import </span><span class="s1">serialization_lib</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.LearningRateSchedule&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">LearningRateSchedule</span><span class="s3">:</span>
    <span class="s0">&quot;&quot;&quot;The learning rate schedule base class. 
 
    You can use a learning rate schedule to modulate how the learning rate 
    of your optimizer changes over time. 
 
    Several built-in learning rate schedules are available, such as 
    `keras.optimizers.schedules.ExponentialDecay` or 
    `keras.optimizers.schedules.PiecewiseConstantDecay`: 
 
    ```python 
    lr_schedule = keras.optimizers.schedules.ExponentialDecay( 
        initial_learning_rate=1e-2, 
        decay_steps=10000, 
        decay_rate=0.9) 
    optimizer = keras.optimizers.SGD(learning_rate=lr_schedule) 
    ``` 
 
    A `LearningRateSchedule` instance can be passed in as the `learning_rate` 
    argument of any optimizer. 
 
    To implement your own schedule object, you should implement the `__call__` 
    method, which takes a `step` argument (scalar integer tensor, the 
    current training step count). 
    Like for any other Keras object, you can also optionally 
    make your object serializable by implementing the `get_config` 
    and `from_config` methods. 
 
    Example: 
 
    ```python 
    class MyLRSchedule(keras.optimizers.schedules.LearningRateSchedule): 
 
        def __init__(self, initial_learning_rate): 
            self.initial_learning_rate = initial_learning_rate 
 
        def __call__(self, step): 
            return self.initial_learning_rate / (step + 1) 
 
    optimizer = keras.optimizers.SGD(learning_rate=MyLRSchedule(0.1)) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span><span class="s3">(</span>
            <span class="s4">f&quot;Learning rate schedule '</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">__class__</span><span class="s3">.</span><span class="s1">__name__</span><span class="s2">}</span><span class="s4">' &quot;</span>
            <span class="s4">&quot;must override `__call__(self, step)`.&quot;</span>
        <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">raise </span><span class="s1">NotImplementedError</span><span class="s3">(</span>
            <span class="s4">f&quot;Learning rate schedule '</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">__class__</span><span class="s3">.</span><span class="s1">__name__</span><span class="s2">}</span><span class="s4">' &quot;</span>
            <span class="s4">&quot;must override `get_config()` in order to be serializable.&quot;</span>
        <span class="s3">)</span>

    <span class="s3">@</span><span class="s1">classmethod</span>
    <span class="s2">def </span><span class="s1">from_config</span><span class="s3">(</span><span class="s1">cls</span><span class="s3">, </span><span class="s1">config</span><span class="s3">):</span>
        <span class="s0">&quot;&quot;&quot;Instantiates a `LearningRateSchedule` from its config. 
 
        Args: 
            config: Output of `get_config()`. 
 
        Returns: 
            A `LearningRateSchedule` instance. 
        &quot;&quot;&quot;</span>
        <span class="s2">return </span><span class="s1">cls</span><span class="s3">(**</span><span class="s1">config</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.ExponentialDecay&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">ExponentialDecay</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses an exponential decay schedule. 
 
    When training a model, it is often useful to lower the learning rate as 
    the training progresses. This schedule applies an exponential decay function 
    to an optimizer step, given a provided initial learning rate. 
 
    The schedule is a 1-arg callable that produces a decayed learning 
    rate when passed the current optimizer step. This can be useful for changing 
    the learning rate value across different invocations of optimizer functions. 
    It is computed as: 
 
    ```python 
    def decayed_learning_rate(step): 
        return initial_learning_rate * decay_rate ^ (step / decay_steps) 
    ``` 
 
    If the argument `staircase` is `True`, then `step / decay_steps` is 
    an integer division and the decayed learning rate follows a 
    staircase function. 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. 
    Example: When fitting a Keras model, decay every 100000 steps with a base 
    of 0.96: 
 
    ```python 
    initial_learning_rate = 0.1 
    lr_schedule = keras.optimizers.schedules.ExponentialDecay( 
        initial_learning_rate, 
        decay_steps=100000, 
        decay_rate=0.96, 
        staircase=True) 
 
    model.compile(optimizer=keras.optimizers.SGD(learning_rate=lr_schedule), 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy']) 
 
    model.fit(data, labels, epochs=5) 
    ``` 
 
    The learning rate schedule is also serializable and deserializable using 
    `keras.optimizers.schedules.serialize` and 
    `keras.optimizers.schedules.deserialize`. 
 
    Args: 
        initial_learning_rate: A Python float. The initial learning rate. 
        decay_steps: A Python integer. Must be positive. See the decay 
            computation above. 
        decay_rate: A Python float. The decay rate. 
        staircase: Boolean.  If `True` decay the learning rate at discrete 
            intervals. 
        name: String.  Optional name of the operation.  Defaults to 
            `&quot;ExponentialDecay`&quot;. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as `initial_learning_rate`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">initial_learning_rate</span><span class="s3">,</span>
        <span class="s1">decay_steps</span><span class="s3">,</span>
        <span class="s1">decay_rate</span><span class="s3">,</span>
        <span class="s1">staircase</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
        <span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;ExponentialDecay&quot;</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">initial_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">decay_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate </span><span class="s3">= </span><span class="s1">decay_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">staircase </span><span class="s3">= </span><span class="s1">staircase</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;Argument `decay_steps` must be &gt; 0. &quot;</span>
                <span class="s4">f&quot;Received: decay_steps=</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s2">}</span><span class="s4">&quot;</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span>
                <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span>
            <span class="s3">)</span>
            <span class="s1">dtype </span><span class="s3">= </span><span class="s1">initial_learning_rate</span><span class="s3">.</span><span class="s1">dtype</span>
            <span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">decay_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">step</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">p </span><span class="s3">= </span><span class="s1">global_step_recomp </span><span class="s3">/ </span><span class="s1">decay_steps</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">staircase</span><span class="s3">:</span>
                <span class="s1">p </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)</span>
            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span><span class="s1">initial_learning_rate</span><span class="s3">, </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">power</span><span class="s3">(</span><span class="s1">decay_rate</span><span class="s3">, </span><span class="s1">p</span><span class="s3">))</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;initial_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;decay_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">,</span>
            <span class="s4">&quot;decay_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate</span><span class="s3">,</span>
            <span class="s4">&quot;staircase&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">staircase</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.PiecewiseConstantDecay&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">PiecewiseConstantDecay</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses a piecewise constant decay schedule. 
 
    The function returns a 1-arg callable to compute the piecewise constant 
    when passed the current optimizer step. This can be useful for changing the 
    learning rate value across different invocations of optimizer functions. 
 
    Example: use a learning rate that's 1.0 for the first 100001 steps, 0.5 
        for the next 10000 steps, and 0.1 for any additional steps. 
 
    ```python 
    step = ops.array(0) 
    boundaries = [100000, 110000] 
    values = [1.0, 0.5, 0.1] 
    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay( 
        boundaries, values) 
 
    # Later, whenever we perform an optimization step, we pass in the step. 
    learning_rate = learning_rate_fn(step) 
    ``` 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. The learning rate schedule is also serializable and 
    deserializable using `keras.optimizers.schedules.serialize` and 
    `keras.optimizers.schedules.deserialize`. 
 
    Args: 
        boundaries: A list of Python numbers with strictly increasing 
            entries, and with all elements having the same type as the 
            optimizer step. 
        values: A list of Python numbers that specifies the values for the 
            intervals defined by `boundaries`. It should have one more 
            element than `boundaries`, and all elements should have the same 
            type. 
        name: A string. Optional name of the operation. Defaults to 
            `&quot;PiecewiseConstant&quot;`. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as the boundary tensors. 
 
        The output of the 1-arg function that takes the `step` 
        is `values[0]` when `step &lt;= boundaries[0]`, 
        `values[1]` when `step &gt; boundaries[0]` and `step &lt;= boundaries[1]`, 
        ..., and `values[-1]` when `step &gt; boundaries[-1]`. 
 
 
    Raises: 
        ValueError: if the number of elements in the `boundaries` and `values` 
        lists do not match. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">boundaries</span><span class="s3">, </span><span class="s1">values</span><span class="s3">, </span><span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;PiecewiseConstant&quot;</span><span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>

        <span class="s2">if </span><span class="s1">len</span><span class="s3">(</span><span class="s1">boundaries</span><span class="s3">) != </span><span class="s1">len</span><span class="s3">(</span><span class="s1">values</span><span class="s3">) - </span><span class="s5">1</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;The length of boundaries should be 1 less than the length of &quot;</span>
                <span class="s4">f&quot;values. Received: boundaries=</span><span class="s2">{</span><span class="s1">boundaries</span><span class="s2">} </span><span class="s4">of length &quot;</span>
                <span class="s4">f&quot;</span><span class="s2">{</span><span class="s1">len</span><span class="s3">(</span><span class="s1">boundaries</span><span class="s3">)</span><span class="s2">}</span><span class="s4">, and values=</span><span class="s2">{</span><span class="s1">values</span><span class="s2">} </span><span class="s4">&quot;</span>
                <span class="s4">f&quot;of length </span><span class="s2">{</span><span class="s1">len</span><span class="s3">(</span><span class="s1">values</span><span class="s3">)</span><span class="s2">}</span><span class="s4">.&quot;</span>
            <span class="s3">)</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">boundaries </span><span class="s3">= </span><span class="s1">boundaries</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">values </span><span class="s3">= </span><span class="s1">values</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">boundaries </span><span class="s3">= [</span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span><span class="s1">x</span><span class="s3">) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">self</span><span class="s3">.</span><span class="s1">boundaries</span><span class="s3">]</span>
            <span class="s1">values </span><span class="s3">= [</span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span><span class="s1">x</span><span class="s3">) </span><span class="s2">for </span><span class="s1">x </span><span class="s2">in </span><span class="s1">self</span><span class="s3">.</span><span class="s1">values</span><span class="s3">]</span>
            <span class="s1">step </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span><span class="s1">step</span><span class="s3">)</span>

            <span class="s2">for </span><span class="s1">i</span><span class="s3">, </span><span class="s1">b </span><span class="s2">in </span><span class="s1">enumerate</span><span class="s3">(</span><span class="s1">boundaries</span><span class="s3">):</span>
                <span class="s2">if </span><span class="s1">b</span><span class="s3">.</span><span class="s1">dtype </span><span class="s3">!= </span><span class="s1">step</span><span class="s3">.</span><span class="s1">dtype</span><span class="s3">:</span>
                    <span class="s6"># We cast the boundaries to have the same type as the step</span>
                    <span class="s1">b </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">b</span><span class="s3">, </span><span class="s1">step</span><span class="s3">.</span><span class="s1">dtype</span><span class="s3">)</span>
                    <span class="s1">boundaries</span><span class="s3">[</span><span class="s1">i</span><span class="s3">] = </span><span class="s1">b</span>

            <span class="s1">result_dtype </span><span class="s3">= </span><span class="s1">values</span><span class="s3">[</span><span class="s5">0</span><span class="s3">].</span><span class="s1">dtype</span>
            <span class="s1">result_value </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s5">0</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">result_dtype</span><span class="s3">)</span>

            <span class="s6"># For each range between boundaries, we check whether the step is</span>
            <span class="s6"># within that range, cast the resulting boolean to a number,</span>
            <span class="s6"># and multiply the result by the corresponding value for the range.</span>
            <span class="s6"># Taking the sum of these yields a piecewise constant function.</span>
            <span class="s1">step_less_than_first_boundary </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span>
                <span class="s1">step </span><span class="s3">&lt;= </span><span class="s1">boundaries</span><span class="s3">[</span><span class="s5">0</span><span class="s3">], </span><span class="s1">result_dtype</span>
            <span class="s3">)</span>
            <span class="s1">result_value </span><span class="s3">+= </span><span class="s1">step_less_than_first_boundary </span><span class="s3">* </span><span class="s1">values</span><span class="s3">[</span><span class="s5">0</span><span class="s3">]</span>

            <span class="s1">step_greater_than_last_boundary </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span>
                <span class="s1">step </span><span class="s3">&gt; </span><span class="s1">boundaries</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">result_dtype</span>
            <span class="s3">)</span>
            <span class="s1">result_value </span><span class="s3">+= </span><span class="s1">step_greater_than_last_boundary </span><span class="s3">* </span><span class="s1">values</span><span class="s3">[-</span><span class="s5">1</span><span class="s3">]</span>

            <span class="s2">for </span><span class="s1">low</span><span class="s3">, </span><span class="s1">high</span><span class="s3">, </span><span class="s1">value </span><span class="s2">in </span><span class="s1">zip</span><span class="s3">(</span>
                <span class="s1">boundaries</span><span class="s3">[:-</span><span class="s5">1</span><span class="s3">], </span><span class="s1">boundaries</span><span class="s3">[</span><span class="s5">1</span><span class="s3">:], </span><span class="s1">values</span><span class="s3">[</span><span class="s5">1</span><span class="s3">:-</span><span class="s5">1</span><span class="s3">]</span>
            <span class="s3">):</span>
                <span class="s6"># Need to bind v here; can do this with lambda v=v: ...</span>
                <span class="s1">step_in_range </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span>
                    <span class="s3">(</span><span class="s1">step </span><span class="s3">&gt; </span><span class="s1">low</span><span class="s3">) &amp; (</span><span class="s1">step </span><span class="s3">&lt;= </span><span class="s1">high</span><span class="s3">), </span><span class="s1">result_dtype</span>
                <span class="s3">)</span>
                <span class="s1">result_value </span><span class="s3">+= </span><span class="s1">step_in_range </span><span class="s3">* </span><span class="s1">value</span>

            <span class="s2">return </span><span class="s1">result_value</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;boundaries&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">boundaries</span><span class="s3">,</span>
            <span class="s4">&quot;values&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">values</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.PolynomialDecay&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">PolynomialDecay</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses a polynomial decay schedule. 
 
    It is commonly observed that a monotonically decreasing learning rate, whose 
    degree of change is carefully chosen, results in a better performing model. 
    This schedule applies a polynomial decay function to an optimizer step, 
    given a provided `initial_learning_rate`, to reach an `end_learning_rate` 
    in the given `decay_steps`. 
 
    It requires a `step` value to compute the decayed learning rate. You 
    can just pass a backend variable that you increment at each training 
    step. 
 
    The schedule is a 1-arg callable that produces a decayed learning rate 
    when passed the current optimizer step. This can be useful for changing the 
    learning rate value across different invocations of optimizer functions. 
    It is computed as: 
 
    ```python 
    def decayed_learning_rate(step): 
        step = min(step, decay_steps) 
        return ((initial_learning_rate - end_learning_rate) * 
                (1 - step / decay_steps) ^ (power) 
               ) + end_learning_rate 
    ``` 
 
    If `cycle` is True then a multiple of `decay_steps` is used, the first one 
    that is bigger than `step`. 
 
    ```python 
    def decayed_learning_rate(step): 
        decay_steps = decay_steps * ceil(step / decay_steps) 
        return ((initial_learning_rate - end_learning_rate) * 
                (1 - step / decay_steps) ^ (power) 
               ) + end_learning_rate 
    ``` 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. 
    Example: Fit a model while decaying from 0.1 to 0.01 in 10000 steps using 
    sqrt (i.e. power=0.5): 
 
    ```python 
    ... 
    starter_learning_rate = 0.1 
    end_learning_rate = 0.01 
    decay_steps = 10000 
    learning_rate_fn = keras.optimizers.schedules.PolynomialDecay( 
        starter_learning_rate, 
        decay_steps, 
        end_learning_rate, 
        power=0.5) 
 
    model.compile(optimizer=keras.optimizers.SGD( 
                      learning_rate=learning_rate_fn), 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy']) 
 
    model.fit(data, labels, epochs=5) 
    ``` 
 
    The learning rate schedule is also serializable and deserializable using 
    `keras.optimizers.schedules.serialize` and 
    `keras.optimizers.schedules.deserialize`. 
 
    Args: 
        initial_learning_rate: A Python float. The initial learning rate. 
        decay_steps: A Python integer. Must be positive. See the decay 
            computation above. 
        end_learning_rate: A Python float. The minimal end learning rate. 
        power: A Python float. The power of the polynomial. Defaults to 
            `1.0`. 
        cycle: A boolean, whether it should cycle beyond decay_steps. 
        name: String.  Optional name of the operation. Defaults to 
            `&quot;PolynomialDecay&quot;`. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as `initial_learning_rate`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">initial_learning_rate</span><span class="s3">,</span>
        <span class="s1">decay_steps</span><span class="s3">,</span>
        <span class="s1">end_learning_rate</span><span class="s3">=</span><span class="s5">0.0001</span><span class="s3">,</span>
        <span class="s1">power</span><span class="s3">=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">cycle</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
        <span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;PolynomialDecay&quot;</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">initial_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">decay_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">end_learning_rate </span><span class="s3">= </span><span class="s1">end_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">power </span><span class="s3">= </span><span class="s1">power</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">cycle </span><span class="s3">= </span><span class="s1">cycle</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;Argument `decay_steps` must be &gt; 0. &quot;</span>
                <span class="s4">f&quot;Received: decay_steps=</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s2">}</span><span class="s4">&quot;</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span>
                <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span>
            <span class="s3">)</span>
            <span class="s1">dtype </span><span class="s3">= </span><span class="s1">initial_learning_rate</span><span class="s3">.</span><span class="s1">dtype</span>
            <span class="s1">end_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">end_learning_rate</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">power </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">power</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">step</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">decay_steps_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">cycle</span><span class="s3">:</span>
                <span class="s6"># Find the first multiple of decay_steps that is bigger than</span>
                <span class="s6"># global_step. If global_step is zero set the multiplier to 1</span>
                <span class="s1">multiplier </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">where</span><span class="s3">(</span>
                    <span class="s1">ops</span><span class="s3">.</span><span class="s1">equal</span><span class="s3">(</span><span class="s1">global_step_recomp</span><span class="s3">, </span><span class="s5">0</span><span class="s3">),</span>
                    <span class="s5">1.0</span><span class="s3">,</span>
                    <span class="s1">ops</span><span class="s3">.</span><span class="s1">ceil</span><span class="s3">(</span><span class="s1">global_step_recomp </span><span class="s3">/ </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">),</span>
                <span class="s3">)</span>
                <span class="s1">decay_steps_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span>
                    <span class="s1">decay_steps_recomp</span><span class="s3">, </span><span class="s1">multiplier</span>
                <span class="s3">)</span>
            <span class="s2">else</span><span class="s3">:</span>
                <span class="s6"># Make sure that the global_step used is not bigger than</span>
                <span class="s6"># decay_steps.</span>
                <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">minimum</span><span class="s3">(</span>
                    <span class="s1">global_step_recomp</span><span class="s3">, </span><span class="s1">decay_steps_recomp</span>
                <span class="s3">)</span>

            <span class="s1">p </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">divide</span><span class="s3">(</span><span class="s1">global_step_recomp</span><span class="s3">, </span><span class="s1">decay_steps_recomp</span><span class="s3">)</span>
            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">add</span><span class="s3">(</span>
                <span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span>
                    <span class="s1">initial_learning_rate </span><span class="s3">- </span><span class="s1">end_learning_rate</span><span class="s3">,</span>
                    <span class="s1">ops</span><span class="s3">.</span><span class="s1">power</span><span class="s3">(</span><span class="s5">1 </span><span class="s3">- </span><span class="s1">p</span><span class="s3">, </span><span class="s1">power</span><span class="s3">),</span>
                <span class="s3">),</span>
                <span class="s1">end_learning_rate</span><span class="s3">,</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;initial_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;decay_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">,</span>
            <span class="s4">&quot;end_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">end_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;power&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">power</span><span class="s3">,</span>
            <span class="s4">&quot;cycle&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">cycle</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.InverseTimeDecay&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">InverseTimeDecay</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses an inverse time decay schedule. 
 
    When training a model, it is often useful to lower the learning rate as 
    the training progresses. This schedule applies the inverse decay function 
    to an optimizer step, given a provided initial learning rate. 
    It requires a `step` value to compute the decayed learning rate. You can 
    just pass a backend variable that you increment at each training step. 
 
    The schedule is a 1-arg callable that produces a decayed learning 
    rate when passed the current optimizer step. This can be useful for changing 
    the learning rate value across different invocations of optimizer functions. 
    It is computed as: 
 
    ```python 
    def decayed_learning_rate(step): 
        return initial_learning_rate / (1 + decay_rate * step / decay_step) 
    ``` 
 
    or, if `staircase` is `True`, as: 
 
    ```python 
    def decayed_learning_rate(step): 
        return initial_learning_rate / 
               (1 + decay_rate * floor(step / decay_step)) 
    ``` 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. 
    Example: Fit a Keras model when decaying 1/t with a rate of 0.5: 
 
    ```python 
    ... 
    initial_learning_rate = 0.1 
    decay_steps = 1.0 
    decay_rate = 0.5 
    learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay( 
        initial_learning_rate, decay_steps, decay_rate) 
 
    model.compile(optimizer=keras.optimizers.SGD( 
                      learning_rate=learning_rate_fn), 
                  loss='sparse_categorical_crossentropy', 
                  metrics=['accuracy']) 
 
    model.fit(data, labels, epochs=5) 
    ``` 
 
    Args: 
        initial_learning_rate: A Python float. The initial learning rate. 
        decay_steps: How often to apply decay. 
        decay_rate: A Python number.  The decay rate. 
        staircase: Whether to apply decay in a discrete staircase, as o 
        pposed to continuous, fashion. 
        name: String.  Optional name of the operation.  Defaults to 
            `&quot;InverseTimeDecay&quot;`. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as `initial_learning_rate`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">initial_learning_rate</span><span class="s3">,</span>
        <span class="s1">decay_steps</span><span class="s3">,</span>
        <span class="s1">decay_rate</span><span class="s3">,</span>
        <span class="s1">staircase</span><span class="s3">=</span><span class="s2">False</span><span class="s3">,</span>
        <span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;InverseTimeDecay&quot;</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">initial_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">decay_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate </span><span class="s3">= </span><span class="s1">decay_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">staircase </span><span class="s3">= </span><span class="s1">staircase</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;Argument `decay_steps` must be &gt; 0. &quot;</span>
                <span class="s4">f&quot;Received: decay_steps=</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s2">}</span><span class="s4">&quot;</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span>
                <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span>
            <span class="s3">)</span>
            <span class="s1">dtype </span><span class="s3">= </span><span class="s1">initial_learning_rate</span><span class="s3">.</span><span class="s1">dtype</span>
            <span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">decay_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">step</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">p </span><span class="s3">= </span><span class="s1">global_step_recomp </span><span class="s3">/ </span><span class="s1">decay_steps</span>
            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">staircase</span><span class="s3">:</span>
                <span class="s1">p </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span><span class="s1">p</span><span class="s3">)</span>
            <span class="s1">const </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">ops</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s5">1</span><span class="s3">), </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">denom </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">add</span><span class="s3">(</span><span class="s1">const</span><span class="s3">, </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span><span class="s1">decay_rate</span><span class="s3">, </span><span class="s1">p</span><span class="s3">))</span>
            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">divide</span><span class="s3">(</span><span class="s1">initial_learning_rate</span><span class="s3">, </span><span class="s1">denom</span><span class="s3">)</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;initial_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;decay_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">,</span>
            <span class="s4">&quot;decay_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_rate</span><span class="s3">,</span>
            <span class="s4">&quot;staircase&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">staircase</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.CosineDecay&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">CosineDecay</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses a cosine decay with optional warmup. 
 
    See [Loshchilov &amp; Hutter, ICLR2016](https://arxiv.org/abs/1608.03983), 
    SGDR: Stochastic Gradient Descent with Warm Restarts. 
 
    For the idea of a linear warmup of our learning rate, 
    see [Goyal et al.](https://arxiv.org/pdf/1706.02677.pdf). 
 
    When we begin training a model, we often want an initial increase in our 
    learning rate followed by a decay. If `warmup_target` is an int, this 
    schedule applies a linear increase per optimizer step to our learning rate 
    from `initial_learning_rate` to `warmup_target` for a duration of 
    `warmup_steps`. Afterwards, it applies a cosine decay function taking our 
    learning rate from `warmup_target` to `alpha` for a duration of 
    `decay_steps`. If `warmup_target` is None we skip warmup and our decay 
    will take our learning rate from `initial_learning_rate` to `alpha`. 
    It requires a `step` value to  compute the learning rate. You can 
    just pass a backend variable that you increment at each training step. 
 
    The schedule is a 1-arg callable that produces a warmup followed by a 
    decayed learning rate when passed the current optimizer step. This can be 
    useful for changing the learning rate value across different invocations of 
    optimizer functions. 
 
    Our warmup is computed as: 
 
    ```python 
    def warmup_learning_rate(step): 
        completed_fraction = step / warmup_steps 
        total_delta = target_warmup - initial_learning_rate 
        return completed_fraction * total_delta 
    ``` 
 
    And our decay is computed as: 
 
    ```python 
    if warmup_target is None: 
        initial_decay_lr = initial_learning_rate 
    else: 
        initial_decay_lr = warmup_target 
 
    def decayed_learning_rate(step): 
        step = min(step, decay_steps) 
        cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps)) 
        decayed = (1 - alpha) * cosine_decay + alpha 
        return initial_decay_lr * decayed 
    ``` 
 
    Example usage without warmup: 
 
    ```python 
    decay_steps = 1000 
    initial_learning_rate = 0.1 
    lr_decayed_fn = keras.optimizers.schedules.CosineDecay( 
        initial_learning_rate, decay_steps) 
    ``` 
 
    Example usage with warmup: 
 
    ```python 
    decay_steps = 1000 
    initial_learning_rate = 0 
    warmup_steps = 1000 
    target_learning_rate = 0.1 
    lr_warmup_decayed_fn = keras.optimizers.schedules.CosineDecay( 
        initial_learning_rate, decay_steps, warmup_target=target_learning_rate, 
        warmup_steps=warmup_steps 
    ) 
    ``` 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. The learning rate schedule is also serializable and 
    deserializable using `keras.optimizers.schedules.serialize` and 
    `keras.optimizers.schedules.deserialize`. 
 
    Args: 
        initial_learning_rate: A Python float. The initial learning rate. 
        decay_steps: A Python int. Number of steps to decay over. 
        alpha: A Python float. Minimum learning rate value for decay as a 
            fraction of `initial_learning_rate`. 
        name: String. Optional name of the operation.  Defaults to 
            `&quot;CosineDecay&quot;`. 
        warmup_target: A Python float. The target learning rate for our 
            warmup phase. Will cast to the `initial_learning_rate` datatype. 
            Setting to `None` will skip warmup and begins decay phase from 
            `initial_learning_rate`. Otherwise scheduler will warmup from 
            `initial_learning_rate` to `warmup_target`. 
        warmup_steps: A Python int. Number of steps to warmup over. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as `initial_learning_rate`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">initial_learning_rate</span><span class="s3">,</span>
        <span class="s1">decay_steps</span><span class="s3">,</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0</span><span class="s3">,</span>
        <span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;CosineDecay&quot;</span><span class="s3">,</span>
        <span class="s1">warmup_target</span><span class="s3">=</span><span class="s2">None</span><span class="s3">,</span>
        <span class="s1">warmup_steps</span><span class="s3">=</span><span class="s5">0</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">initial_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">decay_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_steps </span><span class="s3">= </span><span class="s1">warmup_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_target </span><span class="s3">= </span><span class="s1">warmup_target</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;Argument `decay_steps` must be &gt; 0. &quot;</span>
                <span class="s4">f&quot;Received: decay_steps=</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s2">}</span><span class="s4">&quot;</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">_decay_function</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">, </span><span class="s1">decay_steps</span><span class="s3">, </span><span class="s1">decay_from_lr</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">completed_fraction </span><span class="s3">= </span><span class="s1">step </span><span class="s3">/ </span><span class="s1">decay_steps</span>
            <span class="s1">pi </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">math</span><span class="s3">.</span><span class="s1">pi</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">cosine_decayed </span><span class="s3">= </span><span class="s5">0.5 </span><span class="s3">* (</span><span class="s5">1.0 </span><span class="s3">+ </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cos</span><span class="s3">(</span><span class="s1">pi </span><span class="s3">* </span><span class="s1">completed_fraction</span><span class="s3">))</span>
            <span class="s1">decayed </span><span class="s3">= (</span><span class="s5">1 </span><span class="s3">- </span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span><span class="s3">) * </span><span class="s1">cosine_decayed </span><span class="s3">+ </span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span>
            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span><span class="s1">decay_from_lr</span><span class="s3">, </span><span class="s1">decayed</span><span class="s3">)</span>

    <span class="s2">def </span><span class="s1">_warmup_function</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">, </span><span class="s1">warmup_steps</span><span class="s3">, </span><span class="s1">warmup_target</span><span class="s3">, </span><span class="s1">initial_learning_rate</span>
    <span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">completed_fraction </span><span class="s3">= </span><span class="s1">step </span><span class="s3">/ </span><span class="s1">warmup_steps</span>
            <span class="s1">total_step_delta </span><span class="s3">= </span><span class="s1">warmup_target </span><span class="s3">- </span><span class="s1">initial_learning_rate</span>
            <span class="s2">return </span><span class="s1">total_step_delta </span><span class="s3">* </span><span class="s1">completed_fraction </span><span class="s3">+ </span><span class="s1">initial_learning_rate</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span>
                <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span>
            <span class="s3">)</span>
            <span class="s1">dtype </span><span class="s3">= </span><span class="s1">initial_learning_rate</span><span class="s3">.</span><span class="s1">dtype</span>
            <span class="s1">decay_steps </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">step</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_target </span><span class="s2">is None</span><span class="s3">:</span>
                <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">minimum</span><span class="s3">(</span>
                    <span class="s1">global_step_recomp</span><span class="s3">, </span><span class="s1">decay_steps</span>
                <span class="s3">)</span>
                <span class="s2">return </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_decay_function</span><span class="s3">(</span>
                    <span class="s1">global_step_recomp</span><span class="s3">,</span>
                    <span class="s1">decay_steps</span><span class="s3">,</span>
                    <span class="s1">initial_learning_rate</span><span class="s3">,</span>
                    <span class="s1">dtype</span><span class="s3">,</span>
                <span class="s3">)</span>

            <span class="s1">warmup_target </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_target</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">warmup_steps </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">minimum</span><span class="s3">(</span>
                <span class="s1">global_step_recomp</span><span class="s3">, </span><span class="s1">decay_steps </span><span class="s3">+ </span><span class="s1">warmup_steps</span>
            <span class="s3">)</span>

            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cond</span><span class="s3">(</span>
                <span class="s1">global_step_recomp </span><span class="s3">&lt; </span><span class="s1">warmup_steps</span><span class="s3">,</span>
                <span class="s2">lambda</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_warmup_function</span><span class="s3">(</span>
                    <span class="s1">global_step_recomp</span><span class="s3">,</span>
                    <span class="s1">warmup_steps</span><span class="s3">,</span>
                    <span class="s1">warmup_target</span><span class="s3">,</span>
                    <span class="s1">initial_learning_rate</span><span class="s3">,</span>
                <span class="s3">),</span>
                <span class="s2">lambda</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_decay_function</span><span class="s3">(</span>
                    <span class="s1">global_step_recomp </span><span class="s3">- </span><span class="s1">warmup_steps</span><span class="s3">,</span>
                    <span class="s1">decay_steps</span><span class="s3">,</span>
                    <span class="s1">warmup_target</span><span class="s3">,</span>
                    <span class="s1">dtype</span><span class="s3">,</span>
                <span class="s3">),</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;initial_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;decay_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">decay_steps</span><span class="s3">,</span>
            <span class="s4">&quot;alpha&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
            <span class="s4">&quot;warmup_target&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_target</span><span class="s3">,</span>
            <span class="s4">&quot;warmup_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">warmup_steps</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.CosineDecayRestarts&quot;</span><span class="s3">)</span>
<span class="s2">class </span><span class="s1">CosineDecayRestarts</span><span class="s3">(</span><span class="s1">LearningRateSchedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;A `LearningRateSchedule` that uses a cosine decay schedule with restarts. 
 
    See [Loshchilov &amp; Hutter, ICLR2016](https://arxiv.org/abs/1608.03983), 
    SGDR: Stochastic Gradient Descent with Warm Restarts. 
 
    When training a model, it is often useful to lower the learning rate as 
    the training progresses. This schedule applies a cosine decay function with 
    restarts to an optimizer step, given a provided initial learning rate. 
    It requires a `step` value to compute the decayed learning rate. You can 
    just pass a backend variable that you increment at each training step. 
 
    The schedule is a 1-arg callable that produces a decayed learning 
    rate when passed the current optimizer step. This can be useful for changing 
    the learning rate value across different invocations of optimizer functions. 
 
    The learning rate multiplier first decays 
    from 1 to `alpha` for `first_decay_steps` steps. Then, a warm 
    restart is performed. Each new warm restart runs for `t_mul` times more 
    steps and with `m_mul` times initial learning rate as the new learning rate. 
 
    Example: 
    ```python 
    first_decay_steps = 1000 
    lr_decayed_fn = ( 
        keras.optimizers.schedules.CosineDecayRestarts( 
            initial_learning_rate, 
            first_decay_steps)) 
    ``` 
 
    You can pass this schedule directly into a `keras.optimizers.Optimizer` 
    as the learning rate. The learning rate schedule is also serializable and 
    deserializable using `keras.optimizers.schedules.serialize` and 
    `keras.optimizers.schedules.deserialize`. 
 
    Args: 
        initial_learning_rate: A Python float. The initial learning rate. 
        first_decay_steps: A Python integer. Number of steps to decay over. 
        t_mul: A Python float. Used to derive the number of iterations in 
            the i-th period. 
        m_mul: A Python float. Used to derive the initial learning rate of 
            the i-th period. 
        alpha: A Python float. Minimum learning rate value as a fraction of 
            the `initial_learning_rate`. 
        name: String. Optional name of the operation. Defaults to 
            `&quot;SGDRDecay&quot;`. 
 
    Returns: 
        A 1-arg callable learning rate schedule that takes the current optimizer 
        step and outputs the decayed learning rate, a scalar tensor of the 
        same type as `initial_learning_rate`. 
    &quot;&quot;&quot;</span>

    <span class="s2">def </span><span class="s1">__init__</span><span class="s3">(</span>
        <span class="s1">self</span><span class="s3">,</span>
        <span class="s1">initial_learning_rate</span><span class="s3">,</span>
        <span class="s1">first_decay_steps</span><span class="s3">,</span>
        <span class="s1">t_mul</span><span class="s3">=</span><span class="s5">2.0</span><span class="s3">,</span>
        <span class="s1">m_mul</span><span class="s3">=</span><span class="s5">1.0</span><span class="s3">,</span>
        <span class="s1">alpha</span><span class="s3">=</span><span class="s5">0.0</span><span class="s3">,</span>
        <span class="s1">name</span><span class="s3">=</span><span class="s4">&quot;SGDRDecay&quot;</span><span class="s3">,</span>
    <span class="s3">):</span>
        <span class="s1">super</span><span class="s3">().</span><span class="s1">__init__</span><span class="s3">()</span>

        <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">initial_learning_rate</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">first_decay_steps </span><span class="s3">= </span><span class="s1">first_decay_steps</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">_t_mul </span><span class="s3">= </span><span class="s1">t_mul</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">_m_mul </span><span class="s3">= </span><span class="s1">m_mul</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">alpha </span><span class="s3">= </span><span class="s1">alpha</span>
        <span class="s1">self</span><span class="s3">.</span><span class="s1">name </span><span class="s3">= </span><span class="s1">name</span>

        <span class="s2">if </span><span class="s1">self</span><span class="s3">.</span><span class="s1">first_decay_steps </span><span class="s3">&lt;= </span><span class="s5">0</span><span class="s3">:</span>
            <span class="s2">raise </span><span class="s1">ValueError</span><span class="s3">(</span>
                <span class="s4">&quot;Argument `first_decay_steps` must be &gt; 0. &quot;</span>
                <span class="s4">f&quot;Received: first_decay_steps=</span><span class="s2">{</span><span class="s1">self</span><span class="s3">.</span><span class="s1">first_decay_steps</span><span class="s2">}</span><span class="s4">&quot;</span>
            <span class="s3">)</span>

    <span class="s2">def </span><span class="s1">__call__</span><span class="s3">(</span><span class="s1">self</span><span class="s3">, </span><span class="s1">step</span><span class="s3">):</span>
        <span class="s2">with </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">name_scope</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">):</span>
            <span class="s1">initial_learning_rate </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">convert_to_tensor</span><span class="s3">(</span>
                <span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span>
            <span class="s3">)</span>
            <span class="s1">dtype </span><span class="s3">= </span><span class="s1">initial_learning_rate</span><span class="s3">.</span><span class="s1">dtype</span>
            <span class="s1">first_decay_steps </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">first_decay_steps</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">alpha </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">t_mul </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">_t_mul</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">m_mul </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">self</span><span class="s3">.</span><span class="s1">_m_mul</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>

            <span class="s1">global_step_recomp </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span><span class="s1">step</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">)</span>
            <span class="s1">completed_fraction </span><span class="s3">= </span><span class="s1">global_step_recomp </span><span class="s3">/ </span><span class="s1">first_decay_steps</span>

            <span class="s2">def </span><span class="s1">compute_step</span><span class="s3">(</span><span class="s1">completed_fraction</span><span class="s3">, </span><span class="s1">geometric</span><span class="s3">=</span><span class="s2">False</span><span class="s3">):</span>
                <span class="s0">&quot;&quot;&quot;Helper for `cond` operation.&quot;&quot;&quot;</span>
                <span class="s2">if </span><span class="s1">geometric</span><span class="s3">:</span>
                    <span class="s6"># ops.log is sensitive to the precision of dtype, so we need</span>
                    <span class="s6"># the additional casting</span>
                    <span class="s1">i_restart </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span>
                        <span class="s1">ops</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span>
                            <span class="s1">ops</span><span class="s3">.</span><span class="s1">cast</span><span class="s3">(</span>
                                <span class="s5">1.0 </span><span class="s3">- </span><span class="s1">completed_fraction </span><span class="s3">* (</span><span class="s5">1.0 </span><span class="s3">- </span><span class="s1">t_mul</span><span class="s3">), </span><span class="s1">dtype</span>
                            <span class="s3">)</span>
                        <span class="s3">)</span>
                        <span class="s3">/ </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">log</span><span class="s3">(</span><span class="s1">t_mul</span><span class="s3">)</span>
                    <span class="s3">)</span>

                    <span class="s1">sum_r </span><span class="s3">= (</span><span class="s5">1.0 </span><span class="s3">- </span><span class="s1">t_mul</span><span class="s3">**</span><span class="s1">i_restart</span><span class="s3">) / (</span><span class="s5">1.0 </span><span class="s3">- </span><span class="s1">t_mul</span><span class="s3">)</span>
                    <span class="s1">completed_fraction </span><span class="s3">= (</span>
                        <span class="s1">completed_fraction </span><span class="s3">- </span><span class="s1">sum_r</span>
                    <span class="s3">) / </span><span class="s1">t_mul</span><span class="s3">**</span><span class="s1">i_restart</span>

                <span class="s2">else</span><span class="s3">:</span>
                    <span class="s1">i_restart </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">floor</span><span class="s3">(</span><span class="s1">completed_fraction</span><span class="s3">)</span>
                    <span class="s1">completed_fraction </span><span class="s3">-= </span><span class="s1">i_restart</span>

                <span class="s2">return </span><span class="s1">i_restart</span><span class="s3">, </span><span class="s1">completed_fraction</span>

            <span class="s1">i_restart</span><span class="s3">, </span><span class="s1">completed_fraction </span><span class="s3">= </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cond</span><span class="s3">(</span>
                <span class="s1">ops</span><span class="s3">.</span><span class="s1">equal</span><span class="s3">(</span><span class="s1">t_mul</span><span class="s3">, </span><span class="s5">1.0</span><span class="s3">),</span>
                <span class="s2">lambda</span><span class="s3">: </span><span class="s1">compute_step</span><span class="s3">(</span><span class="s1">completed_fraction</span><span class="s3">, </span><span class="s1">geometric</span><span class="s3">=</span><span class="s2">False</span><span class="s3">),</span>
                <span class="s2">lambda</span><span class="s3">: </span><span class="s1">compute_step</span><span class="s3">(</span><span class="s1">completed_fraction</span><span class="s3">, </span><span class="s1">geometric</span><span class="s3">=</span><span class="s2">True</span><span class="s3">),</span>
            <span class="s3">)</span>

            <span class="s1">m_fac </span><span class="s3">= </span><span class="s1">m_mul</span><span class="s3">**</span><span class="s1">i_restart</span>
            <span class="s1">cosine_decayed </span><span class="s3">= (</span>
                <span class="s5">0.5</span>
                <span class="s3">* </span><span class="s1">m_fac</span>
                <span class="s3">* (</span>
                    <span class="s5">1.0</span>
                    <span class="s3">+ </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">cos</span><span class="s3">(</span>
                        <span class="s1">ops</span><span class="s3">.</span><span class="s1">array</span><span class="s3">(</span><span class="s1">math</span><span class="s3">.</span><span class="s1">pi</span><span class="s3">, </span><span class="s1">dtype</span><span class="s3">=</span><span class="s1">dtype</span><span class="s3">) * </span><span class="s1">completed_fraction</span>
                    <span class="s3">)</span>
                <span class="s3">)</span>
            <span class="s3">)</span>
            <span class="s1">decayed </span><span class="s3">= (</span><span class="s5">1 </span><span class="s3">- </span><span class="s1">alpha</span><span class="s3">) * </span><span class="s1">cosine_decayed </span><span class="s3">+ </span><span class="s1">alpha</span>

            <span class="s2">return </span><span class="s1">ops</span><span class="s3">.</span><span class="s1">multiply</span><span class="s3">(</span><span class="s1">initial_learning_rate</span><span class="s3">, </span><span class="s1">decayed</span><span class="s3">)</span>

    <span class="s2">def </span><span class="s1">get_config</span><span class="s3">(</span><span class="s1">self</span><span class="s3">):</span>
        <span class="s2">return </span><span class="s3">{</span>
            <span class="s4">&quot;initial_learning_rate&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">initial_learning_rate</span><span class="s3">,</span>
            <span class="s4">&quot;first_decay_steps&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">first_decay_steps</span><span class="s3">,</span>
            <span class="s4">&quot;t_mul&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_t_mul</span><span class="s3">,</span>
            <span class="s4">&quot;m_mul&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">_m_mul</span><span class="s3">,</span>
            <span class="s4">&quot;alpha&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">alpha</span><span class="s3">,</span>
            <span class="s4">&quot;name&quot;</span><span class="s3">: </span><span class="s1">self</span><span class="s3">.</span><span class="s1">name</span><span class="s3">,</span>
        <span class="s3">}</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.serialize&quot;</span><span class="s3">)</span>
<span class="s2">def </span><span class="s1">serialize</span><span class="s3">(</span><span class="s1">learning_rate_schedule</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Serializes a `LearningRateSchedule` into a JSON-compatible dict. 
 
    Args: 
        learning_rate_schedule: The `LearningRateSchedule` object to serialize. 
 
    Returns: 
        A JSON-serializable dict representing the object's config. 
 
    Example: 
 
    &gt;&gt;&gt; lr_schedule = keras.optimizers.schedules.ExponentialDecay( 
    ...     0.1, decay_steps=100000, decay_rate=0.96, staircase=True) 
    &gt;&gt;&gt; keras.optimizers.schedules.serialize(lr_schedule) 
    {'module': 'keras.optimizers.schedules', 
    'class_name': 'ExponentialDecay', 'config': {...}, 
    'registered_name': None} 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">serialization_lib</span><span class="s3">.</span><span class="s1">serialize_keras_object</span><span class="s3">(</span><span class="s1">learning_rate_schedule</span><span class="s3">)</span>


<span class="s3">@</span><span class="s1">keras_export</span><span class="s3">(</span><span class="s4">&quot;keras.optimizers.schedules.deserialize&quot;</span><span class="s3">)</span>
<span class="s2">def </span><span class="s1">deserialize</span><span class="s3">(</span><span class="s1">config</span><span class="s3">, </span><span class="s1">custom_objects</span><span class="s3">=</span><span class="s2">None</span><span class="s3">):</span>
    <span class="s0">&quot;&quot;&quot;Instantiates a `LearningRateSchedule` object from a serialized form. 
 
    Args: 
        config: The serialized form of the `LearningRateSchedule`. Dictionary of 
            the form {'class_name': str, 'config': dict}. 
        custom_objects: A dictionary mapping class names (or function names) of 
            custom (non-Keras) objects to class/functions. 
 
    Returns: 
        A `LearningRateSchedule` object. 
 
    Example: 
 
    ```python 
    # Configuration for PolynomialDecay 
    config = { 
        'class_name': 'PolynomialDecay', 
        'config': {'cycle': False, 
            'decay_steps': 10000, 
            'end_learning_rate': 0.01, 
            'initial_learning_rate': 0.1, 
            'name': None, 
            'power': 0.5 
        } 
    } 
    lr_schedule = keras.optimizers.schedules.deserialize(config) 
    ``` 
    &quot;&quot;&quot;</span>
    <span class="s2">return </span><span class="s1">serialization_lib</span><span class="s3">.</span><span class="s1">deserialize_keras_object</span><span class="s3">(</span>
        <span class="s1">config</span><span class="s3">,</span>
        <span class="s1">module_objects</span><span class="s3">=</span><span class="s1">globals</span><span class="s3">(),</span>
        <span class="s1">custom_objects</span><span class="s3">=</span><span class="s1">custom_objects</span><span class="s3">,</span>
        <span class="s1">printable_module_name</span><span class="s3">=</span><span class="s4">&quot;decay&quot;</span><span class="s3">,</span>
    <span class="s3">)</span>
</pre>
</body>
</html>