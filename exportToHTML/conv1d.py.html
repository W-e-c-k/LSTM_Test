<html>
<head>
<title>conv1d.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #2aacb8;}
.s6 { color: #7a7e85;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
conv1d.py</font>
</center></td></tr></table>
<pre><span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">layers</span><span class="s2">.</span><span class="s1">convolutional</span><span class="s2">.</span><span class="s1">base_conv </span><span class="s0">import </span><span class="s1">BaseConv</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">([</span><span class="s3">&quot;keras.layers.Conv1D&quot;</span><span class="s2">, </span><span class="s3">&quot;keras.layers.Convolution1D&quot;</span><span class="s2">])</span>
<span class="s0">class </span><span class="s1">Conv1D</span><span class="s2">(</span><span class="s1">BaseConv</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;1D convolution layer (e.g. temporal convolution). 
 
    This layer creates a convolution kernel that is convolved with the layer 
    input over a single spatial (or temporal) dimension to produce a tensor of 
    outputs. If `use_bias` is True, a bias vector is created and added to the 
    outputs. Finally, if `activation` is not `None`, it is applied to the 
    outputs as well. 
 
    Args: 
        filters: int, the dimension of the output space (the number of filters 
            in the convolution). 
        kernel_size: int or tuple/list of 1 integer, specifying the size of the 
            convolution window. 
        strides: int or tuple/list of 1 integer, specifying the stride length 
            of the convolution. `strides &gt; 1` is incompatible with 
            `dilation_rate &gt; 1`. 
        padding: string, `&quot;valid&quot;`, `&quot;same&quot;` or `&quot;causal&quot;`(case-insensitive). 
            `&quot;valid&quot;` means no padding. `&quot;same&quot;` results in padding evenly to 
            the left/right or up/down of the input. When `padding=&quot;same&quot;` and 
            `strides=1`, the output has the same size as the input. 
            `&quot;causal&quot;` results in causal(dilated) convolutions, e.g. `output[t]` 
            does not depend on`input[t+1:]`. Useful when modeling temporal data 
            where the model should not violate the temporal order. 
            See [WaveNet: A Generative Model for Raw Audio, section2.1]( 
            https://arxiv.org/abs/1609.03499). 
        data_format: string, either `&quot;channels_last&quot;` or `&quot;channels_first&quot;`. 
            The ordering of the dimensions in the inputs. `&quot;channels_last&quot;` 
            corresponds to inputs with shape `(batch, steps, features)` 
            while `&quot;channels_first&quot;` corresponds to inputs with shape 
            `(batch, features, steps)`. It defaults to the `image_data_format` 
            value found in your Keras config file at `~/.keras/keras.json`. 
            If you never set it, then it will be `&quot;channels_last&quot;`. 
        dilation_rate: int or tuple/list of 1 integers, specifying the dilation 
            rate to use for dilated convolution. 
        groups: A positive int specifying the number of groups in which the 
            input is split along the channel axis. Each group is convolved 
            separately with `filters // groups` filters. The output is the 
            concatenation of all the `groups` results along the channel axis. 
            Input channels and `filters` must both be divisible by `groups`. 
        activation: Activation function. If `None`, no activation is applied. 
        use_bias: bool, if `True`, bias will be added to the output. 
        kernel_initializer: Initializer for the convolution kernel. If `None`, 
            the default initializer (`&quot;glorot_uniform&quot;`) will be used. 
        bias_initializer: Initializer for the bias vector. If `None`, the 
            default initializer (`&quot;zeros&quot;`) will be used. 
        kernel_regularizer: Optional regularizer for the convolution kernel. 
        bias_regularizer: Optional regularizer for the bias vector. 
        activity_regularizer: Optional regularizer function for the output. 
        kernel_constraint: Optional projection function to be applied to the 
            kernel after being updated by an `Optimizer` (e.g. used to implement 
            norm constraints or value constraints for layer weights). The 
            function must take as input the unprojected variable and must return 
            the projected variable (which must have the same shape). Constraints 
            are not safe to use when doing asynchronous distributed training. 
        bias_constraint: Optional projection function to be applied to the 
            bias after being updated by an `Optimizer`. 
 
    Input shape: 
 
    - If `data_format=&quot;channels_last&quot;`: 
        A 3D tensor with shape: `(batch_shape, steps, channels)` 
    - If `data_format=&quot;channels_first&quot;`: 
        A 3D tensor with shape: `(batch_shape, channels, steps)` 
 
    Output shape: 
 
    - If `data_format=&quot;channels_last&quot;`: 
        A 3D tensor with shape: `(batch_shape, new_steps, filters)` 
    - If `data_format=&quot;channels_first&quot;`: 
        A 3D tensor with shape: `(batch_shape, filters, new_steps)` 
 
    Returns: 
        A 3D tensor representing `activation(conv1d(inputs, kernel) + bias)`. 
 
    Raises: 
        ValueError: when both `strides &gt; 1` and `dilation_rate &gt; 1`. 
 
    Example: 
 
    &gt;&gt;&gt; # The inputs are 128-length vectors with 10 timesteps, and the 
    &gt;&gt;&gt; # batch size is 4. 
    &gt;&gt;&gt; x = np.random.rand(4, 10, 128) 
    &gt;&gt;&gt; y = keras.layers.Conv1D(32, 3, activation='relu')(x) 
    &gt;&gt;&gt; print(y.shape) 
    (4, 8, 32) 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">filters</span><span class="s2">,</span>
        <span class="s1">kernel_size</span><span class="s2">,</span>
        <span class="s1">strides</span><span class="s2">=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">padding</span><span class="s2">=</span><span class="s3">&quot;valid&quot;</span><span class="s2">,</span>
        <span class="s1">data_format</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">groups</span><span class="s2">=</span><span class="s5">1</span><span class="s2">,</span>
        <span class="s1">activation</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">use_bias</span><span class="s2">=</span><span class="s0">True</span><span class="s2">,</span>
        <span class="s1">kernel_initializer</span><span class="s2">=</span><span class="s3">&quot;glorot_uniform&quot;</span><span class="s2">,</span>
        <span class="s1">bias_initializer</span><span class="s2">=</span><span class="s3">&quot;zeros&quot;</span><span class="s2">,</span>
        <span class="s1">kernel_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">kernel_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s1">bias_constraint</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
        <span class="s2">**</span><span class="s1">kwargs</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span>
            <span class="s1">rank</span><span class="s2">=</span><span class="s5">1</span><span class="s2">,</span>
            <span class="s1">filters</span><span class="s2">=</span><span class="s1">filters</span><span class="s2">,</span>
            <span class="s1">kernel_size</span><span class="s2">=</span><span class="s1">kernel_size</span><span class="s2">,</span>
            <span class="s1">strides</span><span class="s2">=</span><span class="s1">strides</span><span class="s2">,</span>
            <span class="s1">padding</span><span class="s2">=</span><span class="s1">padding</span><span class="s2">,</span>
            <span class="s1">data_format</span><span class="s2">=</span><span class="s1">data_format</span><span class="s2">,</span>
            <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s1">dilation_rate</span><span class="s2">,</span>
            <span class="s1">groups</span><span class="s2">=</span><span class="s1">groups</span><span class="s2">,</span>
            <span class="s1">activation</span><span class="s2">=</span><span class="s1">activation</span><span class="s2">,</span>
            <span class="s1">use_bias</span><span class="s2">=</span><span class="s1">use_bias</span><span class="s2">,</span>
            <span class="s1">kernel_initializer</span><span class="s2">=</span><span class="s1">kernel_initializer</span><span class="s2">,</span>
            <span class="s1">bias_initializer</span><span class="s2">=</span><span class="s1">bias_initializer</span><span class="s2">,</span>
            <span class="s1">kernel_regularizer</span><span class="s2">=</span><span class="s1">kernel_regularizer</span><span class="s2">,</span>
            <span class="s1">bias_regularizer</span><span class="s2">=</span><span class="s1">bias_regularizer</span><span class="s2">,</span>
            <span class="s1">activity_regularizer</span><span class="s2">=</span><span class="s1">activity_regularizer</span><span class="s2">,</span>
            <span class="s1">kernel_constraint</span><span class="s2">=</span><span class="s1">kernel_constraint</span><span class="s2">,</span>
            <span class="s1">bias_constraint</span><span class="s2">=</span><span class="s1">bias_constraint</span><span class="s2">,</span>
            <span class="s2">**</span><span class="s1">kwargs</span>
        <span class="s2">)</span>

    <span class="s0">def </span><span class="s1">_compute_causal_padding</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">left_pad </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dilation_rate</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] * (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">kernel_size</span><span class="s2">[</span><span class="s5">0</span><span class="s2">] - </span><span class="s5">1</span><span class="s2">)</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">data_format </span><span class="s2">== </span><span class="s3">&quot;channels_last&quot;</span><span class="s2">:</span>
            <span class="s1">causal_padding </span><span class="s2">= [[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">], [</span><span class="s1">left_pad</span><span class="s2">, </span><span class="s5">0</span><span class="s2">], [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">]]</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">causal_padding </span><span class="s2">= [[</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">], [</span><span class="s5">0</span><span class="s2">, </span><span class="s5">0</span><span class="s2">], [</span><span class="s1">left_pad</span><span class="s2">, </span><span class="s5">0</span><span class="s2">]]</span>
        <span class="s0">return </span><span class="s1">causal_padding</span>

    <span class="s0">def </span><span class="s1">call</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">inputs</span><span class="s2">):</span>
        <span class="s1">padding </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">padding</span>
        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">padding </span><span class="s2">== </span><span class="s3">&quot;causal&quot;</span><span class="s2">:</span>
            <span class="s6"># Apply causal padding to inputs.</span>
            <span class="s1">inputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">pad</span><span class="s2">(</span><span class="s1">inputs</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_compute_causal_padding</span><span class="s2">())</span>
            <span class="s1">padding </span><span class="s2">= </span><span class="s3">&quot;valid&quot;</span>

        <span class="s1">outputs </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">conv</span><span class="s2">(</span>
            <span class="s1">inputs</span><span class="s2">,</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">kernel</span><span class="s2">,</span>
            <span class="s1">strides</span><span class="s2">=</span><span class="s1">list</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">strides</span><span class="s2">),</span>
            <span class="s1">padding</span><span class="s2">=</span><span class="s1">padding</span><span class="s2">,</span>
            <span class="s1">dilation_rate</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dilation_rate</span><span class="s2">,</span>
            <span class="s1">data_format</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">data_format</span><span class="s2">,</span>
        <span class="s2">)</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">use_bias</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">data_format </span><span class="s2">== </span><span class="s3">&quot;channels_last&quot;</span><span class="s2">:</span>
                <span class="s1">bias_shape </span><span class="s2">= (</span><span class="s5">1</span><span class="s2">,) * (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">rank </span><span class="s2">+ </span><span class="s5">1</span><span class="s2">) + (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">filters</span><span class="s2">,)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">bias_shape </span><span class="s2">= (</span><span class="s5">1</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">filters</span><span class="s2">) + (</span><span class="s5">1</span><span class="s2">,) * </span><span class="s1">self</span><span class="s2">.</span><span class="s1">rank</span>
            <span class="s1">bias </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">reshape</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">bias</span><span class="s2">, </span><span class="s1">bias_shape</span><span class="s2">)</span>
            <span class="s1">outputs </span><span class="s2">+= </span><span class="s1">bias</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation </span><span class="s0">is not None</span><span class="s2">:</span>
            <span class="s0">return </span><span class="s1">self</span><span class="s2">.</span><span class="s1">activation</span><span class="s2">(</span><span class="s1">outputs</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">outputs</span>
</pre>
</body>
</html>