<html>
<head>
<title>regression_metrics.py</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #cf8e6d;}
.s1 { color: #bcbec4;}
.s2 { color: #bcbec4;}
.s3 { color: #6aab73;}
.s4 { color: #5f826b; font-style: italic;}
.s5 { color: #7a7e85;}
.s6 { color: #2aacb8;}
</style>
</head>
<body bgcolor="#1e1f22">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
regression_metrics.py</font>
</center></td></tr></table>
<pre><span class="s0">import </span><span class="s1">warnings</span>

<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">initializers</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src </span><span class="s0">import </span><span class="s1">ops</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">api_export </span><span class="s0">import </span><span class="s1">keras_export</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">loss </span><span class="s0">import </span><span class="s1">squeeze_or_expand_to_same_rank</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">log_cosh</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">mean_absolute_error</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">mean_absolute_percentage_error</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">mean_squared_error</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">losses</span><span class="s2">.</span><span class="s1">losses </span><span class="s0">import </span><span class="s1">mean_squared_logarithmic_error</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">metrics </span><span class="s0">import </span><span class="s1">reduction_metrics</span>
<span class="s0">from </span><span class="s1">keras</span><span class="s2">.</span><span class="s1">src</span><span class="s2">.</span><span class="s1">utils</span><span class="s2">.</span><span class="s1">numerical_utils </span><span class="s0">import </span><span class="s1">normalize</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.MeanSquaredError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">MeanSquaredError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the mean squared error between `y_true` and `y_pred`. 
 
    Formula: 
 
    ```python 
    loss = mean(square(y_true - y_pred)) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.MeanSquaredError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.25 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;mean_squared_error&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">fn</span><span class="s2">=</span><span class="s1">mean_squared_error</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.MeanAbsoluteError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">MeanAbsoluteError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the mean absolute error between the labels and predictions. 
 
    Formula: 
 
    ```python 
    loss = mean(abs(y_true - y_pred)) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Examples: 
 
    &gt;&gt;&gt; m = keras.metrics.MeanAbsoluteError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.25 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.5 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.MeanAbsoluteError()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;mean_absolute_error&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">mean_absolute_error</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.MeanAbsolutePercentageError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">MeanAbsolutePercentageError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes mean absolute percentage error between `y_true` and `y_pred`. 
 
    Formula: 
 
    ```python 
    loss = 100 * mean(abs((y_true - y_pred) / y_true)) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.MeanAbsolutePercentageError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    250000000.0 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    500000000.0 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.MeanAbsolutePercentageError()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;mean_absolute_percentage_error&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">mean_absolute_percentage_error</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.MeanSquaredLogarithmicError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">MeanSquaredLogarithmicError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes mean squared logarithmic error between `y_true` and `y_pred`. 
 
    Formula: 
 
    ```python 
    loss = mean(square(log(y_true + 1) - log(y_pred + 1))) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.MeanSquaredLogarithmicError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.12011322 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.24022643 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.MeanSquaredLogarithmicError()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;mean_squared_logarithmic_error&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">mean_squared_logarithmic_error</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.RootMeanSquaredError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">RootMeanSquaredError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">Mean</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes root mean squared error metric between `y_true` and `y_pred`. 
 
    Formula: 
 
    ```python 
    loss = sqrt(mean((y_pred - y_true) ** 2)) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.RootMeanSquaredError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.5 
 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.70710677 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.RootMeanSquaredError()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;root_mean_squared_error&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">update_state</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Accumulates root mean squared error statistics. 
 
        Args: 
            y_true: The ground truth values. 
            y_pred: The predicted values. 
            sample_weight: Optional weighting of each example. Can 
                be a `Tensor` whose rank is either 0, or the same rank as 
                `y_true`, and must be broadcastable to `y_true`. 
                Defaults to `1`. 
 
        Returns: 
            Update op. 
        &quot;&quot;&quot;</span>
        <span class="s1">y_true </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_dtype</span><span class="s2">)</span>
        <span class="s1">y_pred </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_dtype</span><span class="s2">)</span>
        <span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred </span><span class="s2">= </span><span class="s1">squeeze_or_expand_to_same_rank</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">)</span>
        <span class="s1">error_sq </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">square</span><span class="s2">(</span><span class="s1">y_pred </span><span class="s2">- </span><span class="s1">y_true</span><span class="s2">)</span>
        <span class="s0">return </span><span class="s1">super</span><span class="s2">().</span><span class="s1">update_state</span><span class="s2">(</span><span class="s1">error_sq</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s1">sample_weight</span><span class="s2">)</span>

    <span class="s0">def </span><span class="s1">result</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sqrt</span><span class="s2">(</span><span class="s1">super</span><span class="s2">().</span><span class="s1">result</span><span class="s2">())</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.CosineSimilarity&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">CosineSimilarity</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the cosine similarity between the labels and predictions. 
 
    Formula: 
 
    ```python 
    loss = sum(l2_norm(y_true) * l2_norm(y_pred)) 
    ``` 
    See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity). 
    This metric keeps the average cosine similarity between `predictions` and 
    `labels` over a stream of data. 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
        axis: (Optional) Defaults to `-1`. The dimension along which the cosine 
            similarity is computed. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]] 
    &gt;&gt;&gt; # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]] 
    &gt;&gt;&gt; # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]] 
    &gt;&gt;&gt; # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1)) 
    &gt;&gt;&gt; #        = ((0. + 0.) +  (0.5 + 0.5)) / 2 
    &gt;&gt;&gt; m = keras.metrics.CosineSimilarity(axis=1) 
    &gt;&gt;&gt; m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]]) 
    &gt;&gt;&gt; m.result() 
    0.49999997 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]], 
    ...                sample_weight=[0.3, 0.7]) 
    &gt;&gt;&gt; m.result() 
    0.6999999 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile( 
        optimizer='sgd', 
        loss='mse', 
        metrics=[keras.metrics.CosineSimilarity(axis=1)]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;cosine_similarity&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s6">1</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">cosine_similarity</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
        <span class="s5"># Metric should be maximized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;up&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.LogCoshError&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">LogCoshError</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">MeanMetricWrapper</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the logarithm of the hyperbolic cosine of the prediction error. 
 
    Formula: 
 
    ```python 
    error = y_pred - y_true 
    logcosh = mean(log((exp(error) + exp(-error))/2), axis=-1) 
    ``` 
 
    Args: 
        name: (Optional) string name of the metric instance. 
        dtype: (Optional) data type of the metric result. 
 
    Example: 
 
    Example: 
 
    &gt;&gt;&gt; m = keras.metrics.LogCoshError() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]]) 
    &gt;&gt;&gt; m.result() 
    0.10844523 
    &gt;&gt;&gt; m.reset_state() 
    &gt;&gt;&gt; m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]], 
    ...                sample_weight=[1, 0]) 
    &gt;&gt;&gt; m.result() 
    0.21689045 
 
    Usage with `compile()` API: 
 
    ```python 
    model.compile(optimizer='sgd', 
                  loss='mse', 
                  metrics=[keras.metrics.LogCoshError()]) 
    ``` 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;logcosh&quot;</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">log_cosh</span><span class="s2">, </span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be minimized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;down&quot;</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">return </span><span class="s2">{</span><span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">, </span><span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">}</span>


<span class="s5"># Adapted from TF-Addons implementation (RSquare class).</span>
<span class="s2">@</span><span class="s1">keras_export</span><span class="s2">(</span><span class="s3">&quot;keras.metrics.R2Score&quot;</span><span class="s2">)</span>
<span class="s0">class </span><span class="s1">R2Score</span><span class="s2">(</span><span class="s1">reduction_metrics</span><span class="s2">.</span><span class="s1">Metric</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes R2 score. 
 
    Formula: 
 
    ```python 
    sum_squares_residuals = sum((y_true - y_pred) ** 2) 
    sum_squares = sum((y_true - mean(y_true)) ** 2) 
    R2 = 1 - sum_squares_residuals / sum_squares 
    ``` 
 
    This is also called the 
    [coefficient of determination]( 
    https://en.wikipedia.org/wiki/Coefficient_of_determination). 
 
    It indicates how close the fitted regression line 
    is to ground-truth data. 
 
    - The highest score possible is 1.0. It indicates that the predictors 
        perfectly accounts for variation in the target. 
    - A score of 0.0 indicates that the predictors do not 
        account for variation in the target. 
    - It can also be negative if the model is worse than random. 
 
    This metric can also compute the &quot;Adjusted R2&quot; score. 
 
    Args: 
        class_aggregation: Specifies how to aggregate scores corresponding to 
            different output classes (or target dimensions), 
            i.e. different dimensions on the last axis of the predictions. 
            Equivalent to `multioutput` argument in Scikit-Learn. 
            Should be one of 
            `None` (no aggregation), `&quot;uniform_average&quot;`, 
            `&quot;variance_weighted_average&quot;`. 
        num_regressors: Number of independent regressors used 
            (&quot;Adjusted R2&quot; score). 0 is the standard R2 score. 
            Defaults to `0`. 
        name: Optional. string name of the metric instance. 
        dtype: Optional. data type of the metric result. 
 
    Example: 
 
    &gt;&gt;&gt; y_true = np.array([[1], [4], [3]], dtype=np.float32) 
    &gt;&gt;&gt; y_pred = np.array([[2], [4], [4]], dtype=np.float32) 
    &gt;&gt;&gt; metric = keras.metrics.R2Score() 
    &gt;&gt;&gt; metric.update_state(y_true, y_pred) 
    &gt;&gt;&gt; result = metric.result() 
    &gt;&gt;&gt; result 
    0.57142854 
    &quot;&quot;&quot;</span>

    <span class="s0">def </span><span class="s1">__init__</span><span class="s2">(</span>
        <span class="s1">self</span><span class="s2">,</span>
        <span class="s1">class_aggregation</span><span class="s2">=</span><span class="s3">&quot;uniform_average&quot;</span><span class="s2">,</span>
        <span class="s1">num_regressors</span><span class="s2">=</span><span class="s6">0</span><span class="s2">,</span>
        <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;r2_score&quot;</span><span class="s2">,</span>
        <span class="s1">dtype</span><span class="s2">=</span><span class="s0">None</span><span class="s2">,</span>
    <span class="s2">):</span>
        <span class="s1">super</span><span class="s2">().</span><span class="s1">__init__</span><span class="s2">(</span><span class="s1">name</span><span class="s2">=</span><span class="s1">name</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s5"># Metric should be maximized during optimization.</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_direction </span><span class="s2">= </span><span class="s3">&quot;up&quot;</span>

        <span class="s1">valid_class_aggregation_values </span><span class="s2">= (</span>
            <span class="s0">None</span><span class="s2">,</span>
            <span class="s3">&quot;uniform_average&quot;</span><span class="s2">,</span>
            <span class="s3">&quot;variance_weighted_average&quot;</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">class_aggregation </span><span class="s0">not in </span><span class="s1">valid_class_aggregation_values</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Invalid value for argument `class_aggregation`. Expected &quot;</span>
                <span class="s3">f&quot;one of </span><span class="s0">{</span><span class="s1">valid_class_aggregation_values</span><span class="s0">}</span><span class="s3">. &quot;</span>
                <span class="s3">f&quot;Received: class_aggregation=</span><span class="s0">{</span><span class="s1">class_aggregation</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">num_regressors </span><span class="s2">&lt; </span><span class="s6">0</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;Invalid value for argument `num_regressors`. &quot;</span>
                <span class="s3">&quot;Expected a value &gt;= 0. &quot;</span>
                <span class="s3">f&quot;Received: num_regressors=</span><span class="s0">{</span><span class="s1">num_regressors</span><span class="s0">}</span><span class="s3">&quot;</span>
            <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">class_aggregation </span><span class="s2">= </span><span class="s1">class_aggregation</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors </span><span class="s2">= </span><span class="s1">num_regressors</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">shape</span><span class="s2">=(),</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">Zeros</span><span class="s2">(),</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;num_samples&quot;</span><span class="s2">,</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_built </span><span class="s2">= </span><span class="s0">False</span>

    <span class="s0">def </span><span class="s1">_build</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">y_true_shape</span><span class="s2">, </span><span class="s1">y_pred_shape</span><span class="s2">):</span>
        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">y_pred_shape</span><span class="s2">) != </span><span class="s6">2 </span><span class="s0">or </span><span class="s1">len</span><span class="s2">(</span><span class="s1">y_true_shape</span><span class="s2">) != </span><span class="s6">2</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;R2Score expects 2D inputs with shape &quot;</span>
                <span class="s3">&quot;(batch_size, output_dim). Received input &quot;</span>
                <span class="s3">f&quot;shapes: y_pred.shape=</span><span class="s0">{</span><span class="s1">y_pred_shape</span><span class="s0">} </span><span class="s3">and &quot;</span>
                <span class="s3">f&quot;y_true.shape=</span><span class="s0">{</span><span class="s1">y_true_shape</span><span class="s0">}</span><span class="s3">.&quot;</span>
            <span class="s2">)</span>
        <span class="s0">if </span><span class="s1">y_pred_shape</span><span class="s2">[-</span><span class="s6">1</span><span class="s2">] </span><span class="s0">is None or </span><span class="s1">y_true_shape</span><span class="s2">[-</span><span class="s6">1</span><span class="s2">] </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s0">raise </span><span class="s1">ValueError</span><span class="s2">(</span>
                <span class="s3">&quot;R2Score expects 2D inputs with shape &quot;</span>
                <span class="s3">&quot;(batch_size, output_dim), with output_dim fully &quot;</span>
                <span class="s3">&quot;defined (not None). Received input &quot;</span>
                <span class="s3">f&quot;shapes: y_pred.shape=</span><span class="s0">{</span><span class="s1">y_pred_shape</span><span class="s0">} </span><span class="s3">and &quot;</span>
                <span class="s3">f&quot;y_true.shape=</span><span class="s0">{</span><span class="s1">y_true_shape</span><span class="s0">}</span><span class="s3">.&quot;</span>
            <span class="s2">)</span>
        <span class="s1">num_classes </span><span class="s2">= </span><span class="s1">y_pred_shape</span><span class="s2">[-</span><span class="s6">1</span><span class="s2">]</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">squared_sum </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;squared_sum&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=[</span><span class="s1">num_classes</span><span class="s2">],</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">Zeros</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">sum </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;sum&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=[</span><span class="s1">num_classes</span><span class="s2">],</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">Zeros</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">total_mse </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;residual&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=[</span><span class="s1">num_classes</span><span class="s2">],</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">Zeros</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">count </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">add_variable</span><span class="s2">(</span>
            <span class="s1">name</span><span class="s2">=</span><span class="s3">&quot;count&quot;</span><span class="s2">,</span>
            <span class="s1">shape</span><span class="s2">=[</span><span class="s1">num_classes</span><span class="s2">],</span>
            <span class="s1">initializer</span><span class="s2">=</span><span class="s1">initializers</span><span class="s2">.</span><span class="s1">Zeros</span><span class="s2">(),</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">_built </span><span class="s2">= </span><span class="s0">True</span>

    <span class="s0">def </span><span class="s1">update_state</span><span class="s2">(</span><span class="s1">self</span><span class="s2">, </span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">sample_weight</span><span class="s2">=</span><span class="s0">None</span><span class="s2">):</span>
        <span class="s4">&quot;&quot;&quot;Accumulates root mean squared error statistics. 
 
        Args: 
            y_true: The ground truth values. 
            y_pred: The predicted values. 
            sample_weight: Optional weighting of each example. Can 
                be a `Tensor` whose rank is either 0, or the same rank as 
                `y_true`, and must be broadcastable to `y_true`. 
                Defaults to `1`. 
 
        Returns: 
            Update op. 
        &quot;&quot;&quot;</span>
        <span class="s1">y_true </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_dtype</span><span class="s2">)</span>
        <span class="s1">y_pred </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">_dtype</span><span class="s2">)</span>
        <span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred </span><span class="s2">= </span><span class="s1">squeeze_or_expand_to_same_rank</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">)</span>
        <span class="s0">if not </span><span class="s1">self</span><span class="s2">.</span><span class="s1">_built</span><span class="s2">:</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">_build</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">sample_weight </span><span class="s0">is None</span><span class="s2">:</span>
            <span class="s1">sample_weight </span><span class="s2">= </span><span class="s6">1</span>

        <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">len</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">) == </span><span class="s6">1</span><span class="s2">:</span>
            <span class="s5"># Make sure there's a features dimension</span>
            <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">expand_dims</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s6">1</span><span class="s2">)</span>

        <span class="s1">sample_weight </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">broadcast_to</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">))</span>

        <span class="s1">weighted_y_true </span><span class="s2">= </span><span class="s1">y_true </span><span class="s2">* </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">y_true</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">sum </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">weighted_y_true</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s6">0</span><span class="s2">))</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">squared_sum</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">squared_sum </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">y_true </span><span class="s2">* </span><span class="s1">weighted_y_true</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s6">0</span><span class="s2">)</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">total_mse</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span>
            <span class="s1">self</span><span class="s2">.</span><span class="s1">total_mse</span>
            <span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span>
                <span class="s2">(</span><span class="s1">y_true </span><span class="s2">- </span><span class="s1">y_pred</span><span class="s2">) ** </span><span class="s6">2 </span><span class="s2">* </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">cast</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">y_true</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">),</span>
                <span class="s1">axis</span><span class="s2">=</span><span class="s6">0</span><span class="s2">,</span>
            <span class="s2">)</span>
        <span class="s2">)</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">count</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">count </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">sample_weight</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s6">0</span><span class="s2">))</span>
        <span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples </span><span class="s2">+ </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">size</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">result</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">mean </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">sum </span><span class="s2">/ </span><span class="s1">self</span><span class="s2">.</span><span class="s1">count</span>
        <span class="s1">total </span><span class="s2">= </span><span class="s1">self</span><span class="s2">.</span><span class="s1">squared_sum </span><span class="s2">- </span><span class="s1">self</span><span class="s2">.</span><span class="s1">sum </span><span class="s2">* </span><span class="s1">mean</span>
        <span class="s1">raw_scores </span><span class="s2">= </span><span class="s6">1 </span><span class="s2">- (</span><span class="s1">self</span><span class="s2">.</span><span class="s1">total_mse </span><span class="s2">/ </span><span class="s1">total</span><span class="s2">)</span>
        <span class="s1">raw_scores </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">where</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">isinf</span><span class="s2">(</span><span class="s1">raw_scores</span><span class="s2">), </span><span class="s6">0.0</span><span class="s2">, </span><span class="s1">raw_scores</span><span class="s2">)</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">class_aggregation </span><span class="s2">== </span><span class="s3">&quot;uniform_average&quot;</span><span class="s2">:</span>
            <span class="s1">r2_score </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">mean</span><span class="s2">(</span><span class="s1">raw_scores</span><span class="s2">)</span>
        <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">class_aggregation </span><span class="s2">== </span><span class="s3">&quot;variance_weighted_average&quot;</span><span class="s2">:</span>
            <span class="s1">weighted_sum </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">total </span><span class="s2">* </span><span class="s1">raw_scores</span><span class="s2">)</span>
            <span class="s1">sum_of_weights </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">total</span><span class="s2">)</span>
            <span class="s1">r2_score </span><span class="s2">= </span><span class="s1">weighted_sum </span><span class="s2">/ </span><span class="s1">sum_of_weights</span>
        <span class="s0">else</span><span class="s2">:</span>
            <span class="s1">r2_score </span><span class="s2">= </span><span class="s1">raw_scores</span>

        <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors </span><span class="s2">!= </span><span class="s6">0</span><span class="s2">:</span>
            <span class="s0">if </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors </span><span class="s2">&gt; </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples </span><span class="s2">- </span><span class="s6">1</span><span class="s2">:</span>
                <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
                    <span class="s3">&quot;More independent predictors than datapoints &quot;</span>
                    <span class="s3">&quot;in adjusted R2 score. Falling back to standard R2 score.&quot;</span><span class="s2">,</span>
                    <span class="s1">stacklevel</span><span class="s2">=</span><span class="s6">2</span><span class="s2">,</span>
                <span class="s2">)</span>
            <span class="s0">elif </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors </span><span class="s2">== </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples </span><span class="s2">- </span><span class="s6">1</span><span class="s2">:</span>
                <span class="s1">warnings</span><span class="s2">.</span><span class="s1">warn</span><span class="s2">(</span>
                    <span class="s3">&quot;Division by zero in Adjusted R2 score. &quot;</span>
                    <span class="s3">&quot;Falling back to standard R2 score.&quot;</span><span class="s2">,</span>
                    <span class="s1">stacklevel</span><span class="s2">=</span><span class="s6">2</span><span class="s2">,</span>
                <span class="s2">)</span>
            <span class="s0">else</span><span class="s2">:</span>
                <span class="s1">n </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_samples</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;float32&quot;</span><span class="s2">)</span>
                <span class="s1">p </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s3">&quot;float32&quot;</span><span class="s2">)</span>
                <span class="s1">num </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">multiply</span><span class="s2">(</span>
                    <span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s6">1.0</span><span class="s2">, </span><span class="s1">r2_score</span><span class="s2">), </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s6">1.0</span><span class="s2">)</span>
                <span class="s2">)</span>
                <span class="s1">den </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s1">n</span><span class="s2">, </span><span class="s1">p</span><span class="s2">), </span><span class="s6">1.0</span><span class="s2">)</span>
                <span class="s1">r2_score </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">subtract</span><span class="s2">(</span><span class="s6">1.0</span><span class="s2">, </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">divide</span><span class="s2">(</span><span class="s1">num</span><span class="s2">, </span><span class="s1">den</span><span class="s2">))</span>
        <span class="s0">return </span><span class="s1">r2_score</span>

    <span class="s0">def </span><span class="s1">reset_state</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s0">for </span><span class="s1">v </span><span class="s0">in </span><span class="s1">self</span><span class="s2">.</span><span class="s1">variables</span><span class="s2">:</span>
            <span class="s1">v</span><span class="s2">.</span><span class="s1">assign</span><span class="s2">(</span><span class="s1">ops</span><span class="s2">.</span><span class="s1">zeros</span><span class="s2">(</span><span class="s1">v</span><span class="s2">.</span><span class="s1">shape</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">v</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">))</span>

    <span class="s0">def </span><span class="s1">get_config</span><span class="s2">(</span><span class="s1">self</span><span class="s2">):</span>
        <span class="s1">config </span><span class="s2">= {</span>
            <span class="s3">&quot;name&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">name</span><span class="s2">,</span>
            <span class="s3">&quot;dtype&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">,</span>
            <span class="s3">&quot;class_aggregation&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">class_aggregation</span><span class="s2">,</span>
            <span class="s3">&quot;num_regressors&quot;</span><span class="s2">: </span><span class="s1">self</span><span class="s2">.</span><span class="s1">num_regressors</span><span class="s2">,</span>
        <span class="s2">}</span>
        <span class="s1">base_config </span><span class="s2">= </span><span class="s1">super</span><span class="s2">().</span><span class="s1">get_config</span><span class="s2">()</span>
        <span class="s0">return </span><span class="s2">{**</span><span class="s1">base_config</span><span class="s2">, **</span><span class="s1">config</span><span class="s2">}</span>


<span class="s0">def </span><span class="s1">cosine_similarity</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=-</span><span class="s6">1</span><span class="s2">):</span>
    <span class="s4">&quot;&quot;&quot;Computes the cosine similarity between labels and predictions. 
 
    Formula: 
 
    ```python 
    loss = sum(l2_norm(y_true) * l2_norm(y_pred)) 
    ``` 
 
    Args: 
        y_true: Tensor of true targets. 
        y_pred: Tensor of predicted targets. 
        axis: Axis along which to determine similarity. Defaults to `-1`. 
 
    Returns: 
        Cosine similarity tensor. 
 
    Example: 
 
    &gt;&gt;&gt; y_true = [[0., 1.], [1., 1.], [1., 1.]] 
    &gt;&gt;&gt; y_pred = [[1., 0.], [1., 1.], [-1., -1.]] 
    &gt;&gt;&gt; loss = keras.losses.cosine_similarity(y_true, y_pred, axis=-1) 
    [0., 0.99999994, -0.99999994] 
    &quot;&quot;&quot;</span>
    <span class="s1">y_pred </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_pred</span><span class="s2">)</span>
    <span class="s1">y_true </span><span class="s2">= </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">convert_to_tensor</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">dtype</span><span class="s2">=</span><span class="s1">y_pred</span><span class="s2">.</span><span class="s1">dtype</span><span class="s2">)</span>
    <span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred </span><span class="s2">= </span><span class="s1">squeeze_or_expand_to_same_rank</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">y_pred</span><span class="s2">)</span>
    <span class="s1">y_pred </span><span class="s2">= </span><span class="s1">normalize</span><span class="s2">(</span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s1">y_true </span><span class="s2">= </span><span class="s1">normalize</span><span class="s2">(</span><span class="s1">y_true</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
    <span class="s0">return </span><span class="s1">ops</span><span class="s2">.</span><span class="s1">sum</span><span class="s2">(</span><span class="s1">y_true </span><span class="s2">* </span><span class="s1">y_pred</span><span class="s2">, </span><span class="s1">axis</span><span class="s2">=</span><span class="s1">axis</span><span class="s2">)</span>
</pre>
</body>
</html>